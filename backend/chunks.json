[
  "[ Skip to content ](https://ai.pydantic.dev/#introduction)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nIntroduction\n\nType to start searching\n\n[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai \"Go to\nrepository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai \"Go to\nrepository\")\n\n  * Introduction  [ Introduction  ](https://ai.pydantic.dev/)\n    * [ Why use PydanticAI  ](https://ai.pydantic.dev/#why-use-pydanticai)\n    * [ Hello World Example  ](https://ai.pydantic.dev/#hello-world-example)\n    * [ Tools & Dependency Injection Example  ](https://ai.pydantic.dev/#tools-dependency-injection-example)\n    * [ Instrumentation with Pydantic Logfire  ](https://ai.pydantic.dev/#instrumentation-with-pydantic-logfire)\n    * [ llms.txt  ](https://ai.pydantic.dev/#llmstxt)\n    * [ Next Steps  ](https://ai.pydantic.dev/#next-steps)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic",
  "_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Why use PydanticAI  ](https://ai.pydantic.dev/#why-use-pydanticai)\n  * [ Hello World Example  ](https://ai.pydantic.dev/#hello-world-example)\n  * [ Tools & Dependency Injection Example  ](https://ai.pydantic.dev/#tools-dependency-injection-example)\n  * [ Instrumentation with Pydantic Logfire  ](https://ai.pydantic.dev/#instrumentation-with-pydantic-logfire)\n  * [ llms.txt  ](https://ai.pydantic.dev/#llmstxt)\n  * [ Next Steps  ](https://ai.pydantic.dev/#next-steps)\n\n# Introduction\n\n![PydanticAI](https://ai.pydantic.dev/img/pydantic-ai-dark.svg#only-dark)\n\n![PydanticAI](https://ai.pydantic.dev/img/pydantic-ai-light.svg#only-light)\n\n_Agent Framework / shim to use Pydantic with LLMs_\n\n[ ![CI](https://github.com/pydantic/pydantic-\nai/actions/workflows/ci.yml/badge.svg?event=push)\n](https://github.com/pydantic/pydantic-\nai/actions/workflows/ci.yml?query=branch%3Amain) [ ![Coverage](https://coverage-\nbadge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg) ](https://coverage-\nbadge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai) [\n![PyPI](https://img.shields.io/pypi/v/pydantic-ai.svg)\n](https://pypi.python.org/pypi/pydantic-ai) [\n![versions](https://img.shields.io/pypi/pyversions/pydantic-ai.svg)\n](https://github.com/pydantic/pydantic-ai) [\n![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg)\n](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE) [ ![Join\nSlack](https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack)\n](https://logfire.pydantic.dev/docs/join-slack/)\n\nPydanticAI is a Python agent framework designed to make it less painful to build\nproduction grade applications with Generative AI.\n\nFastAPI revolutionized web development by offering an innovative and ergonomic\ndesign, built on the foundation of [Pydantic](https://docs.pydantic.dev).\n\nSimilarly, virtually every agent framework and LLM library in Python uses\nPydantic, yet when we began to use LLMs in [Pydantic\nLogfire](https://pydantic.dev/logfire), we couldn't find anything that gave us\nthe same feeling.\n\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI\napp development.\n\n## Why use PydanticAI",
  "* **Built by the Pydantic Team** : Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n  * **Model-agnostic** : Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n  * **Pydantic Logfire Integration** : Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n  * **Type-safe** : Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n  * **Python-centric Design** : Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n  * **Structured Responses** : Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/output/#structured-output) model outputs, ensuring responses are consistent across runs.\n  * **Dependency Injection System** : Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [output validators](https://ai.pydantic.dev/output/#output-validator-functions). This is useful for testing and eval-driven iterative development.\n  * **Streamed Responses** : Provides the ability to [stream](https://ai.pydantic.dev/output/#streamed-results) LLM responses continuously, with immediate validation, ensuring real time access to validated outputs.\n  * **Graph Support** : [Pydantic Graph](https://ai.pydantic.dev/graph/) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n\n## Hello World Example\n\nHere's a minimal example of PydanticAI:\n\nhello_world.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent(  [](https://ai.pydantic.dev/#__code_0_annotation_1)\n\n    'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  [](https://ai.pydantic.dev/#__code_0_annotation_2)\n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')\n[](https://ai.pydantic.dev/#__code_0_annotation_3)\n\nprint(result.output)\n\n\"\"\"\n\nThe first known use of \"hello, world\" was in a 1974 textbook about the C\nprogramming language.\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nThe exchange should be very short: PydanticAI will send the system prompt and\nthe user query to the LLM, the model will return a text response.\n\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts,\nand structured responses to build more powerful agents.\n\n## Tools & Dependency Injection Example\n\nHere is a concise example using PydanticAI to build a support agent for a bank:\n\nbank_support.py",
  "```\n\nfromdataclassesimport dataclass\n\nfrompydanticimport BaseModel, Field\n\nfrompydantic_aiimport Agent, RunContext\n\nfrombank_databaseimport DatabaseConn\n\n@dataclass\n\nclassSupportDependencies:  [](https://ai.pydantic.dev/#__code_1_annotation_3)\n\n    customer_id: int\n    db: DatabaseConn  [](https://ai.pydantic.dev/#__code_1_annotation_12)\n\nclassSupportOutput(BaseModel):\n[](https://ai.pydantic.dev/#__code_1_annotation_13)\n\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\nsupport_agent = Agent(  [](https://ai.pydantic.dev/#__code_1_annotation_1)\n\n    'openai:gpt-4o',  [](https://ai.pydantic.dev/#__code_1_annotation_2)\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,  [](https://ai.pydantic.dev/#__code_1_annotation_9)\n    system_prompt=(  [](https://ai.pydantic.dev/#__code_1_annotation_4)\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n@support_agent.system_prompt  [](https://ai.pydantic.dev/#__code_1_annotation_5)\n\nasync defadd_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n@support_agent.tool  [](https://ai.pydantic.dev/#__code_1_annotation_6)\n\nasync defcustomer_balance(\n\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n\n\"\"\"Returns the customer's current account balance.\"\"\"\n[](https://ai.pydantic.dev/#__code_1_annotation_7)\n\n    return await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n\n...  [](https://ai.pydantic.dev/#__code_1_annotation_11)\n\nasync defmain():\n\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = await support_agent.run('What is my balance?', deps=deps)  [](https://ai.pydantic.dev/#__code_1_annotation_8)\n    print(result.output)  \nThe output will be validated with Pydantic to guarantee it is a SupportOutput,\nsince the agent is generic, it'll also be typed as a SupportOutput to aid with\nstatic type checking.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_10)\n\n\"\"\"\n\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n\"\"\"\n\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n\n```\n\nComplete `bank_support.py` example\n\nThe code included here is incomplete for the sake of brevity (the definition of\n`DatabaseConn` is missing); you can find the complete `bank_support.py` example\n[here](https://ai.pydantic.dev/examples/bank-support/).\n\n## Instrumentation with Pydantic Logfire\n\nTo understand the flow of the above runs, we can watch the agent in action using\nPydantic Logfire.\n\nTo do this, we need to set up logfire, and add the following to our code:\n\nbank_support_with_logfire.py```\n\n...\n\nfrompydantic_aiimport Agent, RunContext\n\nfrombank_databaseimport DatabaseConn\n\nimportlogfire\n\nlogfire.configure()  \nConfigure logfire, this will fail if project is not set up.\n\n[](https://ai.pydantic.dev/#__code_2_annotation_1)\n\nlogfire.instrument_asyncpg()  \nIn our demo, DatabaseConn uses asyncpg[](https://ai.pydantic.dev/) to connect to\na PostgreSQL database, so\nlogfire.instrument_asyncpg()[](https://magicstack.github.io/asyncpg/current/) is\nused to log the database queries.\n\n[](https://ai.pydantic.dev/#__code_2_annotation_2)\n\n...\n\nsupport_agent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n    instrument=True,\n)",
  "```\n\nThat's enough to get the following view of your agent in action:\n\nSee [Monitoring and Performance](https://ai.pydantic.dev/logfire/) to learn\nmore.\n\n## llms.txt\n\nThe PydanticAI documentation is available in the\n[llms.txt](https://llmstxt.org/) format. This format is defined in Markdown and\nsuited for large language models.\n\nTwo formats are available:\n\n  * [llms.txt](https://ai.pydantic.dev/llms.txt): a file containing a brief description of the project, along with links to the different sections of the documentation. The structure of this file is described in details [here](https://llmstxt.org/#format).\n  * [llms-full.txt](https://ai.pydantic.dev/llms-full.txt): Similar to the `llms.txt` file, but every link content is included. Note that this file may be too large for some LLMs.\n\nAs of today, these files _cannot_ be natively leveraged by LLM frameworks or\nIDEs. Alternatively, an [MCP server](https://modelcontextprotocol.io/) can be\nimplemented to properly parse the `llms.txt` file.\n\n## Next Steps\n\nTo try PydanticAI yourself, follow the instructions [in the\nexamples](https://ai.pydantic.dev/examples/).\n\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building\napplications with PydanticAI.\n\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand\nPydanticAI's interface.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/#introduction)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nIntroduction\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* Introduction  [ Introduction  ](https://ai.pydantic.dev/)\n    * [ Why use PydanticAI  ](https://ai.pydantic.dev/#why-use-pydanticai)\n    * [ Hello World Example  ](https://ai.pydantic.dev/#hello-world-example)\n    * [ Tools & Dependency Injection Example  ](https://ai.pydantic.dev/#tools-dependency-injection-example)\n    * [ Instrumentation with Pydantic Logfire  ](https://ai.pydantic.dev/#instrumentation-with-pydantic-logfire)\n    * [ llms.txt  ](https://ai.pydantic.dev/#llmstxt)\n    * [ Next Steps  ](https://ai.pydantic.dev/#next-steps)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    *",
  "[ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Why use PydanticAI  ](https://ai.pydantic.dev/#why-use-pydanticai)\n  * [ Hello World Example  ](https://ai.pydantic.dev/#hello-world-example)\n  * [ Tools & Dependency Injection Example  ](https://ai.pydantic.dev/#tools-dependency-injection-example)\n  * [ Instrumentation with Pydantic Logfire  ](https://ai.pydantic.dev/#instrumentation-with-pydantic-logfire)\n  * [ llms.txt  ](https://ai.pydantic.dev/#llmstxt)\n  * [ Next Steps  ](https://ai.pydantic.dev/#next-steps)\n\n# Introduction\n\n![PydanticAI](https://ai.pydantic.dev/img/pydantic-ai-dark.svg#only-dark)\n\n![PydanticAI](https://ai.pydantic.dev/img/pydantic-ai-light.svg#only-light)\n\n_Agent Framework / shim to use Pydantic with LLMs_\n\n[ ![CI](https://github.com/pydantic/pydantic-\nai/actions/workflows/ci.yml/badge.svg?event=push)\n](https://github.com/pydantic/pydantic-\nai/actions/workflows/ci.yml?query=branch%3Amain) [ ![Coverage](https://coverage-\nbadge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg) ](https://coverage-\nbadge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai) [\n![PyPI](https://img.shields.io/pypi/v/pydantic-ai.svg)\n](https://pypi.python.org/pypi/pydantic-ai) [\n![versions](https://img.shields.io/pypi/pyversions/pydantic-ai.svg)\n](https://github.com/pydantic/pydantic-ai) [\n![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg)\n](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE) [ ![Join\nSlack](https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack)\n](https://logfire.pydantic.dev/docs/join-slack/)\n\nPydanticAI is a Python agent framework designed to make it less painful to build\nproduction grade applications with Generative AI.\n\nFastAPI revolutionized web development by offering an innovative and ergonomic\ndesign, built on the foundation of [Pydantic](https://docs.pydantic.dev).\n\nSimilarly, virtually every agent framework and LLM library in Python uses\nPydantic, yet when we began to use LLMs in [Pydantic\nLogfire](https://pydantic.dev/logfire), we couldn't find anything that gave us\nthe same feeling.\n\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI\napp development.\n\n## Why use PydanticAI",
  "* **Built by the Pydantic Team** : Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n  * **Model-agnostic** : Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n  * **Pydantic Logfire Integration** : Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n  * **Type-safe** : Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n  * **Python-centric Design** : Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n  * **Structured Responses** : Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/output/#structured-output) model outputs, ensuring responses are consistent across runs.\n  * **Dependency Injection System** : Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [output validators](https://ai.pydantic.dev/output/#output-validator-functions). This is useful for testing and eval-driven iterative development.\n  * **Streamed Responses** : Provides the ability to [stream](https://ai.pydantic.dev/output/#streamed-results) LLM responses continuously, with immediate validation, ensuring real time access to validated outputs.\n  * **Graph Support** : [Pydantic Graph](https://ai.pydantic.dev/graph/) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n\n## Hello World Example\n\nHere's a minimal example of PydanticAI:\n\nhello_world.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent(  [](https://ai.pydantic.dev/#__code_0_annotation_1)\n\n    'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  [](https://ai.pydantic.dev/#__code_0_annotation_2)\n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')\n[](https://ai.pydantic.dev/#__code_0_annotation_3)\n\nprint(result.output)\n\n\"\"\"\n\nThe first known use of \"hello, world\" was in a 1974 textbook about the C\nprogramming language.\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nThe exchange should be very short: PydanticAI will send the system prompt and\nthe user query to the LLM, the model will return a text response.\n\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts,\nand structured responses to build more powerful agents.\n\n## Tools & Dependency Injection Example\n\nHere is a concise example using PydanticAI to build a support agent for a bank:\n\nbank_support.py",
  "```\n\nfromdataclassesimport dataclass\n\nfrompydanticimport BaseModel, Field\n\nfrompydantic_aiimport Agent, RunContext\n\nfrombank_databaseimport DatabaseConn\n\n@dataclass\n\nclassSupportDependencies:  [](https://ai.pydantic.dev/#__code_1_annotation_3)\n\n    customer_id: int\n    db: DatabaseConn  [](https://ai.pydantic.dev/#__code_1_annotation_12)\n\nclassSupportOutput(BaseModel):\n[](https://ai.pydantic.dev/#__code_1_annotation_13)\n\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\nsupport_agent = Agent(  [](https://ai.pydantic.dev/#__code_1_annotation_1)\n\n    'openai:gpt-4o',  [](https://ai.pydantic.dev/#__code_1_annotation_2)\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,  \nThe response from the agent will, be guaranteed to be a SupportOutput, if\nvalidation fails reflection[](https://ai.pydantic.dev/agents/#reflection-and-\nself-correction) will mean the agent is prompted to try again.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_9)\n\n    system_prompt=(  \nStatic system prompts[](https://ai.pydantic.dev/agents/#system-prompts) can be\nregistered with the system_prompt keyword\nargument[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__)\nto the agent.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_4)\n\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n@support_agent.system_prompt  \nDynamic system prompts[](https://ai.pydantic.dev/agents/#system-prompts) can be\nregistered with the\n@agent.system_prompt[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.system_prompt)\ndecorator, and can make use of dependency injection. Dependencies are carried\nvia the\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nargument, which is parameterized with the deps_type from above. If the type\nannotation here is wrong, static type checkers will catch it.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_5)\n\nasync defadd_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n@support_agent.tool  \n\ntool[](https://ai.pydantic.dev/tools/) let you register functions which the LLM\nmay call while responding to a user. Again, dependencies are carried via\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext),\nany other arguments become the tool schema passed to the LLM. Pydantic is used\nto validate these arguments, and errors are passed back to the LLM so it can\nretry.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_6)\n\nasync defcustomer_balance(\n\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n\n\"\"\"Returns the customer's current account balance.\"\"\"  \nThe docstring of a tool is also passed to the LLM as the description of the\ntool. Parameter descriptions are\nextracted[](https://ai.pydantic.dev/tools/#function-tools-and-schema) from the\ndocstring and added to the parameter schema sent to the LLM.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_7)\n\n    return await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n\n...  \nIn a real use case, you'd add more tools and a longer system prompt to the agent\nto extend the context it's equipped with and support it can provide.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_11)\n\nasync defmain():\n\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = await support_agent.run('What is my balance?', deps=deps)  \n\nRun the agent[](https://ai.pydantic.dev/agents/#running-agents) asynchronously,\nconducting a conversation with the LLM until a final response is reached. Even\nin this fairly simple case, the agent will exchange multiple messages with the\nLLM as tools are called to retrieve an output.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_8)\n\n    print(result.output)  \nThe output will be validated with Pydantic to guarantee it is a SupportOutput,\nsince the agent is generic, it'll also be typed as a SupportOutput to aid with\nstatic type checking.\n\n[](https://ai.pydantic.dev/#__code_1_annotation_10)\n\n\"\"\"\n\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n\"\"\"\n\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"",
  "```\n\nComplete `bank_support.py` example\n\nThe code included here is incomplete for the sake of brevity (the definition of\n`DatabaseConn` is missing); you can find the complete `bank_support.py` example\n[here](https://ai.pydantic.dev/examples/bank-support/).\n\n## Instrumentation with Pydantic Logfire\n\nTo understand the flow of the above runs, we can watch the agent in action using\nPydantic Logfire.\n\nTo do this, we need to set up logfire, and add the following to our code:\n\nbank_support_with_logfire.py```\n\n...\n\nfrompydantic_aiimport Agent, RunContext\n\nfrombank_databaseimport DatabaseConn\n\nimportlogfire\n\nlogfire.configure()  \nConfigure logfire, this will fail if project is not set up.\n\n[](https://ai.pydantic.dev/#__code_2_annotation_1)\n\nlogfire.instrument_asyncpg()  \nIn our demo, DatabaseConn uses asyncpg[](https://ai.pydantic.dev/) to connect to\na PostgreSQL database, so\nlogfire.instrument_asyncpg()[](https://magicstack.github.io/asyncpg/current/) is\nused to log the database queries.\n\n[](https://ai.pydantic.dev/#__code_2_annotation_2)\n\n...\n\nsupport_agent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n    instrument=True,\n)\n\n```\n\nThat's enough to get the following view of your agent in action:\n\nSee [Monitoring and Performance](https://ai.pydantic.dev/logfire/) to learn\nmore.\n\n## llms.txt\n\nThe PydanticAI documentation is available in the\n[llms.txt](https://llmstxt.org/) format. This format is defined in Markdown and\nsuited for large language models.\n\nTwo formats are available:\n\n  * [llms.txt](https://ai.pydantic.dev/llms.txt): a file containing a brief description of the project, along with links to the different sections of the documentation. The structure of this file is described in details [here](https://llmstxt.org/#format).\n  * [llms-full.txt](https://ai.pydantic.dev/llms-full.txt): Similar to the `llms.txt` file, but every link content is included. Note that this file may be too large for some LLMs.\n\nAs of today, these files _cannot_ be natively leveraged by LLM frameworks or\nIDEs. Alternatively, an [MCP server](https://modelcontextprotocol.io/) can be\nimplemented to properly parse the `llms.txt` file.\n\n## Next Steps\n\nTo try PydanticAI yourself, follow the instructions [in the\nexamples](https://ai.pydantic.dev/examples/).\n\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building\napplications with PydanticAI.\n\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand\nPydanticAI's interface.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/agents/#introduction)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nAgents\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * Agents  [ Agents  ](https://ai.pydantic.dev/agents/)\n      * [ Introduction  ](https://ai.pydantic.dev/agents/#introduction)\n      * [ Running Agents  ](https://ai.pydantic.dev/agents/#running-agents)\n        * [ Iterating Over an Agent's Graph  ](https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph)\n          * [ async for iteration  ](https://ai.pydantic.dev/agents/#async-for-iteration)\n          * [ Using .next(...) manually  ](https://ai.pydantic.dev/agents/#using-next-manually)\n          * [ Accessing usage and the final output  ](https://ai.pydantic.dev/agents/#accessing-usage-and-the-final-output)\n        * [ Streaming  ](https://ai.pydantic.dev/agents/#streaming)\n        * [ Additional Configuration  ](https://ai.pydantic.dev/agents/#additional-configuration)\n          * [ Usage Limits  ](https://ai.pydantic.dev/agents/#usage-limits)\n          * [ Model (Run) Settings  ](https://ai.pydantic.dev/agents/#model-run-settings)\n        * [ Model specific settings  ](https://ai.pydantic.dev/agents/#model-specific-settings)\n      * [ Runs vs. Conversations  ](https://ai.pydantic.dev/agents/#runs-vs-conversations)\n      * [ Type safe by design  ](https://ai.pydantic.dev/agents/#static-type-checking)\n      * [ System Prompts  ](https://ai.pydantic.dev/agents/#system-prompts)\n      * [ Instructions  ](https://ai.pydantic.dev/agents/#instructions)\n      * [ Reflection and self-correction  ](https://ai.pydantic.dev/agents/#reflection-and-self-correction)\n      * [ Model errors  ](https://ai.pydantic.dev/agents/#model-errors)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    *",
  "[ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Introduction  ](https://ai.pydantic.dev/agents/#introduction)\n  * [ Running Agents  ](https://ai.pydantic.dev/agents/#running-agents)\n    * [ Iterating Over an Agent's Graph  ](https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph)\n      * [ async for iteration  ](https://ai.pydantic.dev/agents/#async-for-iteration)\n      * [ Using .next(...) manually  ](https://ai.pydantic.dev/agents/#using-next-manually)\n      * [ Accessing usage and the final output  ](https://ai.pydantic.dev/agents/#accessing-usage-and-the-final-output)\n    * [ Streaming  ](https://ai.pydantic.dev/agents/#streaming)\n    * [ Additional Configuration  ](https://ai.pydantic.dev/agents/#additional-configuration)\n      * [ Usage Limits  ](https://ai.pydantic.dev/agents/#usage-limits)\n      * [ Model (Run) Settings  ](https://ai.pydantic.dev/agents/#model-run-settings)\n    * [ Model specific settings  ](https://ai.pydantic.dev/agents/#model-specific-settings)\n  * [ Runs vs. Conversations  ](https://ai.pydantic.dev/agents/#runs-vs-conversations)\n  * [ Type safe by design  ](https://ai.pydantic.dev/agents/#static-type-checking)\n  * [ System Prompts  ](https://ai.pydantic.dev/agents/#system-prompts)\n  * [ Instructions  ](https://ai.pydantic.dev/agents/#instructions)\n  * [ Reflection and self-correction  ](https://ai.pydantic.dev/agents/#reflection-and-self-correction)\n  * [ Model errors  ](https://ai.pydantic.dev/agents/#model-errors)\n\n# Agents\n\n## Introduction\n\nAgents are PydanticAI's primary interface for interacting with LLMs.\n\nIn some use cases a single Agent will control an entire application or\ncomponent, but multiple agents can also interact to embody more complex\nworkflows.\n\nThe [`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) class\nhas full API documentation, but conceptually you can think of an agent as a\ncontainer for:",
  "**Component** | **Description**  \n---|---  \n[System prompt(s)](https://ai.pydantic.dev/agents/#system-prompts) | A set of instructions for the LLM written by the developer.  \n[Function tool(s)](https://ai.pydantic.dev/tools/) | Functions that the LLM may call to get information while generating a response.  \n[Structured output type](https://ai.pydantic.dev/output/) | The structured datatype the LLM must return at the end of a run, if specified.  \n[Dependency type constraint](https://ai.pydantic.dev/dependencies/) | System prompt functions, tools, and output validators may all use dependencies when they're run.  \n[LLM model](https://ai.pydantic.dev/api/models/base/) | Optional default LLM model associated with the agent. Can also be specified when running the agent.  \n[Model Settings](https://ai.pydantic.dev/agents/#additional-configuration) | Optional default model settings to help fine tune requests. Can also be specified when running the agent.  \nIn typing terms, agents are generic in their dependency and output types, e.g.,\nan agent which required dependencies of type `Foobar` and produced outputs of\ntype `list[str]` would have type `Agent[Foobar, list[str]]`. In practice, you\nshouldn't need to care about this, it should just mean your IDE can tell you\nwhen you have the right type, and if you choose to use [static type\nchecking](https://ai.pydantic.dev/agents/#static-type-checking) it should work\nwell with PydanticAI.\n\nHere's a toy example of an agent that simulates a roulette wheel:\n\nroulette_wheel.py```\n\nfrompydantic_aiimport Agent, RunContext\n\nroulette_agent = Agent(\n[](https://ai.pydantic.dev/agents/#__code_0_annotation_1)\n\n    'openai:gpt-4o',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n@roulette_agent.tool\n\nasync defroulette_wheel(ctx: RunContext[int], square: int) -> str:\n[](https://ai.pydantic.dev/agents/#__code_0_annotation_2)\n\n\"\"\"check if the square is a winner\"\"\"\n\n    return 'winner' if square == ctx.deps else 'loser'\n\n# Run the agent\n\nsuccess_number = 18  [](https://ai.pydantic.dev/agents/#__code_0_annotation_3)\n\nresult = roulette_agent.run_sync('Put my money on square eighteen',\ndeps=success_number)\n\nprint(result.output)  [](https://ai.pydantic.dev/agents/#__code_0_annotation_4)\n\n#> True\n\nresult = roulette_agent.run_sync('I bet five is the winner',\ndeps=success_number)\n\nprint(result.output)\n\n#> False\n\n```\n\nAgents are designed for reuse, like FastAPI Apps\n\nAgents are intended to be instantiated once (frequently as module globals) and\nreused throughout your application, similar to a small\n[FastAPI](https://fastapi.tiangolo.com/reference/fastapi/#fastapi.FastAPI) app\nor an\n[APIRouter](https://fastapi.tiangolo.com/reference/apirouter/#fastapi.APIRouter).\n\n## Running Agents\n\nThere are four ways to run an agent:\n\n  1. [`agent.run()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run) — a coroutine which returns a [`RunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult) containing a completed response.\n  2. [`agent.run_sync()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_sync) — a plain, synchronous function which returns a [`RunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult) containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`).\n  3. [`agent.run_stream()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_stream) — a coroutine which returns a [`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult), which contains methods to stream a response as an async iterable.\n  4. [`agent.iter()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.iter) — a context manager which returns an [`AgentRun`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRun), an async-iterable over the nodes of the agent's underlying [`Graph`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph).\n\nHere's a simple example demonstrating the first three:\n\nrun_agent.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\n\nprint(result_sync.output)\n\n#> Rome\n\nasync defmain():\n\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n    #> Paris\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> London",
  "```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\nYou can also pass messages from previous runs to continue a conversation or\nprovide context, as described in [Messages and Chat\nHistory](https://ai.pydantic.dev/message-history/).\n\n### Iterating Over an Agent's Graph\n\nUnder the hood, each `Agent` in PydanticAI uses **pydantic-graph** to manage its\nexecution flow. **pydantic-graph** is a generic, type-centric library for\nbuilding and running finite state machines in Python. It doesn't actually depend\non PydanticAI — you can use it standalone for workflows that have nothing to do\nwith GenAI — but PydanticAI makes use of it to orchestrate the handling of model\nrequests and model responses in an agent's run.\n\nIn many scenarios, you don't need to worry about pydantic-graph at all; calling\n`agent.run(...)` simply traverses the underlying graph from start to finish.\nHowever, if you need deeper insight or control — for example to capture each\ntool invocation, or to inject your own logic at specific stages — PydanticAI\nexposes the lower-level iteration process via\n[`Agent.iter`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.iter).\nThis method returns an\n[`AgentRun`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRun),\nwhich you can async-iterate over, or manually drive node-by-node via the\n[`next`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRun.next)\nmethod. Once the agent's graph returns an\n[`End`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End),\nyou have the final result along with a detailed history of all steps.\n\n####  `async for` iteration\n\nHere's an example of using `async for` with `iter` to record each node the agent\nexecutes:\n\nagent_iter_async_for.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync defmain():\n\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n\"\"\"\n\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions=None,\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='Paris')],\n                usage=Usage(\n                    requests=1, request_tokens=56, response_tokens=1, total_tokens=57\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='Paris')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    #> Paris\n\n```\n\n  * The `AgentRun` is an async iterator that yields each node (`BaseNode` or `End`) in the flow.\n  * The run ends when an `End` node is returned.\n\n#### Using `.next(...)` manually\n\nYou can also drive the iteration manually by passing the node you want to run\nnext to the `AgentRun.next(...)` method. This allows you to inspect or modify\nthe node before it executes or skip nodes based on your own logic, and to catch\nerrors in `next()` more easily:\n\nagent_iter_next.py",
  "```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_graphimport End\n\nagent = Agent('openai:gpt-4o')\n\nasync defmain():\n\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  [](https://ai.pydantic.dev/agents/#__code_3_annotation_1)\n\n        all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  [](https://ai.pydantic.dev/agents/#__code_3_annotation_2)\n            node = await agent_run.next(node)  [](https://ai.pydantic.dev/agents/#__code_3_annotation_3)\n            all_nodes.append(node)  [](https://ai.pydantic.dev/agents/#__code_3_annotation_4)\n\n        print(all_nodes)\n\"\"\"\n\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions=None,\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='Paris')],\n                    usage=Usage(\n                        requests=1,\n                        request_tokens=56,\n                        response_tokens=1,\n                        total_tokens=57,\n                    ),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='Paris')),\n        ]\n        \"\"\"\n\n```\n\n#### Accessing usage and the final output\n\nYou can retrieve usage statistics (tokens, requests, etc.) at any time from the\n[`AgentRun`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRun)\nobject via `agent_run.usage()`. This method returns a\n[`Usage`](https://ai.pydantic.dev/api/usage/#pydantic_ai.usage.Usage) object\ncontaining the usage data.\n\nOnce the run finishes, `agent_run.result` becomes a\n[`AgentRunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult)\nobject containing the final output (and related metadata).\n\n* * *\n### Streaming\n\nHere is an example of streaming an agent run in combination with `async for`\niteration:\n\nstreaming.py",
  "```\n\nimportasyncio\n\nfromdataclassesimport dataclass\n\nfromdatetimeimport date\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport (\n\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ToolCallPartDelta,\n)\n\nfrompydantic_ai.toolsimport RunContext\n\n@dataclass\n\nclassWeatherService:\n\n    async defget_forecast(self, location: str, forecast_date: date) -> str:\n        # In real code: call weather API, DB queries, etc.\n        return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n    async defget_historic_weather(self, location: str, forecast_date: date) -> str:\n        # In real code: call a historical weather API or DB\n        return (\n            f'The weather in {location} on {forecast_date} was 18°C and partly cloudy.'\n        )\n\nweather_agent = Agent[WeatherService, str](\n\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    output_type=str,  # We'll produce a final answer as plain text\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n@weather_agent.tool\n\nasync defweather_forecast(\n\n    ctx: RunContext[WeatherService],\n    location: str,\n    forecast_date: date,\n) -> str:\n\n    if forecast_date >= date.today():\n        return await ctx.deps.get_forecast(location, forecast_date)\n    else:\n        return await ctx.deps.get_historic_weather(location, forecast_date)\n\noutput_messages: list[str] = []\n\nasync defmain():\n\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f'=== UserPromptNode: {node.user_prompt} ===')\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append(\n                    '=== ModelRequestNode: streaming partial request tokens ==='\n                )\n                async with node.stream(run.ctx) as request_stream:\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(\n                                f'[Request] Starting part {event.index}: {event.part!r}'\n                            )\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} args_delta={event.delta.args_delta}'\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f'[Result] The model produced a final output (tool_name={event.tool_name})'\n                            )\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append(\n                    '=== CallToolsNode: streaming partial response & tool usage ==='\n                )\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n                            )\n            elif Agent.is_end_node(node):\n                assert run.result.output == node.data.output\n                # Once an End node is reached, the agent run is complete\n                output_messages.append(\n                    f'=== Final Agent Output: {run.result.output} ==='\n                )\n\nif __name__ == '__main__':\n\n    asyncio.run(main())\n\n    print(output_messages)\n\"\"\"",
  "[\n        '=== UserPromptNode: What will the weather be like in Paris on Tuesday? ===',\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args_delta={\"location\":\"Pa',\n        '[Request] Part 0 args_delta=ris\",\"forecast_',\n        '[Request] Part 0 args_delta=date\":\"2030-01-',\n        '[Request] Part 0 args_delta=01\"}',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model produced a final output (tool_name=None)',\n        \"[Request] Part 0 text delta: 'warm and sunny '\",\n        \"[Request] Part 0 text delta: 'in Paris on '\",\n        \"[Request] Part 0 text delta: 'Tuesday.'\",\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '=== Final Agent Output: It will be warm and sunny in Paris on Tuesday. ===',\n    ]\n    \"\"\"\n\n```\n\n* * *\n### Additional Configuration\n\n#### Usage Limits\n\nPydanticAI offers a\n[`UsageLimits`](https://ai.pydantic.dev/api/usage/#pydantic_ai.usage.UsageLimits)\nstructure to help you limit your usage (tokens and/or requests) on model runs.\n\nYou can apply these settings by passing the `usage_limits` argument to the\n`run{_sync,_stream}` functions.\n\nConsider the following example, where we limit the number of response tokens:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.exceptionsimport UsageLimitExceeded\n\nfrompydantic_ai.usageimport UsageLimits\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n\nresult_sync = agent.run_sync(\n\n    'What is the capital of Italy? Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\n\nprint(result_sync.output)\n\n#> Rome\n\nprint(result_sync.usage())\n\n#> Usage(requests=1, request_tokens=62, response_tokens=1, total_tokens=63)\n\ntry:\n\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n\n    print(e)\n    #> Exceeded the response_tokens_limit of 10 (response_tokens=32)\n\n```\n\nRestricting the number of requests can be useful in preventing infinite loops or\nexcessive tool calling:\n\n```\n\nfromtyping_extensionsimport TypedDict\n\nfrompydantic_aiimport Agent, ModelRetry\n\nfrompydantic_ai.exceptionsimport UsageLimitExceeded\n\nfrompydantic_ai.usageimport UsageLimits\n\nclassNeverOutputType(TypedDict):\n\n\"\"\"\n\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\nagent = Agent(\n\n    'anthropic:claude-3-5-sonnet-latest',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n@agent.tool_plain(retries=5)\n[](https://ai.pydantic.dev/agents/#__code_6_annotation_1)\n\ndefinfinite_retry_tool() -> int:\n\n    raise ModelRetry('Please try again.')\n\ntry:\n\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  \nThis run will error after 3 requests, preventing the infinite tool calling.\n\n[](https://ai.pydantic.dev/agents/#__code_6_annotation_2)\n\n    )\nexcept UsageLimitExceeded as e:\n\n    print(e)\n    #> The next request would exceed the request_limit of 3\n\n```\n\nNote\n\nThis is especially relevant if you've registered many tools. The `request_limit`\ncan be used to prevent the model from calling them in a loop too many times.\n\n#### Model (Run) Settings\n\nPydanticAI offers a\n[`settings.ModelSettings`](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings)\nstructure to help you fine tune your requests. This structure allows you to\nconfigure common parameters that influence the model's behavior, such as\n`temperature`, `max_tokens`, `timeout`, and more.\n\nThere are two ways to apply these settings: 1. Passing to `run{_sync,_stream}`\nfunctions via the `model_settings` argument. This allows for fine-tuning on a\nper-request basis. 2. Setting during\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent)\ninitialization via the `model_settings` argument. These settings will be applied\nby default to all subsequent run calls using said agent. However,\n`model_settings` provided during a specific run call will override the agent's\ndefault settings.\n\nFor example, if you'd like to set the `temperature` setting to `0.0` to ensure\nless random behavior, you can do the following:",
  "```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync(\n\n    'What is the capital of Italy?', model_settings={'temperature': 0.0}\n)\n\nprint(result_sync.output)\n\n#> Rome\n\n```\n\n### Model specific settings\n\nIf you wish to further customize model behavior, you can use a subclass of\n[`ModelSettings`](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings),\nlike\n[`GeminiModelSettings`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModelSettings),\nassociated with your model of choice.\n\nFor example:\n\n```\n\nfrompydantic_aiimport Agent, UnexpectedModelBehavior\n\nfrompydantic_ai.models.geminiimport GeminiModelSettings\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\ntry:\n\n    result = agent.run_sync(\n        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n        model_settings=GeminiModelSettings(\n            temperature=0.0,  # general model settings can also be specified\n            gemini_safety_settings=[\n                {\n                    'category': 'HARM_CATEGORY_HARASSMENT',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n                {\n                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n            ],\n        ),\n    )\nexcept UnexpectedModelBehavior as e:\n\n    print(e)  \nThis error is raised because the safety thresholds were exceeded.\n\n[](https://ai.pydantic.dev/agents/#__code_8_annotation_1)\n\n\"\"\"\n\n    Safety settings triggered, body:\n    <safety settings details>\n    \"\"\"\n\n```\n\n## Runs vs. Conversations\n\nAn agent **run** might represent an entire conversation — there's no limit to\nhow many messages can be exchanged in a single run. However, a **conversation**\nmight also be composed of multiple runs, especially if you need to maintain\nstate between separate interactions or API calls.\n\nHere's an example of a conversation comprised of multiple runs:\n\nconversation_example.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o')\n\n# First run\n\nresult1 = agent.run_sync('Who was Albert Einstein?')\n\nprint(result1.output)\n\n#> Albert Einstein was a German-born theoretical physicist.\n\n# Second run, passing previous messages\n\nresult2 = agent.run_sync(\n\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  \nContinue the conversation; without message_history the model would not know who\n\"his\" was referring to.\n\n[](https://ai.pydantic.dev/agents/#__code_9_annotation_1)\n\n)\n\nprint(result2.output)\n\n#> Albert Einstein's most famous equation is (E = mc^2).\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Type safe by design\n\nPydanticAI is designed to work well with static type checkers, like mypy and\npyright.\n\nTyping is (somewhat) optional\n\nPydanticAI is designed to make type checking as useful as possible for you if\nyou choose to use it, but you don't have to use types everywhere all the time.\n\nThat said, because PydanticAI uses Pydantic, and Pydantic uses type hints as the\ndefinition for schema and validation, some types (specifically type hints on\nparameters to tools, and the `output_type` arguments to\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent)) are used\nat runtime.\n\nWe (the library developers) have messed up if type hints are confusing you more\nthan helping you, if you find this, please create an\n[issue](https://github.com/pydantic/pydantic-ai/issues) explaining what's\nannoying you!\n\nIn particular, agents are generic in both the type of their dependencies and the\ntype of the outputs they return, so you can use the type hints to ensure you're\nusing the right types.\n\nConsider the following script with type mistakes:\n\ntype_mistakes.py```\n\nfromdataclassesimport dataclass\n\nfrompydantic_aiimport Agent, RunContext\n\n@dataclass\n\nclassUser:\n\n    name: str\n\nagent = Agent(\n\n    'test',\n    deps_type=User,  \nThe agent is defined as expecting an instance of User as deps.\n\n[](https://ai.pydantic.dev/agents/#__code_10_annotation_1)\n\n    output_type=bool,\n)\n\n@agent.system_prompt\n\ndefadd_user_name(ctx: RunContext[str]) -> str:  \nBut here add_user_name is defined as taking a str as the dependency, not a User.\n\n[](https://ai.pydantic.dev/agents/#__code_10_annotation_2)\n\n    return f\"The user's name is {ctx.deps}.\"\n\ndeffoobar(x: bytes) -> None:\n\n    pass\n\nresult = agent.run_sync('Does their name start with \"A\"?', deps=User('Anne'))\n\nfoobar(result.output)  \nSince the agent is defined as returning a bool, this will raise a type error\nsince foobar expects bytes.\n\n[](https://ai.pydantic.dev/agents/#__code_10_annotation_3)\n\n```\n\nRunning `mypy` on this will give the following output:\n\n```\n\n➤1\"system_prompt\"\"Agent\"type\"Callable[[RunContext[str]],\nstr]\";\"Callable[[RunContext[User]], str]\"[arg-type]\n\ntype_mistakes.py:28:1\"foobar\"type\"bool\";\"bytes\"[arg-type]\n\nFound2in1(checked1source)",
  "```\n\nRunning `pyright` would identify the same issues.\n\n## System Prompts\n\nSystem prompts might seem simple at first glance since they're just strings (or\nsequences of strings that are concatenated), but crafting the right system\nprompt is key to getting the model to behave as you want.\n\nTip\n\nFor most use cases, you should use `instructions` instead of \"system prompts\".\n\nIf you know what you are doing though and want to preserve system prompt\nmessages in the message history sent to the LLM in subsequent completions\nrequests, you can achieve this using the `system_prompt` argument/decorator.\n\nSee the section below on\n[Instructions](https://ai.pydantic.dev/agents/#instructions) for more\ninformation.\n\nGenerally, system prompts fall into two categories:\n\n  1. **Static system prompts** : These are known when writing the code and can be defined via the `system_prompt` parameter of the [`Agent` constructor](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__).\n  2. **Dynamic system prompts** : These depend in some way on context that isn't known until runtime, and should be defined via functions decorated with [`@agent.system_prompt`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.system_prompt).\n\nYou can add both to a single agent; they're appended in the order they're\ndefined at runtime.\n\nHere's an example using both types of system prompts:\n\nsystem_prompts.py```\n\nfromdatetimeimport date\n\nfrompydantic_aiimport Agent, RunContext\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=str,  \nThe agent expects a string dependency.\n\n[](https://ai.pydantic.dev/agents/#__code_12_annotation_1)\n\n    system_prompt=\"Use the customer's name while replying to them.\",  \nStatic system prompt defined at agent creation time.\n\n[](https://ai.pydantic.dev/agents/#__code_12_annotation_2)\n\n)\n\n@agent.system_prompt  \nDynamic system prompt defined via a decorator with\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext),\nthis is called just after run_sync, not when the agent is created, so can\nbenefit from runtime information like the dependencies used on that run.\n\n[](https://ai.pydantic.dev/agents/#__code_12_annotation_3)\n\ndefadd_the_users_name(ctx: RunContext[str]) -> str:\n\n    return f\"The user's name is {ctx.deps}.\"\n\n@agent.system_prompt\n\ndefadd_the_date() -> str:  \nAnother dynamic system prompt, system prompts don't have to have the RunContext\nparameter.\n\n[](https://ai.pydantic.dev/agents/#__code_12_annotation_4)\n\n    return f'The date is {date.today()}.'\n\nresult = agent.run_sync('What is the date?', deps='Frank')\n\nprint(result.output)\n\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Instructions\n\nInstructions are similar to system prompts. The main difference is that when an\nexplicit `message_history` is provided in a call to `Agent.run` and similar\nmethods, _instructions_ from any existing messages in the history are not\nincluded in the request to the model — only the instructions of the _current_\nagent are included.\n\nYou should use:\n\n  * `instructions` when you want your request to the model to only include system prompts for the _current_ agent\n  * `system_prompt` when you want your request to the model to _retain_ the system prompts used in previous requests (possibly made using other agents)\n\nIn general, we recommend using `instructions` instead of `system_prompt` unless\nyou have a specific reason to use `system_prompt`.\n\nInstructions, like system prompts, fall into two categories:\n\n  1. **Static instructions** : These are known when writing the code and can be defined via the `instructions` parameter of the [`Agent` constructor](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__).\n  2. **Dynamic instructions** : These rely on context that is only available at runtime and should be defined using functions decorated with [`@agent.instructions`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.instructions). Unlike dynamic system prompts, which may be reused when `message_history` is present, dynamic instructions are always reevaluated.\n\nBoth static and dynamic instructions can be added to a single agent, and they\nare appended in the order they are defined at runtime.\n\nHere's an example using both types of instructions:\n\ninstructions.py",
  "```\n\nfromdatetimeimport date\n\nfrompydantic_aiimport Agent, RunContext\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=str,  \nThe agent expects a string dependency.\n\n[](https://ai.pydantic.dev/agents/#__code_13_annotation_1)\n\n    instructions=\"Use the customer's name while replying to them.\",  \nStatic instructions defined at agent creation time.\n\n[](https://ai.pydantic.dev/agents/#__code_13_annotation_2)\n\n)\n\n@agent.instructions  \nDynamic instructions defined via a decorator with\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext),\n\n   this is called just after run_sync, not when the agent is created, so can\nbenefit from runtime\n\n   information like the dependencies used on that run.\n\n[](https://ai.pydantic.dev/agents/#__code_13_annotation_3)\n\ndefadd_the_users_name(ctx: RunContext[str]) -> str:\n\n    return f\"The user's name is {ctx.deps}.\"\n\n@agent.instructions\n\ndefadd_the_date() -> str:  \nAnother dynamic instruction, instructions don't have to have the RunContext\nparameter.\n\n[](https://ai.pydantic.dev/agents/#__code_13_annotation_4)\n\n    return f'The date is {date.today()}.'\n\nresult = agent.run_sync('What is the date?', deps='Frank')\n\nprint(result.output)\n\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nNote that returning an empty string will result in no instruction message added.\n\n## Reflection and self-correction\n\nValidation errors from both function tool parameter validation and [structured\noutput validation](https://ai.pydantic.dev/output/#structured-output) can be\npassed back to the model with a request to retry.\n\nYou can also raise\n[`ModelRetry`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.ModelRetry)\nfrom within a [tool](https://ai.pydantic.dev/tools/) or [output validator\nfunction](https://ai.pydantic.dev/output/#output-validator-functions) to tell\nthe model it should retry generating a response.\n\n  * The default retry count is **1** but can be altered for the [entire agent](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__), a [specific tool](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool), or an [output validator](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__).\n  * You can access the current retry count from within a tool or output validator via [`ctx.retry`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext).\n\nHere's an example:\n\ntool_retry.py```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, RunContext, ModelRetry\n\nfromfake_databaseimport DatabaseConn\n\nclassChatResult(BaseModel):\n\n    user_id: int\n    message: str\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n@agent.tool(retries=2)\n\ndefget_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n\n\"\"\"Get a user's ID from their full name.\"\"\"\n\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\nresult = agent.run_sync(\n\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\n\nprint(result.output)\n\n\"\"\"\n\nuser_id=123 message='Hello John, would you be free for coffee sometime next\nweek? Let me know what works for you!'\n\n\"\"\"\n\n```\n\n## Model errors\n\nIf models behave unexpectedly (e.g., the retry limit is exceeded, or their API\nreturns `503`), agent runs will raise\n[`UnexpectedModelBehavior`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.UnexpectedModelBehavior).\n\nIn these cases,\n[`capture_run_messages`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.capture_run_messages)\ncan be used to access the messages exchanged during the run to help diagnose the\nissue.\n\nagent_model_errors.py",
  "```\n\nfrompydantic_aiimport Agent, ModelRetry, UnexpectedModelBehavior,\ncapture_run_messages\n\nagent = Agent('openai:gpt-4o')\n\n@agent.tool_plain\n\ndefcalc_volume(size: int) -> int:  \nDefine a tool that will raise ModelRetry repeatedly in this case.\n\n[](https://ai.pydantic.dev/agents/#__code_15_annotation_1)\n\n    if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\nwith capture_run_messages() as messages:  \n\ncapture_run_messages[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.capture_run_messages)\nis used to capture the messages exchanged during the run.\n\n[](https://ai.pydantic.dev/agents/#__code_15_annotation_2)\n\n    try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        #> An error occurred: Tool exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        #> cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n\"\"\"\n\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=Usage(\n                    requests=1, request_tokens=62, response_tokens=4, total_tokens=66\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=Usage(\n                    requests=1, request_tokens=72, response_tokens=8, total_tokens=80\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\nNote\n\nIf you call\n[`run`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run),\n[`run_sync`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_sync),\nor\n[`run_stream`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_stream)\nmore than once within a single `capture_run_messages` context, `messages` will\nrepresent the messages exchanged during the first call only.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/changelog/#upgrade-guide)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nUpgrade Guide\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * Upgrade Guide  [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n    * [ Breaking Changes  ](https://ai.pydantic.dev/changelog/#breaking-changes)\n      * [ v0.3.0 (2025-06-18)  ](https://ai.pydantic.dev/changelog/#v030-2025-06-18)\n      * [ v0.2.0 (2025-05-12)  ](https://ai.pydantic.dev/changelog/#v020-2025-05-12)\n      * [ v0.1.0 (2025-04-15)  ](https://ai.pydantic.dev/changelog/#v010-2025-04-15)\n    * [ Full Changelog  ](https://ai.pydantic.dev/changelog/#full-changelog)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage",
  "](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Breaking Changes  ](https://ai.pydantic.dev/changelog/#breaking-changes)\n    * [ v0.3.0 (2025-06-18)  ](https://ai.pydantic.dev/changelog/#v030-2025-06-18)\n    * [ v0.2.0 (2025-05-12)  ](https://ai.pydantic.dev/changelog/#v020-2025-05-12)\n    * [ v0.1.0 (2025-04-15)  ](https://ai.pydantic.dev/changelog/#v010-2025-04-15)\n  * [ Full Changelog  ](https://ai.pydantic.dev/changelog/#full-changelog)\n\n# Upgrade Guide\n\nPydanticAI is still pre-version 1, so breaking changes will occur, however:\n\n  * We try to minimize them as much as possible.\n  * We use minor version bumps to signify breaking changes.\n  * Wherever possible we deprecate old features so code continues to work with deprecation warnings when changing the public API.\n  * We intend to release V1 in summer 2025, and then follow strict semantic versioning, e.g. no intentional breaking changes except in minor or patch versions.\n\n## Breaking Changes\n\nNote\n\nHere's a filtered list of the breaking changes for each version to help you\nupgrade PydanticAI.\n\n### v0.3.0 (2025-06-18)\n\nSee [#1142](https://github.com/pydantic/pydantic-ai/pull/1142) — Adds support\nfor thinking parts.\n\nWe now convert the thinking blocks (`\"<think>...\"</think>\"`) in provider\nspecific text parts to PydanticAI `ThinkingPart`s. Also, as part of this\nrelease, we made the choice to not send back the `ThinkingPart`s to the provider\n- the idea is to save costs on behalf of the user. In the future, we intend to\nadd a setting to customize this behavior.\n\n### v0.2.0 (2025-05-12)\n\nSee [#1647](https://github.com/pydantic/pydantic-ai/pull/1647) — usage makes\nsense as part of `ModelResponse`, and could be really useful in \"messages\"\n(really a sequence of requests and response). In this PR:\n\n  * Adds `usage` to `ModelResponse` (field has a default factory of `Usage()` so it'll work to load data that doesn't have usage)\n  * changes the return type of `Model.request` to just `ModelResponse` instead of `tuple[ModelResponse, Usage]`\n\n### v0.1.0 (2025-04-15)\n\nSee [#1248](https://github.com/pydantic/pydantic-ai/pull/1248) — the\nattribute/parameter name `result` was renamed to `output` in many places.\nHopefully all changes keep a deprecated attribute or parameter with the old\nname, so you should get many deprecation warnings.",
  "See [#1484](https://github.com/pydantic/pydantic-ai/pull/1484) — `format_as_xml`\nwas moved and made available to import from the package root, e.g. `from\npydantic_ai import format_as_xml`.\n\n* * *\n## Full Changelog\n\nFor the full changelog, see [GitHub\nReleases](https://github.com/pydantic/pydantic-ai/releases).\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/anthropic/#anthropic)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nAnthropic\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * Anthropic  [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n        * [ Install  ](https://ai.pydantic.dev/models/anthropic/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/anthropic/#configuration)\n        * [ Environment variable  ](https://ai.pydantic.dev/models/anthropic/#environment-variable)\n        * [ provider argument  ](https://ai.pydantic.dev/models/anthropic/#provider-argument)\n        * [ Custom HTTP Client  ](https://ai.pydantic.dev/models/anthropic/#custom-http-client)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydant",
  "ic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/anthropic/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/anthropic/#configuration)\n  * [ Environment variable  ](https://ai.pydantic.dev/models/anthropic/#environment-variable)\n  * [ provider argument  ](https://ai.pydantic.dev/models/anthropic/#provider-argument)\n  * [ Custom HTTP Client  ](https://ai.pydantic.dev/models/anthropic/#custom-http-client)\n\n# Anthropic\n\n## Install\n\nTo use `AnthropicModel` models, you need to either install `pydantic-ai`, or\ninstall `pydantic-ai-slim` with the `anthropic` optional group:\n\n[pip](https://ai.pydantic.dev/models/anthropic/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/anthropic/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[anthropic]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[anthropic]\"\n\n```\n\n## Configuration\n\nTo use [Anthropic](https://anthropic.com) through their API, go to\n[console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)\nto generate an API key.\n\n`AnthropicModelName` contains a list of available Anthropic models.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportANTHROPIC_API_KEY='your-api-key'\n\n```\n\nYou can then use `AnthropicModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nmodel = AnthropicModel('claude-3-5-sonnet-latest')\n\nagent = Agent(model)\n\n...\n\n```\n\n##  `provider` argument\n\nYou can provide a custom `Provider` via the `provider` argument:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nfrompydantic_ai.providers.anthropicimport AnthropicProvider\n\nmodel = AnthropicModel(\n\n    'claude-3-5-sonnet-latest', provider=AnthropicProvider(api_key='your-api-key')\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n## Custom HTTP Client\n\nYou can customize the `AnthropicProvider` with a custom `httpx.AsyncClient`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nfrompydantic_ai.providers.anthropicimport AnthropicProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = AnthropicModel(\n\n    'claude-3-5-sonnet-latest',\n    provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/common-tools/#common-tools)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nCommon Tools\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * Common Tools  [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n      * [ DuckDuckGo Search Tool  ](https://ai.pydantic.dev/common-tools/#duckduckgo-search-tool)\n        * [ Installation  ](https://ai.pydantic.dev/common-tools/#installation)\n        * [ Usage  ](https://ai.pydantic.dev/common-tools/#usage)\n      * [ Tavily Search Tool  ](https://ai.pydantic.dev/common-tools/#tavily-search-tool)\n        * [ Installation  ](https://ai.pydantic.dev/common-tools/#installation_1)\n        * [ Usage  ](https://ai.pydantic.dev/common-tools/#usage_1)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/",
  "models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ DuckDuckGo Search Tool  ](https://ai.pydantic.dev/common-tools/#duckduckgo-search-tool)\n    * [ Installation  ](https://ai.pydantic.dev/common-tools/#installation)\n    * [ Usage  ](https://ai.pydantic.dev/common-tools/#usage)\n  * [ Tavily Search Tool  ](https://ai.pydantic.dev/common-tools/#tavily-search-tool)\n    * [ Installation  ](https://ai.pydantic.dev/common-tools/#installation_1)\n    * [ Usage  ](https://ai.pydantic.dev/common-tools/#usage_1)\n\n# Common Tools\n\nPydanticAI ships with native tools that can be used to enhance your agent's\ncapabilities.\n\n## DuckDuckGo Search Tool\n\nThe DuckDuckGo search tool allows you to search the web for information. It is\nbuilt on top of the [DuckDuckGo\nAPI](https://github.com/deedy5/duckduckgo_search).\n\n### Installation\n\nTo use\n[`duckduckgo_search_tool`](https://ai.pydantic.dev/api/common_tools/#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool),\nyou need to install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-\ninstall) with the `duckduckgo` optional group:\n\n[pip](https://ai.pydantic.dev/common-\ntools/#__tabbed_1_1)[uv](https://ai.pydantic.dev/common-tools/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[duckduckgo]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[duckduckgo]\"\n\n```\n\n### Usage\n\nHere's an example of how you can use the DuckDuckGo search tool with an agent:\n\nduckduckgo_search.py",
  "```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.common_tools.duckduckgoimport duckduckgo_search_tool\n\nagent = Agent(\n\n    'openai:o3-mini',\n    tools=[duckduckgo_search_tool()],\n    system_prompt='Search DuckDuckGo for the given query and return the results.',\n)\n\nresult = agent.run_sync(\n\n    'Can you list the top five highest-grossing animated films of 2025?'\n)\n\nprint(result.output)\n\n\"\"\"\n\nI looked into several sources on animated box‐office performance in 2025, and\nwhile detailed\n\nrankings can shift as more money is tallied, multiple independent reports have\nalready\n\nhighlighted a couple of record‐breaking shows. For example:\n\n• Ne Zha 2 – News outlets (Variety, Wikipedia's \"List of animated feature films\nof 2025\", and others)\n\n    have reported that this Chinese title not only became the highest‑grossing animated film of 2025\n    but also broke records as the highest‑grossing non‑English animated film ever. One article noted\n    its run exceeded US$1.7 billion.\n• Inside Out 2 – According to data shared on Statista and in industry news, this\nPixar sequel has been\n\n    on pace to set new records (with some sources even noting it as the highest‑grossing animated film\n    ever, as of January 2025).\n\nBeyond those two, some entertainment trade sites (for example, a Just Jared\narticle titled\n\n\"Top 10 Highest-Earning Animated Films at the Box Office Revealed\") have begun\nlisting a broader\n\ntop‑10. Although full consolidated figures can sometimes differ by source and\nare updated daily during\n\na box‑office run, many of the industry trackers have begun to single out five\nfilms as the biggest\n\nearners so far in 2025.\n\nUnfortunately, although multiple articles discuss the \"top animated films\" of\n2025, there isn't yet a\n\nsingle, universally accepted list with final numbers that names the complete top\nfive. (Box‑office\n\nrankings, especially mid‑year, can be fluid as films continue to add to their\ntotals.)\n\nBased on what several sources note so far, the two undisputed leaders are:\n\n1. Ne Zha 2\n2. Inside Out 2\n\nThe remaining top spots (3–5) are reported by some outlets in their \"Top‑10\nAnimated Films\"\n\nlists for 2025 but the titles and order can vary depending on the source and the\nexact cut‑off\n\ndate of the data. For the most up‑to‑date and detailed ranking (including the\n3rd, 4th, and 5th\n\nhighest‑grossing films), I recommend checking resources like:\n\n• Wikipedia's \"List of animated feature films of 2025\" page\n\n• Box‑office tracking sites (such as Box Office Mojo or The Numbers)\n\n• Trade articles like the one on Just Jared\n\nTo summarize with what is clear from the current reporting:\n\n1. Ne Zha 2\n2. Inside Out 2\n3–5. Other animated films (yet to be definitively finalized across all reporting\noutlets)\n\nIf you're looking for a final, consensus list of the top five, it may be best to\nwait until\n\nthe 2025 year‑end box‑office tallies are in or to consult a regularly updated\nentertainment industry source.\n\nWould you like help finding a current source or additional details on where to\nlook for the complete updated list?\n\n\"\"\"\n\n```\n\n## Tavily Search Tool\n\nInfo\n\nTavily is a paid service, but they have free credits to explore their product.\n\nYou need to [sign up for an account](https://app.tavily.com/home) and get an API\nkey to use the Tavily search tool.\n\nThe Tavily search tool allows you to search the web for information. It is built\non top of the [Tavily API](https://tavily.com/).\n\n### Installation\n\nTo use\n[`tavily_search_tool`](https://ai.pydantic.dev/api/common_tools/#pydantic_ai.common_tools.tavily.tavily_search_tool),\nyou need to install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-\ninstall) with the `tavily` optional group:\n\n[pip](https://ai.pydantic.dev/common-\ntools/#__tabbed_2_1)[uv](https://ai.pydantic.dev/common-tools/#__tabbed_2_2)\n\n```\n\npip\"pydantic-ai-slim[tavily]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[tavily]\"\n\n```\n\n### Usage\n\nHere's an example of how you can use the Tavily search tool with an agent:\n\ntavily_search.py",
  "```\n\nimportos\n\nfrompydantic_ai.agentimport Agent\n\nfrompydantic_ai.common_tools.tavilyimport tavily_search_tool\n\napi_key = os.getenv('TAVILY_API_KEY')\n\nassert api_key is not None\n\nagent = Agent(\n\n    'openai:o3-mini',\n    tools=[tavily_search_tool(api_key)],\n    system_prompt='Search Tavily for the given query and return the results.',\n)\n\nresult = agent.run_sync('Tell me the top news in the GenAI world, give me\nlinks.')\n\nprint(result.output)\n\n\"\"\"\n\nHere are some of the top recent news articles related to GenAI:\n\n1. How CLEAR users can improve risk analysis with GenAI – Thomson Reuters\n   Read more: https://legal.thomsonreuters.com/blog/how-clear-users-can-improve-\nrisk-analysis-with-genai/\n\n   (This article discusses how CLEAR's new GenAI-powered tool streamlines risk\nanalysis by quickly summarizing key information from various public data\nsources.)\n\n2. TELUS Digital Survey Reveals Enterprise Employees Are Entering Sensitive Data Into AI Assistants More Than You Think – FT.com\n   Read more:\nhttps://markets.ft.com/data/announce/detail?dockey=600-202502260645BIZWIRE_USPRX____20250226_BW490609-1\n\n   (This news piece highlights findings from a TELUS Digital survey showing that\nmany enterprise employees use public GenAI tools and sometimes even enter\nsensitive data.)\n\n3. The Essential Guide to Generative AI – Virtualization Review\n   Read more: https://virtualizationreview.com/Whitepapers/2025/02/SNOWFLAKE-\nThe-Essential-Guide-to-Generative-AI.aspx\n\n   (This guide provides insights into how GenAI is revolutionizing enterprise\nstrategies and productivity, with input from industry leaders.)\n\nFeel free to click on the links to dive deeper into each story!\n\n\"\"\"",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/cohere/#cohere)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nCohere\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * Cohere  [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n        * [ Install  ](https://ai.pydantic.dev/models/cohere/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/cohere/#configuration)\n        * [ Environment variable  ](https://ai.pydantic.dev/models/cohere/#environment-variable)\n        * [ provider argument  ](https://ai.pydantic.dev/models/cohere/#provider-argument)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydanti",
  "c.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/cohere/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/cohere/#configuration)\n  * [ Environment variable  ](https://ai.pydantic.dev/models/cohere/#environment-variable)\n  * [ provider argument  ](https://ai.pydantic.dev/models/cohere/#provider-argument)\n\n# Cohere\n\n## Install\n\nTo use `CohereModel`, you need to either install `pydantic-ai`, or install\n`pydantic-ai-slim` with the `cohere` optional group:\n\n[pip](https://ai.pydantic.dev/models/cohere/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/cohere/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[cohere]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[cohere]\"\n\n```\n\n## Configuration\n\nTo use [Cohere](https://cohere.com/) through their API, go to\n[dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and\nfollow your nose until you find the place to generate an API key.\n\n`CohereModelName` contains a list of the most popular Cohere models.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportCO_API_KEY='your-api-key'\n\n```\n\nYou can then use `CohereModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('cohere:command')\n\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.cohereimport CohereModel\n\nmodel = CohereModel('command')\n\nagent = Agent(model)\n\n...\n\n```\n\n##  `provider` argument\n\nYou can provide a custom `Provider` via the `provider` argument:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.cohereimport CohereModel\n\nfrompydantic_ai.providers.cohereimport CohereProvider\n\nmodel = CohereModel('command', provider=CohereProvider(api_key='your-api-key'))\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize the `CohereProvider` with a custom `http_client`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.cohereimport CohereModel\n\nfrompydantic_ai.providers.cohereimport CohereProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = CohereModel(\n\n    'command',\n    provider=CohereProvider(api_key='your-api-key', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/message-history/#messages-and-chat-\nhistory)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nMessages and chat history\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * Messages and chat history  [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n      * [ Accessing Messages from Results  ](https://ai.pydantic.dev/message-history/#accessing-messages-from-results)\n      * [ Using Messages as Input for Further Agent Runs  ](https://ai.pydantic.dev/message-history/#using-messages-as-input-for-further-agent-runs)\n      * [ Storing and loading messages (to JSON)  ](https://ai.pydantic.dev/message-history/#storing-and-loading-messages-to-json)\n      * [ Other ways of using messages  ](https://ai.pydantic.dev/message-history/#other-ways-of-using-messages)\n      * [ Processing Message History  ](https://ai.pydantic.dev/message-history/#processing-message-history)\n        * [ Usage  ](https://ai.pydantic.dev/message-history/#usage)\n          * [ Keep Only Recent Messages  ](https://ai.pydantic.dev/message-history/#keep-only-recent-messages)\n          * [ RunContext parameter  ](https://ai.pydantic.dev/message-history/#runcontext-parameter)\n          * [ Summarize Old Messages  ](https://ai.pydantic.dev/message-history/#summarize-old-messages)\n        * [ Testing History Processors  ](https://ai.pydantic.dev/message-history/#testing-history-processors)\n        * [ Multiple Processors  ](https://ai.pydantic.dev/message-history/#multiple-processors)\n      * [ Examples  ](https://ai.pydantic.dev/message-history/#examples)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/to",
  "ols/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Accessing Messages from Results  ](https://ai.pydantic.dev/message-history/#accessing-messages-from-results)\n  * [ Using Messages as Input for Further Agent Runs  ](https://ai.pydantic.dev/message-history/#using-messages-as-input-for-further-agent-runs)\n  * [ Storing and loading messages (to JSON)  ](https://ai.pydantic.dev/message-history/#storing-and-loading-messages-to-json)\n  * [ Other ways of using messages  ](https://ai.pydantic.dev/message-history/#other-ways-of-using-messages)\n  * [ Processing Message History  ](https://ai.pydantic.dev/message-history/#processing-message-history)\n    * [ Usage  ](https://ai.pydantic.dev/message-history/#usage)\n      * [ Keep Only Recent Messages  ](https://ai.pydantic.dev/message-history/#keep-only-recent-messages)\n      * [ RunContext parameter  ](https://ai.pydantic.dev/message-history/#runcontext-parameter)\n      * [ Summarize Old Messages  ](https://ai.pydantic.dev/message-history/#summarize-old-messages)\n    * [ Testing History Processors  ](https://ai.pydantic.dev/message-history/#testing-history-processors)\n    * [ Multiple Processors  ](https://ai.pydantic.dev/message-history/#multiple-processors)\n  * [ Examples  ](https://ai.pydantic.dev/message-history/#examples)\n\n# Messages and chat history\n\nPydanticAI provides access to messages exchanged during an agent run. These\nmessages can be used both to continue a coherent conversation, and to understand\nhow an agent performed.\n\n### Accessing Messages from Results\n\nAfter running an agent, you can access the messages exchanged during that run\nfrom the `result` object.",
  "Both\n[`RunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult)\n(returned by\n[`Agent.run`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run),\n[`Agent.run_sync`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_sync))\nand\n[`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult)\n(returned by\n[`Agent.run_stream`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_stream))\nhave the following methods:\n\n  * [`all_messages()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult.all_messages): returns all messages, including messages from prior runs. There's also a variant that returns JSON bytes, [`all_messages_json()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult.all_messages_json).\n  * [`new_messages()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult.new_messages): returns only the messages from the current run. There's also a variant that returns JSON bytes, [`new_messages_json()`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult.new_messages_json).\n\nStreamedRunResult and complete messages\n\nOn\n[`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult),\nthe messages returned from these methods will only include the final result\nmessage once the stream has finished.\n\nE.g. you've awaited one of the following coroutines:\n\n  * [`StreamedRunResult.stream()`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream)\n  * [`StreamedRunResult.stream_text()`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_text)\n  * [`StreamedRunResult.stream_structured()`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_structured)\n  * [`StreamedRunResult.get_output()`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.get_output)\n\n**Note:** The final result message will NOT be added to result messages if you\nuse\n[`.stream_text(delta=True)`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_text)\nsince in this case the result content is never built as one string.\n\nExample of accessing methods on a\n[`RunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult)\n:\n\nrun_result_messages.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\n\nprint(result.output)\n\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\n# all messages from the run\n\nprint(result.all_messages())\n\n\"\"\"\n\n[\n\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nExample of accessing methods on a\n[`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult)\n:\n\nstreamed_run_result_messages.py",
  "```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nasync defmain():\n\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n\"\"\"\n\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            #> Did you hear\n            #> Did you hear about the toothpaste\n            #> Did you hear about the toothpaste scandal? They called\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n        # complete messages once the stream finishes\n        print(result.all_messages())\n\"\"\"\n\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=Usage(request_tokens=50, response_tokens=12, total_tokens=62),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n### Using Messages as Input for Further Agent Runs\n\nThe primary use of message histories in PydanticAI is to maintain context across\nmultiple agent runs.\n\nTo use existing messages in a run, pass them to the `message_history` parameter\nof\n[`Agent.run`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run),\n[`Agent.run_sync`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_sync)\nor\n[`Agent.run_stream`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_stream).\n\nIf `message_history` is set and not empty, a new system prompt is not generated\n— we assume the existing message history includes a system prompt.\n\nReusing messages in a conversation```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\n\nprint(result1.output)\n\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync('Explain?', message_history=result1.new_messages())\n\nprint(result2.output)\n\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\n\"\"\"\n\n[\n\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=61, response_tokens=26, total_tokens=87),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\n\"\"\"",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Storing and loading messages (to JSON)\n\nWhile maintaining conversation state in memory is enough for many applications,\noften times you may want to store the messages history of an agent run on disk\nor in a database. This might be for evals, for sharing data between Python and\nJavaScript/TypeScript, or any number of other use cases.\n\nThe intended way to do this is using a `TypeAdapter`.\n\nWe export\n[`ModelMessagesTypeAdapter`](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.ModelMessagesTypeAdapter)\nthat can be used for this, or you can create your own.\n\nHere's an example showing how:\n\nserialize messages to json```\n\nfrompydantic_coreimport to_jsonable_python\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessagesTypeAdapter  \nAlternatively, you can create a TypeAdapter from scratch:\n\n  \n```\n\nfrompydanticimport TypeAdapter\n\nfrompydantic_ai.messagesimport ModelMessage\n\nModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n\n```\n\n[](https://ai.pydantic.dev/message-history/#__code_3_annotation_1) agent =\nAgent('openai:gpt-4o', system_prompt='Be a helpful assistant.') result1 =\nagent.run_sync('Tell me a joke.') history_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)\n\nAlternatively you can serialize to/from JSON directly:\n\n```\n\nfrompydantic_coreimport to_json\n\n...\n\nas_json_objects = to_json(history_step_1)\n\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n\n```\n\n[](https://ai.pydantic.dev/message-history/#__code_3_annotation_2)\nsame_history_as_step_1 =\nModelMessagesTypeAdapter.validate_python(as_python_objects) result2 =\nagent.run_sync(\n\nYou can now continue the conversation with history `same_history_as_step_1`\ndespite creating a new agent run.\n\n[](https://ai.pydantic.dev/message-history/#__code_3_annotation_3) 'Tell me a\ndifferent joke.', message_history=same_history_as_step_1 ) `\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Other ways of using messages\n\nSince messages are defined by simple dataclasses, you can manually create and\nmanipulate, e.g. for testing.\n\nThe message format is independent of the model used, so you can use messages in\ndifferent agents, or the same agent with different models.\n\nIn the example below, we reuse the message from the first agent run, which uses\nthe `openai:gpt-4o` model, in a second agent run using the `google-\ngla:gemini-1.5-pro` model.\n\nReusing messages with a different model```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\n\nprint(result1.output)\n\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync(\n\n    'Explain?',\n    model='google-gla:gemini-1.5-pro',\n    message_history=result1.new_messages(),\n)\n\nprint(result2.output)\n\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\n\"\"\"\n\n[\n\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=61, response_tokens=26, total_tokens=87),\n        model_name='gemini-1.5-pro',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\n\"\"\"",
  "```\n\n## Processing Message History\n\nSometimes you may want to modify the message history before it's sent to the\nmodel. This could be for privacy reasons (filtering out sensitive information),\nto save costs on tokens, to give less context to the LLM, or custom processing\nlogic.\n\nPydanticAI provides a `history_processors` parameter on `Agent` that allows you\nto intercept and modify the message history before each model request.\n\n### Usage\n\nThe `history_processors` is a list of callables that take a list of\n[`ModelMessage`](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.ModelMessage)\nand return a modified list of the same type.\n\nEach processor is applied in sequence, and processors can be either synchronous\nor asynchronous.\n\nsimple_history_processor.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport (\n\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\ndeffilter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n\n\"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n# Create agent with history processor\n\nagent = Agent('openai:gpt-4o', history_processors=[filter_responses])\n\n# Example: Create some conversation history\n\nmessage_history = [\n\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]\n\n# When you run the agent, the history processor will filter out ModelResponse\nmessages\n\n# result = agent.run_sync('What about 3+3?', message_history=message_history)\n\n```\n\n#### Keep Only Recent Messages\n\nYou can use the `history_processor` to only keep the recent messages:\n\nkeep_recent_messages.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessage\n\nasync defkeep_recent_messages(messages: list[ModelMessage]) ->\nlist[ModelMessage]:\n\n\"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('openai:gpt-4o', history_processors=[keep_recent_messages])\n\n# Example: Even with a long conversation history, only the last 5 messages are\nsent to the model\n\nlong_conversation_history: list[ModelMessage] = []  # Your long conversation\nhistory here\n\n# result = agent.run_sync('What did we discuss?',\nmessage_history=long_conversation_history)\n\n```\n\n####  `RunContext` parameter\n\nHistory processors can optionally accept a\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nparameter to access additional information about the current run, such as\ndependencies, model information, and usage statistics:\n\ncontext_aware_processor.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessage\n\nfrompydantic_ai.toolsimport RunContext\n\ndefcontext_aware_processor(\n\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('openai:gpt-4o', history_processors=[context_aware_processor])\n\n```\n\nThis allows for more sophisticated message processing based on the current state\nof the agent run.\n\n#### Summarize Old Messages\n\nUse an LLM to summarize older messages to preserve context while reducing\ntokens.\n\nsummarize_old_messages.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessage\n\n# Use a cheaper model to summarize old messages.\n\nsummarize_agent = Agent(\n\n    'openai:gpt-4o-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\n\nFocus on the technical discussion and next steps.\n\n\"\"\",\n\n)\n\nasync defsummarize_old_messages(messages: list[ModelMessage]) ->\nlist[ModelMessage]:\n\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\nagent = Agent('openai:gpt-4o', history_processors=[summarize_old_messages])\n\n```\n\n### Testing History Processors\n\nYou can test what messages are actually sent to the model provider using\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel):\n\ntest_history_processor.py",
  "```\n\nimportpytest\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport (\n\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\n\n@pytest.fixture\n\ndefreceived_messages() -> list[ModelMessage]:\n\n    return []\n\n@pytest.fixture\n\ndeffunction_model(received_messages: list[ModelMessage]) -> FunctionModel:\n\n    defcapture_model_function(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n        # Capture the messages that the provider actually receives\n        received_messages.clear()\n        received_messages.extend(messages)\n        return ModelResponse(parts=[TextPart(content='Provider response')])\n\n    return FunctionModel(capture_model_function)\n\ndeftest_history_processor(function_model: FunctionModel, received_messages:\nlist[ModelMessage]):\n\n    deffilter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n        return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n    agent = Agent(function_model, history_processors=[filter_responses])\n\n    message_history = [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelResponse(parts=[TextPart(content='Answer 1')]),\n    ]\n\n    agent.run_sync('Question 2', message_history=message_history)\n    assert received_messages == [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelRequest(parts=[UserPromptPart(content='Question 2')]),\n    ]\n\n```\n\n### Multiple Processors\n\nYou can also use multiple processors:\n\nmultiple_history_processors.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessage, ModelRequest\n\ndeffilter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\ndefsummarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n\n    return messages[-5:]\n\nagent = Agent('openai:gpt-4o', history_processors=[filter_responses,\nsummarize_old_messages])",
  "```\n\nIn this case, the `filter_responses` processor will be applied first, and the\n`summarize_old_messages` processor will be applied second.\n\n## Examples\n\nFor a more complete example of using messages in conversations, see the [chat\napp](https://ai.pydantic.dev/examples/chat-app/) example.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/dependencies/#dependencies)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nDependencies\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * Dependencies  [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n      * [ Defining Dependencies  ](https://ai.pydantic.dev/dependencies/#defining-dependencies)\n      * [ Accessing Dependencies  ](https://ai.pydantic.dev/dependencies/#accessing-dependencies)\n        * [ Asynchronous vs. Synchronous dependencies  ](https://ai.pydantic.dev/dependencies/#asynchronous-vs-synchronous-dependencies)\n      * [ Full Example  ](https://ai.pydantic.dev/dependencies/#full-example)\n      * [ Overriding Dependencies  ](https://ai.pydantic.dev/dependencies/#overriding-dependencies)\n      * [ Examples  ](https://ai.pydantic.dev/dependencies/#examples)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ]",
  "(https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Defining Dependencies  ](https://ai.pydantic.dev/dependencies/#defining-dependencies)\n  * [ Accessing Dependencies  ](https://ai.pydantic.dev/dependencies/#accessing-dependencies)\n    * [ Asynchronous vs. Synchronous dependencies  ](https://ai.pydantic.dev/dependencies/#asynchronous-vs-synchronous-dependencies)\n  * [ Full Example  ](https://ai.pydantic.dev/dependencies/#full-example)\n  * [ Overriding Dependencies  ](https://ai.pydantic.dev/dependencies/#overriding-dependencies)\n  * [ Examples  ](https://ai.pydantic.dev/dependencies/#examples)\n\n# Dependencies\n\nPydanticAI uses a dependency injection system to provide data and services to\nyour agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts),\n[tools](https://ai.pydantic.dev/tools/) and [output\nvalidators](https://ai.pydantic.dev/output/#output-validator-functions).\n\nMatching PydanticAI's design philosophy, our dependency system tries to use\nexisting best practice in Python development rather than inventing esoteric\n\"magic\", this should make dependencies type-safe, understandable easier to test\nand ultimately easier to deploy in production.\n\n## Defining Dependencies\n\nDependencies can be any python type. While in simple cases you might be able to\npass a single object as a dependency (e.g. an HTTP connection),\n[dataclasses](https://docs.python.org/3/library/dataclasses.html#module-\ndataclasses) are generally a convenient container when your dependencies\nincluded multiple objects.\n\nHere's an example of defining an agent that requires dependencies.\n\n(**Note:** dependencies aren't actually used in this example, see [Accessing\nDependencies](https://ai.pydantic.dev/dependencies/#accessing-dependencies)\nbelow)\n\nunused_dependencies.py",
  "```\n\nfromdataclassesimport dataclass\n\nimporthttpx\n\nfrompydantic_aiimport Agent\n\n@dataclass\n\nclassMyDeps:  \nDefine a dataclass to hold dependencies.\n\n[](https://ai.pydantic.dev/dependencies/#__code_0_annotation_1)\n\n    api_key: str\n    http_client: httpx.AsyncClient\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=MyDeps,  \nPass the dataclass type to the deps_type argument of the Agent\nconstructor[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__).\n**Note**: we're passing the type here, NOT an instance, this parameter is not\nactually used at runtime, it's here so we can get full type checking of the\nagent.\n\n[](https://ai.pydantic.dev/dependencies/#__code_0_annotation_2)\n\n)\n\nasync defmain():\n\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run(\n            'Tell me a joke.',\n            deps=deps,  \nWhen running the agent, pass an instance of the dataclass to the deps parameter.\n\n[](https://ai.pydantic.dev/dependencies/#__code_0_annotation_3)\n\n        )\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n## Accessing Dependencies\n\nDependencies are accessed through the\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\ntype, this should be the first parameter of system prompt functions etc.\n\nsystem_prompt_dependencies.py```\n\nfromdataclassesimport dataclass\n\nimporthttpx\n\nfrompydantic_aiimport Agent, RunContext\n\n@dataclass\n\nclassMyDeps:\n\n    api_key: str\n    http_client: httpx.AsyncClient\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n@agent.system_prompt  \n\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nmay optionally be passed to a\nsystem_prompt[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.system_prompt)\nfunction as the only argument.\n\n[](https://ai.pydantic.dev/dependencies/#__code_1_annotation_1)\n\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str:  \n\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext) is\nparameterized with the type of the dependencies, if this type is incorrect,\nstatic type checkers will raise an error.\n\n[](https://ai.pydantic.dev/dependencies/#__code_1_annotation_2)\n\n    response = await ctx.deps.http_client.get(  \nAccess dependencies through the\n.deps[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext.deps)\nattribute.\n\n[](https://ai.pydantic.dev/dependencies/#__code_1_annotation_3)\n\n        'https://example.com',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},  \nAccess dependencies through the\n.deps[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext.deps)\nattribute.\n\n[](https://ai.pydantic.dev/dependencies/#__code_1_annotation_4)\n\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\nasync defmain():\n\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n### Asynchronous vs. Synchronous dependencies\n\n[System prompt functions](https://ai.pydantic.dev/agents/#system-prompts),\n[function tools](https://ai.pydantic.dev/tools/) and [output\nvalidators](https://ai.pydantic.dev/output/#output-validator-functions) are all\nrun in the async context of an agent run.\n\nIf these functions are not coroutines (e.g. `async def`) they are called with\n[`run_in_executor`](https://docs.python.org/3/library/asyncio-\neventloop.html#asyncio.loop.run_in_executor) in a thread pool, it's therefore\nmarginally preferable to use `async` methods where dependencies perform IO,\nalthough synchronous dependencies should work fine too.\n\n`run` vs. `run_sync` and Asynchronous vs. Synchronous dependencies\n\nWhether you use synchronous or asynchronous dependencies, is completely\nindependent of whether you use `run` or `run_sync` — `run_sync` is just a\nwrapper around `run` and agents are always run in an async context.\n\nHere's the same example as above, but with a synchronous dependency:\n\nsync_dependencies.py",
  "```\n\nfromdataclassesimport dataclass\n\nimporthttpx\n\nfrompydantic_aiimport Agent, RunContext\n\n@dataclass\n\nclassMyDeps:\n\n    api_key: str\n    http_client: httpx.Client  \nHere we use a synchronous httpx.Client instead of an asynchronous\nhttpx.AsyncClient.\n\n[](https://ai.pydantic.dev/dependencies/#__code_2_annotation_1)\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n@agent.system_prompt\n\ndefget_system_prompt(ctx: RunContext[MyDeps]) -> str:  \nTo match the synchronous dependency, the system prompt function is now a plain\nfunction, not a coroutine.\n\n[](https://ai.pydantic.dev/dependencies/#__code_2_annotation_2)\n\n    response = ctx.deps.http_client.get(\n        'https://example.com', headers={'Authorization': f'Bearer {ctx.deps.api_key}'}\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\nasync defmain():\n\n    deps = MyDeps('foobar', httpx.Client())\n    result = await agent.run(\n        'Tell me a joke.',\n        deps=deps,\n    )\n    print(result.output)\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n## Full Example\n\nAs well as system prompts, dependencies can be used in\n[tools](https://ai.pydantic.dev/tools/) and [output\nvalidators](https://ai.pydantic.dev/output/#output-validator-functions).\n\nfull_example.py```\n\nfromdataclassesimport dataclass\n\nimporthttpx\n\nfrompydantic_aiimport Agent, ModelRetry, RunContext\n\n@dataclass\n\nclassMyDeps:\n\n    api_key: str\n    http_client: httpx.AsyncClient\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n@agent.system_prompt\n\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str:\n\n    response = await ctx.deps.http_client.get('https://example.com')\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n@agent.tool  \nTo pass RunContext to a tool, use the\ntool[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool)\ndecorator.\n\n[](https://ai.pydantic.dev/dependencies/#__code_3_annotation_1)\n\nasync defget_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\n\n    response = await ctx.deps.http_client.get(\n        'https://example.com#jokes',\n        params={'subject': subject},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n@agent.output_validator  \n\nRunContext may optionally be passed to a\noutput_validator[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.output_validator)\nfunction as the first argument.\n\n[](https://ai.pydantic.dev/dependencies/#__code_3_annotation_2)\n\nasync defvalidate_output(ctx: RunContext[MyDeps], output: str) -> str:\n\n    response = await ctx.deps.http_client.post(\n        'https://example.com#validate',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n        params={'query': output},\n    )\n    if response.status_code == 400:\n        raise ModelRetry(f'invalid response: {response.text}')\n    response.raise_for_status()\n    return output\n\nasync defmain():\n\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n## Overriding Dependencies\n\nWhen testing agents, it's useful to be able to customise dependencies.\n\nWhile this can sometimes be done by calling the agent directly within unit\ntests, we can also override dependencies while calling application code which in\nturn calls the agent.\n\nThis is done via the\n[`override`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.override)\nmethod on the agent.\n\njoke_app.py",
  "```\n\nfromdataclassesimport dataclass\n\nimporthttpx\n\nfrompydantic_aiimport Agent, RunContext\n\n@dataclass\n\nclassMyDeps:\n\n    api_key: str\n    http_client: httpx.AsyncClient\n\n    async defsystem_prompt_factory(self) -> str:  \nDefine a method on the dependency to make the system prompt easier to customise.\n\n[](https://ai.pydantic.dev/dependencies/#__code_4_annotation_1)\n\n        response = await self.http_client.get('https://example.com')\n        response.raise_for_status()\n        return f'Prompt: {response.text}'\n\njoke_agent = Agent('openai:gpt-4o', deps_type=MyDeps)\n\n@joke_agent.system_prompt\n\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str:\n\n    return await ctx.deps.system_prompt_factory()  \nCall the system prompt factory from within the system prompt function.\n\n[](https://ai.pydantic.dev/dependencies/#__code_4_annotation_2)\n\nasync defapplication_code(prompt: str) -> str:  \nApplication code that calls the agent, in a real application this might be an\nAPI endpoint.\n\n[](https://ai.pydantic.dev/dependencies/#__code_4_annotation_3)\n\n    ...\n    ...\n    # now deep within application code we call our agent\n    async with httpx.AsyncClient() as client:\n        app_deps = MyDeps('foobar', client)\n        result = await joke_agent.run(prompt, deps=app_deps)  \nCall the agent from within the application code, in a real application this call\nmight be deep within a call stack. Note app_deps here will NOT be used when deps\nare overridden.\n\n[](https://ai.pydantic.dev/dependencies/#__code_4_annotation_4)\n\n    return result.output\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\ntest_joke_app.py```\n\nfromjoke_appimport MyDeps, application_code, joke_agent\n\nclassTestMyDeps(MyDeps):  \nDefine a subclass of MyDeps in tests to customise the system prompt factory.\n\n[](https://ai.pydantic.dev/dependencies/#__code_5_annotation_1)\n\n    async defsystem_prompt_factory(self) -> str:\n        return 'test prompt'\n\nasync deftest_application_code():\n\n    test_deps = TestMyDeps('test_key', None)  \nCreate an instance of the test dependency, we don't need to pass an http_client\nhere as it's not used.\n\n[](https://ai.pydantic.dev/dependencies/#__code_5_annotation_2)\n\n    with joke_agent.override(deps=test_deps):  \nOverride the dependencies of the agent for the duration of the with block,\ntest_deps will be used when the agent is run.\n\n[](https://ai.pydantic.dev/dependencies/#__code_5_annotation_3)\n\n        joke = await application_code('Tell me a joke.')  \nNow we can safely call our application code, the agent will use the overridden\ndependencies.\n\n[](https://ai.pydantic.dev/dependencies/#__code_5_annotation_4)\n\n    assert joke.startswith('Did you hear about the toothpaste scandal?')",
  "```\n\n## Examples\n\nThe following examples demonstrate how to use dependencies in PydanticAI:\n\n  * [Weather Agent](https://ai.pydantic.dev/examples/weather-agent/)\n  * [SQL Generation](https://ai.pydantic.dev/examples/sql-gen/)\n  * [RAG](https://ai.pydantic.dev/examples/rag/)\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/bedrock/#bedrock)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nBedrock\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * Bedrock  [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n        * [ Install  ](https://ai.pydantic.dev/models/bedrock/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/bedrock/#configuration)\n        * [ Environment variables  ](https://ai.pydantic.dev/models/bedrock/#environment-variables)\n        * [ Customizing Bedrock Runtime API  ](https://ai.pydantic.dev/models/bedrock/#customizing-bedrock-runtime-api)\n        * [ provider argument  ](https://ai.pydantic.dev/models/bedrock/#provider-argument)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https:",
  "//ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/bedrock/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/bedrock/#configuration)\n  * [ Environment variables  ](https://ai.pydantic.dev/models/bedrock/#environment-variables)\n  * [ Customizing Bedrock Runtime API  ](https://ai.pydantic.dev/models/bedrock/#customizing-bedrock-runtime-api)\n  * [ provider argument  ](https://ai.pydantic.dev/models/bedrock/#provider-argument)\n\n# Bedrock\n\n## Install\n\nTo use `BedrockConverseModel`, you need to either install `pydantic-ai`, or\ninstall `pydantic-ai-slim` with the `bedrock` optional group:\n\n[pip](https://ai.pydantic.dev/models/bedrock/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/bedrock/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[bedrock]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[bedrock]\"\n\n```\n\n## Configuration\n\nTo use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS\naccount with Bedrock enabled and appropriate credentials. You can use either AWS\ncredentials directly or a pre-configured boto3 client.\n\n`BedrockModelName` contains a list of available Bedrock models, including models\nfrom Anthropic, Amazon, Cohere, Meta, and Mistral.\n\n## Environment variables\n\nYou can set your AWS credentials as environment variables ([among other\noptions](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-\nenvironment-variables)):\n\n```\n\nexportAWS_ACCESS_KEY_ID='your-access-key'\n\nexportAWS_SECRET_ACCESS_KEY='your-secret-key'\n\nexportAWS_DEFAULT_REGION='us-east-1'# or your preferred region\n\n```\n\nYou can then use `BedrockConverseModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')\n\n...\n\n```\n\nOr initialize the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.bedrockimport BedrockConverseModel\n\nmodel = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')\n\nagent = Agent(model)\n\n...\n\n```\n\n## Customizing Bedrock Runtime API\n\nYou can customize the Bedrock Runtime API calls by adding additional parameters,\nsuch as [guardrail\nconfigurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)\nand [performance\nsettings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-\noptimized-inference.html). For a complete list of configurable parameters, refer\nto the documentation for\n[`BedrockModelSettings`](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockModelSettings).\n\ncustomize_bedrock_model_settings.py",
  "```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.bedrockimport BedrockConverseModel, BedrockModelSettings\n\n# Define Bedrock model settings with guardrail and performance configurations\n\nbedrock_model_settings = BedrockModelSettings(\n\n    bedrock_guardrail_config={\n        'guardrailIdentifier': 'v1',\n        'guardrailVersion': 'v1',\n        'trace': 'enabled'\n    },\n    bedrock_performance_configuration={\n        'latency': 'optimized'\n    }\n)\n\nmodel = BedrockConverseModel(model_name='us.amazon.nova-pro-v1:0')\n\nagent = Agent(model=model, model_settings=bedrock_model_settings)\n\n```\n\n##  `provider` argument\n\nYou can provide a custom `BedrockProvider` via the `provider` argument. This is\nuseful when you want to specify credentials directly or use a custom boto3\nclient:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.bedrockimport BedrockConverseModel\n\nfrompydantic_ai.providers.bedrockimport BedrockProvider\n\n# Using AWS credentials directly\n\nmodel = BedrockConverseModel(\n\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(\n        region_name='us-east-1',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key',\n    ),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also pass a pre-configured boto3 client:\n\n```\n\nimportboto3\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.bedrockimport BedrockConverseModel\n\nfrompydantic_ai.providers.bedrockimport BedrockProvider\n\n# Using a pre-configured boto3 client\n\nbedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n\nmodel = BedrockConverseModel(\n\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(bedrock_client=bedrock_client),\n)\n\nagent = Agent(model)\n\n...",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/install/#installation)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nInstallation\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * Installation  [ Installation  ](https://ai.pydantic.dev/install/)\n    * [ Use with Pydantic Logfire  ](https://ai.pydantic.dev/install/#use-with-pydantic-logfire)\n    * [ Running Examples  ](https://ai.pydantic.dev/install/#running-examples)\n    * [ Slim Install  ](https://ai.pydantic.dev/install/#slim-install)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic",
  ".dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Use with Pydantic Logfire  ](https://ai.pydantic.dev/install/#use-with-pydantic-logfire)\n  * [ Running Examples  ](https://ai.pydantic.dev/install/#running-examples)\n  * [ Slim Install  ](https://ai.pydantic.dev/install/#slim-install)\n\n# Installation\n\nPydanticAI is available on PyPI as [`pydantic-\nai`](https://pypi.org/project/pydantic-ai/) so installation is as simple as:\n\n[pip](https://ai.pydantic.dev/install/#__tabbed_1_1)[uv](https://ai.pydantic.dev/install/#__tabbed_1_2)\n\n```\n\npip\n\n```\n\n```\n\nuv\n\n```\n\n(Requires Python 3.9+)\n\nThis installs the `pydantic_ai` package, core dependencies, and libraries\nrequired to use all the models included in PydanticAI. If you want to use a\nspecific model, you can install the\n[\"slim\"](https://ai.pydantic.dev/install/#slim-install) version of PydanticAI.\n\n## Use with Pydantic Logfire\n\nPydanticAI has an excellent (but completely optional) integration with [Pydantic\nLogfire](https://pydantic.dev/logfire) to help you view and understand agent\nruns.\n\nTo use Logfire with PydanticAI, install `pydantic-ai` or `pydantic-ai-slim` with\nthe `logfire` optional group:\n\n[pip](https://ai.pydantic.dev/install/#__tabbed_2_1)[uv](https://ai.pydantic.dev/install/#__tabbed_2_2)\n\n```\n\npip\"pydantic-ai[logfire]\"\n\n```\n\n```\n\nuv\"pydantic-ai[logfire]\"\n\n```\n\nFrom there, follow the [Logfire setup\ndocs](https://ai.pydantic.dev/logfire/#using-logfire) to configure Logfire.\n\n## Running Examples\n\nWe distribute the [`pydantic_ai_examples`](https://github.com/pydantic/pydantic-\nai/tree/main/examples/pydantic_ai_examples) directory as a separate PyPI package\n([`pydantic-ai-examples`](https://pypi.org/project/pydantic-ai-examples/)) to\nmake examples extremely easy to customize and run.\n\nTo install examples, use the `examples` optional group:\n\n[pip](https://ai.pydantic.dev/install/#__tabbed_3_1)[uv](https://ai.pydantic.dev/install/#__tabbed_3_2)\n\n```\n\npip\"pydantic-ai[examples]\"\n\n```\n\n```\n\nuv\"pydantic-ai[examples]\"\n\n```\n\nTo run the examples, follow instructions in the [examples\ndocs](https://ai.pydantic.dev/examples/).\n\n## Slim Install\n\nIf you know which model you're going to use and want to avoid installing\nsuperfluous packages, you can use the [`pydantic-ai-\nslim`](https://pypi.org/project/pydantic-ai-slim/) package. For example, if\nyou're using just\n[`OpenAIModel`](https://ai.pydantic.dev/api/models/openai/#pydantic_ai.models.openai.OpenAIModel),\nyou would run:\n\n[pip](https://ai.pydantic.dev/install/#__tabbed_4_1)[uv](https://ai.pydantic.dev/install/#__tabbed_4_2)\n\n```\n\npip\"pydantic-ai-slim[openai]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[openai]\"",
  "```\n\n`pydantic-ai-slim` has the following optional groups:\n\n  * `logfire` — installs [`logfire`](https://ai.pydantic.dev/logfire/) [PyPI ↗](https://pypi.org/project/logfire)\n  * `evals` — installs [`pydantic-evals`](https://ai.pydantic.dev/evals/) [PyPI ↗](https://pypi.org/project/pydantic-evals)\n  * `openai` — installs `openai` [PyPI ↗](https://pypi.org/project/openai)\n  * `vertexai` — installs `google-auth` [PyPI ↗](https://pypi.org/project/google-auth) and `requests` [PyPI ↗](https://pypi.org/project/requests)\n  * `anthropic` — installs `anthropic` [PyPI ↗](https://pypi.org/project/anthropic)\n  * `groq` — installs `groq` [PyPI ↗](https://pypi.org/project/groq)\n  * `mistral` — installs `mistralai` [PyPI ↗](https://pypi.org/project/mistralai)\n  * `cohere` - installs `cohere` [PyPI ↗](https://pypi.org/project/cohere)\n  * `duckduckgo` - installs `duckduckgo-search` [PyPI ↗](https://pypi.org/project/duckduckgo-search)\n  * `tavily` - installs `tavily-python` [PyPI ↗](https://pypi.org/project/tavily-python)\n\nSee the [models](https://ai.pydantic.dev/models/) documentation for information\non which optional dependencies are required for each model.\n\nYou can also install dependencies for multiple models and use cases, for\nexample:\n\n[pip](https://ai.pydantic.dev/install/#__tabbed_5_1)[uv](https://ai.pydantic.dev/install/#__tabbed_5_2)\n\n```\n\npip\"pydantic-ai-slim[openai,vertexai,logfire]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[openai,vertexai,logfire]\"\n\n```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/help/#getting-help)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nGetting Help\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * Getting Help  [ Getting Help  ](https://ai.pydantic.dev/help/)\n    * [ Slack  ](https://ai.pydantic.dev/help/#slack)\n    * [ GitHub Issues  ](https://ai.pydantic.dev/help/#github-issues)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](h",
  "ttps://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Slack  ](https://ai.pydantic.dev/help/#slack)\n  * [ GitHub Issues  ](https://ai.pydantic.dev/help/#github-issues)\n\n# Getting Help\n\nIf you need help getting started with PydanticAI or with advanced usage, the\nfollowing sources may be useful.\n\nJoin the `#pydantic-ai` channel in the [Pydantic\nSlack](https://logfire.pydantic.dev/docs/join-slack/) to ask questions, get\nhelp, and chat about PydanticAI. There's also channels for Pydantic, Logfire,\nand FastUI.\n\nIf you're on a [Logfire](https://pydantic.dev/logfire) Pro plan, you can also\nget a dedicated private slack collab channel with us.\n\nThe [PydanticAI GitHub Issues](https://github.com/pydantic/pydantic-ai/issues)\nare a great place to ask questions and give us feedback.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/#model-providers)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nModel Providers\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)",
  "* [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ OpenAI-compatible Providers  ](https://ai.pydantic.dev/models/#openai-compatible-providers)\n  * [ Models and Providers  ](https://ai.pydantic.dev/models/#models-and-providers)\n  * [ Custom Models  ](https://ai.pydantic.dev/models/#custom-models)\n  * [ Fallback Model  ](https://ai.pydantic.dev/models/#fallback-model)\n\n# Model Providers\n\nPydanticAI is model-agnostic and has built-in support for multiple model\nproviders:\n\n  * [OpenAI](https://ai.pydantic.dev/models/openai/)\n  * [Anthropic](https://ai.pydantic.dev/models/anthropic/)\n  * [Gemini](https://ai.pydantic.dev/models/gemini/) (via two different APIs: Generative Language API and VertexAI API)\n  * [Groq](https://ai.pydantic.dev/models/groq/)\n  * [Mistral](https://ai.pydantic.dev/models/mistral/)\n  * [Cohere](https://ai.pydantic.dev/models/cohere/)\n  * [Bedrock](https://ai.pydantic.dev/models/bedrock/)\n\n## OpenAI-compatible Providers\n\nIn addition, many providers are compatible with the OpenAI API, and can be used\nwith `OpenAIModel` in PydanticAI:\n\n  * [DeepSeek](https://ai.pydantic.dev/models/openai/#deepseek)\n  * [Grok (xAI)](https://ai.pydantic.dev/models/openai/#grok-xai)\n  * [Ollama](https://ai.pydantic.dev/models/openai/#ollama)\n  * [OpenRouter](https://ai.pydantic.dev/models/openai/#openrouter)\n  * [Perplexity](https://ai.pydantic.dev/models/openai/#perplexity)\n  * [Fireworks AI](https://ai.pydantic.dev/models/openai/#fireworks-ai)\n  * [Together AI](https://ai.pydantic.dev/models/openai/#together-ai)\n  * [Azure AI Foundry](https://ai.pydantic.dev/models/openai/#azure-ai-foundry)\n  * [Heroku](https://ai.pydantic.dev/models/openai/#heroku-ai)\n\nPydanticAI also comes with\n[`TestModel`](https://ai.pydantic.dev/api/models/test/) and\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/) for testing and\ndevelopment.\n\nTo use each model provider, you need to configure your local environment and\nmake sure you have the right packages installed.\n\n## Models and Providers\n\nPydanticAI uses a few key terms to describe how it interacts with different\nLLMs:\n\n  * **Model** : This refers to the PydanticAI class used to make requests following a specific LLM API (generally by wrapping a vendor-provided SDK, like the `openai` python SDK). These classes implement a vendor-SDK-agnostic API, ensuring a single PydanticAI agent is portable to different LLM vendors without any other code changes just by swapping out the Model it uses. Model classes are named roughly in the format `<VendorSdk>Model`, for example, we have `OpenAIModel`, `AnthropicModel`, `GeminiModel`, etc. When using a Model class, you specify the actual LLM model name (e.g., `gpt-4o`, `claude-3-5-sonnet-latest`, `gemini-1.5-flash`) as a parameter.\n  * **Provider** : This refers to provider-specific classes which handle the authentication and connections to an LLM vendor. Passing a non-default _Provider_ as a parameter to a Model is how you can ensure that your agent will make requests to a specific endpoint, or make use of a specific approach to authentication (e.g., you can use Vertex-specific auth with the `GeminiModel` by way of the `VertexProvider`). In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility with the vendor SDK used by an existing Model (such as `OpenAIModel`).\n  * **Profile** : This refers to a description of how requests to a specific model or family of models need to be constructed to get the best results, independent of the model and provider classes used. For example, different models have different restrictions on the JSON schemas that can be used for tools, and the same schema transformer needs to be used for Gemini models whether you're using `GoogleModel` with model name `gemini-2.5-pro-preview`, or `OpenAIModel` with `OpenRouterProvider` and model name `google/gemini-2.5-pro-preview`.",
  "When you instantiate an\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) with just\na name formatted as `<provider>:<model>`, e.g. `openai:gpt-4o` or\n`openrouter:google/gemini-2.5-pro-preview`, PydanticAI will automatically select\nthe appropriate model class, provider, and profile. If you want to use a\ndifferent provider or profile, you can instantiate a model class directly and\npass in `provider` and/or `profile` arguments.\n\n## Custom Models\n\nTo implement support for a model API that's not already supported, you will need\nto subclass the\n[`Model`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.Model)\nabstract base class. For streaming, you'll also need to implement the\n[`StreamedResponse`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.StreamedResponse)\nabstract base class.\n\nThe best place to start is to review the source code for existing\nimplementations, e.g. [`OpenAIModel`](https://github.com/pydantic/pydantic-\nai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).\n\nFor details on when we'll accept contributions adding new models to PydanticAI,\nsee the [contributing guidelines](https://ai.pydantic.dev/contributing/#new-\nmodel-rules).\n\nIf a model API is compatible with the OpenAI API, you do not need a custom model\nclass and can provide your own [custom\nprovider](https://ai.pydantic.dev/models/openai/#openai-compatible-models)\ninstead.\n\n## Fallback Model\n\nYou can use\n[`FallbackModel`](https://ai.pydantic.dev/api/models/fallback/#pydantic_ai.models.fallback.FallbackModel)\nto attempt multiple models in sequence until one successfully returns a result.\nUnder the hood, PydanticAI automatically switches from one model to the next if\nthe current model returns a 4xx or 5xx status code.\n\nIn the following example, the agent first makes a request to the OpenAI model\n(which fails due to an invalid API key), and then falls back to the Anthropic\nmodel.\n\nfallback_model.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nfrompydantic_ai.models.fallbackimport FallbackModel\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nopenai_model = OpenAIModel('gpt-4o')\n\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\n\nresponse = agent.run_sync('What is the capital of France?')\n\nprint(response.data)\n\n#> Paris\n\nprint(response.all_messages())\n\n\"\"\"\n\n[\n\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='What is the capital of France?',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            )\n        ],\n        kind='request',\n    ),\n    ModelResponse(\n        parts=[TextPart(content='Paris', part_kind='text')],\n        model_name='claude-3-5-sonnet-latest',\n        timestamp=datetime.datetime(...),\n        kind='response',\n        vendor_id=None,\n    ),\n]\n\n\"\"\"\n\n```\n\nThe `ModelResponse` message above indicates in the `model_name` field that the\noutput was returned by the Anthropic model, which is the second model specified\nin the `FallbackModel`.\n\nNote\n\nEach model's options should be configured individually. For example, `base_url`,\n`api_key`, and custom clients should be set on each model itself, not on the\n`FallbackModel`.\n\nIn this next example, we demonstrate the exception-handling capabilities of\n`FallbackModel`. If all models fail, a\n[`FallbackExceptionGroup`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.FallbackExceptionGroup)\nis raised, which contains all the exceptions encountered during the `run`\nexecution.\n\n[Python >=3.11](https://ai.pydantic.dev/models/#__tabbed_1_1)[Python\n<3.11](https://ai.pydantic.dev/models/#__tabbed_1_2)\n\nfallback_model_failure.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.exceptionsimport ModelHTTPError\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nfrompydantic_ai.models.fallbackimport FallbackModel\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nopenai_model = OpenAIModel('gpt-4o')\n\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\n\ntry:\n\n    response = agent.run_sync('What is the capital of France?')\nexcept* ModelHTTPError as exc_group:\n\n    for exc in exc_group.exceptions:\n        print(exc)\n\n```\n\nSince\n[`except*`](https://docs.python.org/3/reference/compound_stmts.html#except-star)\nis only supported in Python 3.11+, we use the\n[`exceptiongroup`](https://github.com/agronholm/exceptiongroup) backport package\nfor earlier Python versions:\n\nfallback_model_failure.py",
  "```\n\nfromexceptiongroupimport catch\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.exceptionsimport ModelHTTPError\n\nfrompydantic_ai.models.anthropicimport AnthropicModel\n\nfrompydantic_ai.models.fallbackimport FallbackModel\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\ndefmodel_status_error_handler(exc_group: BaseExceptionGroup) -> None:\n\n    for exc in exc_group.exceptions:\n        print(exc)\n\nopenai_model = OpenAIModel('gpt-4o')\n\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\n\nwith catch({ModelHTTPError: model_status_error_handler}):\n\n    response = agent.run_sync('What is the capital of France?')\n\n```\n\nBy default, the `FallbackModel` only moves on to the next model if the current\nmodel raises a\n[`ModelHTTPError`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.ModelHTTPError).\nYou can customize this behavior by passing a custom `fallback_on` argument to\nthe `FallbackModel` constructor.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/contributing/#installation-and-\nsetup)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nContributing\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * Contributing  [ Contributing  ](https://ai.pydantic.dev/contributing/)\n    * [ Installation and Setup  ](https://ai.pydantic.dev/contributing/#installation-and-setup)\n    * [ Running Tests etc.  ](https://ai.pydantic.dev/contributing/#running-tests-etc)\n    * [ Documentation Changes  ](https://ai.pydantic.dev/contributing/#documentation-changes)\n    * [ Rules for adding new models to PydanticAI  ](https://ai.pydantic.dev/contributing/#new-model-rules)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.model",
  "s.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Installation and Setup  ](https://ai.pydantic.dev/contributing/#installation-and-setup)\n  * [ Running Tests etc.  ](https://ai.pydantic.dev/contributing/#running-tests-etc)\n  * [ Documentation Changes  ](https://ai.pydantic.dev/contributing/#documentation-changes)\n  * [ Rules for adding new models to PydanticAI  ](https://ai.pydantic.dev/contributing/#new-model-rules)\n\n# Contributing\n\nWe'd love you to contribute to PydanticAI!\n\n## Installation and Setup\n\nClone your fork and cd into the repo directory\n\n```\n\ngitcd\n\n```\n\nInstall `uv` (version 0.4.30 or later), `pre-commit` and `deno`:\n\n  * [`uv` install docs](https://docs.astral.sh/uv/getting-started/installation/)\n  * [`pre-commit` install docs](https://pre-commit.com/#install)\n  * [`deno` install docs](https://docs.deno.com/runtime/getting_started/installation/)\n\nTo install `pre-commit` you can run the following command:\n\n```\n\nuv\n\n```\n\nFor `deno`, you can run the following, or check [their\ndocumentation](https://docs.deno.com/runtime/getting_started/installation/) for\nalternative installation methods:\n\n```\n\ncurl|\n\n```\n\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\n\n```\n\nmake\n\n```\n\n## Running Tests etc.\n\nWe use `make` to manage most commands you'll need to run.\n\nFor details on available commands, run:\n\n```\n\nmakehelp\n\n```\n\nTo run code formatting, linting, static type checks, and tests with coverage\nreport generation, run:\n\n```\n\nmake\n\n```\n\n## Documentation Changes\n\nTo run the documentation page locally, run:\n\n```\n\nuv",
  "```\n\n## Rules for adding new models to PydanticAI\n\nTo avoid an excessive workload for the maintainers of PydanticAI, we can't\naccept all model contributions, so we're setting the following rules for when\nwe'll accept new models and when we won't. This should hopefully reduce the\nchances of disappointment and wasted work.\n\n  * To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\n  * To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\n  * For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\n  * For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) and implements a model that inherits from our [`Model`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.Model) ABC\n\nIf you're unsure about adding a model, please [create an\nissue](https://github.com/pydantic/pydantic-ai/issues).\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/a2a/#agent2agent-a2a-protocol)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nA2A\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * A2A  [ A2A  ](https://ai.pydantic.dev/a2a/)\n      * [ FastA2A  ](https://ai.pydantic.dev/a2a/#fasta2a)\n        * [ Design  ](https://ai.pydantic.dev/a2a/#design)\n        * [ Installation  ](https://ai.pydantic.dev/a2a/#installation)\n        * [ PydanticAI Agent to A2A Server  ](https://ai.pydantic.dev/a2a/#pydanticai-agent-to-a2a-server)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https:/",
  "/ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ FastA2A  ](https://ai.pydantic.dev/a2a/#fasta2a)\n    * [ Design  ](https://ai.pydantic.dev/a2a/#design)\n    * [ Installation  ](https://ai.pydantic.dev/a2a/#installation)\n    * [ PydanticAI Agent to A2A Server  ](https://ai.pydantic.dev/a2a/#pydanticai-agent-to-a2a-server)\n\n# Agent2Agent (A2A) Protocol\n\nThe [Agent2Agent (A2A) Protocol](https://google.github.io/A2A/) is an open\nstandard introduced by Google that enables communication and interoperability\nbetween AI agents, regardless of the framework or vendor they are built on.\n\nAt Pydantic, we built the [FastA2A](https://ai.pydantic.dev/a2a/#fasta2a)\nlibrary to make it easier to implement the A2A protocol in Python.\n\nWe also built a convenience method that expose PydanticAI agents as A2A servers\n- let's have a quick look at how to use it:\n\nagent_to_a2a.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = agent.to_a2a()\n\n```\n\n_You can run the example with`uvicorn agent_to_a2a:app --host 0.0.0.0 --port\n8000`_\n\nThis will expose the agent as an A2A server, and you can start sending requests\nto it.\n\nSee more about [exposing PydanticAI agents as A2A\nservers](https://ai.pydantic.dev/a2a/#pydanticai-agent-to-a2a-server).\n\n## FastA2A\n\n**FastA2A** is an agentic framework agnostic implementation of the A2A protocol\nin Python. The library is designed to be used with any agentic framework, and is\n**not exclusive to PydanticAI**.\n\n### Design\n\n**FastA2A** is built on top of [Starlette](https://starlette.io), which means\nit's fully compatible with any ASGI server.\n\nGiven the nature of the A2A protocol, it's important to understand the design\nbefore using it, as a developer you'll need to provide some components:\n\n  * [`Storage`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Storage): to save and load tasks\n  * [`Broker`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Broker): to schedule tasks\n  * [`Worker`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Worker): to execute tasks\n\nLet's have a look at how those components fit together:\n\n```\n\nflowchart TB\n\n    Server[\"HTTP Server\"] <--> |Sends Requests/<br>Receives Results| TM\n\n    subgraph CC[Core Components]\n        direction RL\n        TM[\"TaskManager<br>(coordinates)\"] --> |Schedules Tasks| Broker\n        TM <--> Storage\n        Broker[\"Broker<br>(queues & schedules)\"] <--> Storage[\"Storage<br>(persistence)\"]\n        Broker --> |Delegates Execution| Worker\n    end\n\n    Worker[\"Worker<br>(implementation)\"]\n```\n\nFastA2A allows you to bring your own\n[`Storage`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Storage),\n[`Broker`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Broker) and\n[`Worker`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.Worker).\n\n### Installation\n\nFastA2A is available on PyPI as [`fasta2a`](https://pypi.org/project/fasta2a/)\nso installation is as simple as:\n\n[pip](https://ai.pydantic.dev/a2a/#__tabbed_1_1)[uv](https://ai.pydantic.dev/a2a/#__tabbed_1_2)\n\n```\n\npip\n\n```\n\n```\n\nuv\n\n```\n\nThe only dependencies are:\n\n  * [starlette](https://starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/)\n  * [pydantic](https://pydantic.dev): to validate the request/response messages\n  * [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities\n\nYou can install PydanticAI with the `a2a` extra to include **FastA2A** :\n\n[pip](https://ai.pydantic.dev/a2a/#__tabbed_2_1)[uv](https://ai.pydantic.dev/a2a/#__tabbed_2_2)\n\n```\n\npip'pydantic-ai-slim[a2a]'\n\n```\n\n```\n\nuv'pydantic-ai-slim[a2a]'",
  "```\n\n### PydanticAI Agent to A2A Server\n\nTo expose a PydanticAI agent as an A2A server, you can use the `to_a2a` method:\n\nagent_to_a2a.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = agent.to_a2a()\n\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server.\n\n```\n\nuvicorn0.0.0.08000\n\n```\n\nSince the goal of `to_a2a` is to be a convenience method, it accepts the same\narguments as the\n[`FastA2A`](https://ai.pydantic.dev/api/fasta2a/#fasta2a.FastA2A) constructor.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/mistral/#mistral)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nMistral\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * Mistral  [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n        * [ Install  ](https://ai.pydantic.dev/models/mistral/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/mistral/#configuration)\n        * [ Environment variable  ](https://ai.pydantic.dev/models/mistral/#environment-variable)\n        * [ provider argument  ](https://ai.pydantic.dev/models/mistral/#provider-argument)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](http",
  "s://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/mistral/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/mistral/#configuration)\n  * [ Environment variable  ](https://ai.pydantic.dev/models/mistral/#environment-variable)\n  * [ provider argument  ](https://ai.pydantic.dev/models/mistral/#provider-argument)\n\n# Mistral\n\n## Install\n\nTo use `MistralModel`, you need to either install `pydantic-ai`, or install\n`pydantic-ai-slim` with the `mistral` optional group:\n\n[pip](https://ai.pydantic.dev/models/mistral/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/mistral/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[mistral]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[mistral]\"\n\n```\n\n## Configuration\n\nTo use [Mistral](https://mistral.ai) through their API, go to\n[console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow\nyour nose until you find the place to generate an API key.\n\n`LatestMistralModelNames` contains a list of the most popular Mistral models.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportMISTRAL_API_KEY='your-api-key'\n\n```\n\nYou can then use `MistralModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('mistral:mistral-large-latest')\n\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.mistralimport MistralModel\n\nmodel = MistralModel('mistral-small-latest')\n\nagent = Agent(model)\n\n...\n\n```\n\n##  `provider` argument\n\nYou can provide a custom `Provider` via the `provider` argument:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.mistralimport MistralModel\n\nfrompydantic_ai.providers.mistralimport MistralProvider\n\nmodel = MistralModel(\n\n    'mistral-large-latest', provider=MistralProvider(api_key='your-api-key', base_url='https://<mistral-provider-endpoint>')\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize the provider with a custom `httpx.AsyncHTTPClient`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.mistralimport MistralModel\n\nfrompydantic_ai.providers.mistralimport MistralProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = MistralModel(\n\n    'mistral-large-latest',\n    provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/testing/#unit-testing)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nUnit testing\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * Unit testing  [ Unit testing  ](https://ai.pydantic.dev/testing/)\n      * [ Unit testing with TestModel  ](https://ai.pydantic.dev/testing/#unit-testing-with-testmodel)\n      * [ Unit testing with FunctionModel  ](https://ai.pydantic.dev/testing/#unit-testing-with-functionmodel)\n      * [ Overriding model via pytest fixtures  ](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.py",
  "dantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Unit testing with TestModel  ](https://ai.pydantic.dev/testing/#unit-testing-with-testmodel)\n  * [ Unit testing with FunctionModel  ](https://ai.pydantic.dev/testing/#unit-testing-with-functionmodel)\n  * [ Overriding model via pytest fixtures  ](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures)\n\n# Unit testing\n\nWriting unit tests for PydanticAI code is just like unit tests for any other\nPython code.\n\nBecause for the most part they're nothing new, we have pretty well established\ntools and patterns for writing and running these kinds of tests.\n\nUnless you're really sure you know better, you'll probably want to follow\nroughly this strategy:\n\n  * Use [`pytest`](https://docs.pytest.org/en/stable/) as your test harness\n  * If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/)\n  * Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/) can be useful for comparing large data structures\n  * Use [`TestModel`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel) or [`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel) in place of your actual model to avoid the usage, latency and variability of real LLM calls\n  * Use [`Agent.override`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.override) to replace your model inside your application logic\n  * Set [`ALLOW_MODEL_REQUESTS=False`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS) globally to block any requests from being made to non-test models accidentally\n\n### Unit testing with `TestModel`\n\nThe simplest and fastest way to exercise most of your application code is using\n[`TestModel`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel),\nthis will (by default) call all tools in the agent, then return either plain\ntext or a structured response depending on the return type of the agent.\n\n`TestModel` is not magic\n\nThe \"clever\" (but not too clever) part of `TestModel` is that it will attempt to\ngenerate valid structured data for [function\ntools](https://ai.pydantic.dev/tools/) and [output\ntypes](https://ai.pydantic.dev/output/#structured-output) based on the schema of\nthe registered tools.\n\nThere's no ML or AI in `TestModel`, it's just plain old procedural Python code\nthat tries to generate data that satisfies the JSON schema of a tool.\n\nThe resulting data won't look pretty or relevant, but it should pass Pydantic's\nvalidation in most cases. If you want something more sophisticated, use\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel)\nand write your own data generation logic.\n\nLet's write unit tests for the following application code:\n\nweather_app.py",
  "```\n\nimportasyncio\n\nfromdatetimeimport date\n\nfrompydantic_aiimport Agent, RunContext\n\nfromfake_databaseimport DatabaseConn  \n\nDatabaseConn is a class that holds a database connection\n\n[](https://ai.pydantic.dev/testing/#__code_0_annotation_1)\n\nfromweather_serviceimport WeatherService  \n\nWeatherService has methods to get weather forecasts and historic data about the\nweather\n\n[](https://ai.pydantic.dev/testing/#__code_0_annotation_2)\n\nweather_agent = Agent(\n\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n@weather_agent.tool\n\ndefweather_forecast(\n\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n\n    if forecast_date < date.today():  \nWe need to call a different endpoint depending on whether the date is in the\npast or the future, you'll see why this nuance is important below\n\n[](https://ai.pydantic.dev/testing/#__code_0_annotation_3)\n\n        return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\nasync defrun_weather_forecast(  \nThis function is the code we want to test, together with the agent it uses\n\n[](https://ai.pydantic.dev/testing/#__code_0_annotation_4)\n\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n\n\"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n\n    async with WeatherService() as weather_service:\n\n        async defrun_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n\n```\n\nHere we have a function that takes a list of `(user_prompt, user_id)` tuples,\ngets a weather forecast for each prompt, and stores the result in the database.\n\n**We want to test this code without having to mock certain objects or modify our\ncode so we can pass test objects in.**\n\nHere's how we would write tests using\n[`TestModel`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel):\n\ntest_weather_app.py",
  "```\n\nfromdatetimeimport timezone\n\nimportpytest\n\nfromdirty_equalsimport IsNow, IsStr\n\nfrompydantic_aiimport models, capture_run_messages\n\nfrompydantic_ai.models.testimport TestModel\n\nfrompydantic_ai.messagesimport (\n\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\n\nfrompydantic_ai.usageimport Usage\n\nfromfake_databaseimport DatabaseConn\n\nfromweather_appimport run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  \nWe're using anyio[](https://anyio.readthedocs.io/en/stable/) to run async tests.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_1)\n\nmodels.ALLOW_MODEL_REQUESTS = False  \nThis is a safety measure to make sure we don't accidentally make real requests\nto the LLM while testing, see\nALLOW_MODEL_REQUESTS[](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS)\nfor more details.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_2)\n\nasync deftest_forecast():\n\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  \nWe're using\nAgent.override[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.override)\nto replace the agent's model with\nTestModel[](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel),\nthe nice thing about override is that we can replace the model inside agent\nwithout needing access to the agent run* methods call site.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_3)\n\n            prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  \nNow we call the function we want to test inside the override context manager.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_4)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  \nBut default, TestModel will return a JSON string summarising the tools calls\nmade, and what was returned. If you wanted to customise the response to\nsomething more closely aligned with the domain, you could add\ncustom_output_text='Sunny'[](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel.custom_output_text)\nwhen defining TestModel.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_5)\n\n    assert messages == [  \nSo far we don't actually know which tools were called and with which values, we\ncan use\ncapture_run_messages[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.capture_run_messages)\nto inspect messages from the most recent run and assert the exchange between the\nagent and the model occurred as expected.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_6)\n\n        ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  \nThe IsNow[](https://dirty-\nequals.helpmanual.io/latest/types/datetime/#dirty_equals.IsNow) helper allows us\nto use declarative asserts even with data which will contain timestamps that\nchange over time.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_7)\n\n                ),\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={\n                        'location': 'a',\n                        'forecast_date': '2024-01-01',  \n\nTestModel isn't doing anything clever to extract values from the prompt, so\nthese values are hardcoded.\n\n[](https://ai.pydantic.dev/testing/#__code_1_annotation_8)",
  "},\n                    tool_call_id=IsStr(),\n                )\n            ],\n            usage=Usage(\n                requests=1,\n                request_tokens=71,\n                response_tokens=7,\n                total_tokens=78,\n                details=None,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='weather_forecast',\n                    content='Sunny with a chance of rain',\n                    tool_call_id=IsStr(),\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n            ],\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='{\"weather_forecast\":\"Sunny with a chance of rain\"}',\n                )\n            ],\n            usage=Usage(\n                requests=1,\n                request_tokens=77,\n                response_tokens=16,\n                total_tokens=93,\n                details=None,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n    ]\n\n```\n\n### Unit testing with `FunctionModel`\n\nThe above tests are a great start, but careful readers will notice that the\n`WeatherService.get_forecast` is never called since `TestModel` calls\n`weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel)\nto customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool\nwith custom inputs\n\ntest_weather_app2.py```\n\nimportre\n\nimportpytest\n\nfrompydantic_aiimport models\n\nfrompydantic_ai.messagesimport (\n\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\n\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\n\nfromfake_databaseimport DatabaseConn\n\nfromweather_appimport run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\n\nmodels.ALLOW_MODEL_REQUESTS = False\n\ndefcall_weather_forecast(  \nWe define a function call_weather_forecast that will be called by FunctionModel\nin place of the LLM, this function has access to the list of\nModelMessage[](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.ModelMessage)s\nthat make up the run, and\nAgentInfo[](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.AgentInfo)\nwhich contains information about the agent and the function tools and return\ntools.\n\n[](https://ai.pydantic.dev/testing/#__code_2_annotation_1)\n\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  \nOur function is slightly intelligent in that it tries to extract a date from the\nprompt, but just hard codes the location.\n\n[](https://ai.pydantic.dev/testing/#__code_2_annotation_2)\n\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\nasync deftest_forecast_future():\n\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  \nWe use\nFunctionModel[](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel)\nto replace the agent's model with our custom function.\n\n[](https://ai.pydantic.dev/testing/#__code_2_annotation_3)\n\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n\n```\n\n### Overriding model via pytest fixtures\n\nIf you're writing lots of tests that all require model to be overridden, you can\nuse [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override\nthe model with\n[`TestModel`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel)\nor\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel)\nin a reusable way.\n\nHere's an example of a fixture that overrides the model with `TestModel`:\n\ntest_agent.py```\n\nimportpytest\n\nfromweather_appimport weather_agent\n\nfrompydantic_ai.models.testimport TestModel\n\n@pytest.fixture\n\ndefoverride_weather_agent():\n\n    with weather_agent.override(model=TestModel()):\n        yield\n\nasync deftest_forecast(override_weather_agent: None):\n\n    ...\n    # test code here",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/gemini/#gemini)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nGemini\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * Gemini  [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n        * [ Gemini via Generative Language API  ](https://ai.pydantic.dev/models/gemini/#gemini-via-generative-language-api)\n          * [ Install  ](https://ai.pydantic.dev/models/gemini/#install)\n          * [ Configuration  ](https://ai.pydantic.dev/models/gemini/#configuration)\n          * [ Environment variable  ](https://ai.pydantic.dev/models/gemini/#environment-variable)\n          * [ provider argument  ](https://ai.pydantic.dev/models/gemini/#provider-argument)\n        * [ Gemini via VertexAI  ](https://ai.pydantic.dev/models/gemini/#gemini-via-vertexai)\n          * [ Install  ](https://ai.pydantic.dev/models/gemini/#install_1)\n          * [ Configuration  ](https://ai.pydantic.dev/models/gemini/#configuration_1)\n          * [ Application default credentials  ](https://ai.pydantic.dev/models/gemini/#application-default-credentials)\n          * [ Service account  ](https://ai.pydantic.dev/models/gemini/#service-account)\n          * [ Customizing region  ](https://ai.pydantic.dev/models/gemini/#customizing-region)\n          * [ Model settings  ](https://ai.pydantic.dev/models/gemini/#model-settings)\n            * [ Disable thinking  ](https://ai.pydantic.dev/models/gemini/#disable-thinking)\n            * [ Safety settings  ](https://ai.pydantic.dev/models/gemini/#safety-settings)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools",
  "](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Gemini via Generative Language API  ](https://ai.pydantic.dev/models/gemini/#gemini-via-generative-language-api)\n    * [ Install  ](https://ai.pydantic.dev/models/gemini/#install)\n    * [ Configuration  ](https://ai.pydantic.dev/models/gemini/#configuration)\n    * [ Environment variable  ](https://ai.pydantic.dev/models/gemini/#environment-variable)\n    * [ provider argument  ](https://ai.pydantic.dev/models/gemini/#provider-argument)\n  * [ Gemini via VertexAI  ](https://ai.pydantic.dev/models/gemini/#gemini-via-vertexai)\n    * [ Install  ](https://ai.pydantic.dev/models/gemini/#install_1)\n    * [ Configuration  ](https://ai.pydantic.dev/models/gemini/#configuration_1)\n    * [ Application default credentials  ](https://ai.pydantic.dev/models/gemini/#application-default-credentials)\n    * [ Service account  ](https://ai.pydantic.dev/models/gemini/#service-account)\n    * [ Customizing region  ](https://ai.pydantic.dev/models/gemini/#customizing-region)\n    * [ Model settings  ](https://ai.pydantic.dev/models/gemini/#model-settings)\n      * [ Disable thinking  ](https://ai.pydantic.dev/models/gemini/#disable-thinking)\n      * [ Safety settings  ](https://ai.pydantic.dev/models/gemini/#safety-settings)\n\n# Gemini\n\nNote\n\nWe've developed a new Google model called `GoogleModel` which uses `google-\ngenai` under the hood.\n\nHonestly, Google packages are a mess, and that's why we've used plain `httpx`\ninstead of relying on their own client to create `GeminiModel`. That said, it's\neasier to use the `google-genai` package directly, since they keep the package\nup-to-date with the latest API changes. For that reason, we've created a new\nmodel called `GoogleModel` which uses `google-genai` under the hood.\n\nCheck it out [here](https://ai.pydantic.dev/api/models/google/).\n\nPydanticAI supports Google's Gemini models through two different APIs:\n\n  * Generative Language API (`generativelanguage.googleapis.com`)\n  * Vertex AI API (`*-aiplatform.googleapis.com`)",
  "## Gemini via Generative Language API\n\n### Install\n\nTo use `GeminiModel` models, you just need to install `pydantic-ai` or\n`pydantic-ai-slim`, no extra dependencies are required.\n\n### Configuration\n\n`GeminiModel` lets you use Google's Gemini models through their [Generative\nLanguage API](https://ai.google.dev/api/all-methods),\n`generativelanguage.googleapis.com`.\n\n`GeminiModelName` contains a list of available Gemini models that can be used\nthrough this interface.\n\nTo use `GeminiModel`, go to\n[aistudio.google.com](https://aistudio.google.com/apikey) and select \"Create API\nkey\".\n\n### Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportGEMINI_API_KEY=your-api-key\n\n```\n\nYou can then use `GeminiModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('google-gla:gemini-2.0-flash')\n\n...\n\n```\n\nNote\n\nThe `google-gla` provider prefix represents the [Google **G** enerative **L**\nanguage **A** PI](https://ai.google.dev/api/all-methods) for `GeminiModel`s.\n`google-vertex` is used with [Vertex AI](https://cloud.google.com/vertex-\nai/generative-ai/docs/learn/models).\n\nOr initialise the model directly with just the model name and provider:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nmodel = GeminiModel('gemini-2.0-flash', provider='google-gla')\n\nagent = Agent(model)\n\n...\n\n```\n\n###  `provider` argument\n\nYou can provide a custom `Provider` via the `provider` argument:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_glaimport GoogleGLAProvider\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash', provider=GoogleGLAProvider(api_key='your-api-key')\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize the `GoogleGLAProvider` with a custom `http_client`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_glaimport GoogleGLAProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash',\n    provider=GoogleGLAProvider(api_key='your-api-key', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n## Gemini via VertexAI\n\nIf you are an enterprise user, you should use the `google-vertex` provider with\n`GeminiModel` which uses the `*-aiplatform.googleapis.com` API.\n\n`GeminiModelName` contains a list of available Gemini models that can be used\nthrough this interface.\n\n### Install\n\nTo use the `google-vertex` provider with `GeminiModel`, you need to either\ninstall `pydantic-ai`, or install `pydantic-ai-slim` with the `vertexai`\noptional group:\n\n[pip](https://ai.pydantic.dev/models/gemini/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/gemini/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[vertexai]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[vertexai]\"\n\n```\n\n### Configuration\n\nThis interface has a number of advantages over\n`generativelanguage.googleapis.com` documented above:\n\n  1. The VertexAI API comes with more enterprise readiness guarantees.\n  2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with VertexAI to guarantee capacity.\n  3. If you're running PydanticAI inside GCP, you don't need to set up authentication, it should \"just work\".\n  4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.\n\nThe big disadvantage is that for local development you may need to create and\nconfigure a \"service account\", which can be challenging to get right.\n\nWhichever way you authenticate, you'll need to have VertexAI enabled in your GCP\naccount.\n\n### Application default credentials\n\nLuckily if you're running PydanticAI inside GCP, or you have the [`gcloud`\nCLI](https://cloud.google.com/sdk/gcloud) installed and configured, you should\nbe able to use `VertexAIModel` without any additional setup.\n\nTo use `VertexAIModel`, with [application default\ncredentials](https://cloud.google.com/docs/authentication/application-default-\ncredentials) configured (e.g. with `gcloud`), you can simply use:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nmodel = GeminiModel('gemini-2.0-flash', provider='google-vertex')\n\nagent = Agent(model)\n\n...",
  "```\n\nInternally this uses [`google.auth.default()`](https://google-\nauth.readthedocs.io/en/master/reference/google.auth.html) from the `google-auth`\npackage to obtain credentials.\n\nWon't fail until `agent.run()`\n\nBecause `google.auth.default()` requires network requests and can be slow, it's\nnot run until you call `agent.run()`.\n\nYou may also need to pass the `project_id` argument to `GoogleVertexProvider` if\napplication default credentials don't set a project, if you pass `project_id`\nand it conflicts with the project set by application default credentials, an\nerror is raised.\n\n### Service account\n\nIf instead of application default credentials, you want to authenticate with a\nservice account, you'll need to create a service account, add it to your GCP\nproject (note: this step is necessary even if you created the service account\nwithin the project), give that service account the \"Vertex AI Service Agent\"\nrole, and download the service account JSON file.\n\nOnce you have the JSON file, you can use it thus:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_verteximport GoogleVertexProvider\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash',\n    provider=GoogleVertexProvider(service_account_file='path/to/service-account.json'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nAlternatively, if you already have the service account information in memory,\nyou can pass it as a dictionary:\n\n```\n\nimportjson\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_verteximport GoogleVertexProvider\n\nservice_account_info = json.loads(\n\n    '{\"type\": \"service_account\", \"project_id\": \"my-project-id\"}'\n)\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash',\n    provider=GoogleVertexProvider(service_account_info=service_account_info),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Customizing region\n\nWhichever way you authenticate, you can specify which region requests will be\nsent to via the `region` argument.\n\nUsing a region close to your application can improve latency and might be\nimportant from a regulatory perspective.\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_verteximport GoogleVertexProvider\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash', provider=GoogleVertexProvider(region='asia-east1')\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize the `GoogleVertexProvider` with a custom `http_client`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel\n\nfrompydantic_ai.providers.google_verteximport GoogleVertexProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = GeminiModel(\n\n    'gemini-2.0-flash',\n    provider=GoogleVertexProvider(region='asia-east1', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Model settings\n\nYou can use the\n[`GeminiModelSettings`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModelSettings)\nclass to customize the model request.\n\n#### Disable thinking\n\nYou can disable thinking by setting the `thinking_budget` to `0` on the\n`google_thinking_config`:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel, GeminiModelSettings\n\nmodel_settings = GeminiModelSettings(gemini_thinking_config={'thinking_budget':\n0})\n\nmodel = GeminiModel('gemini-2.0-flash')\n\nagent = Agent(model, model_settings=model_settings)\n\n...\n\n```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking)\nfor more on thinking.\n\n#### Safety settings\n\nYou can customize the safety settings by setting the `google_safety_settings`\nfield.\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.geminiimport GeminiModel, GeminiModelSettings\n\nmodel_settings = GeminiModelSettings(\n\n    gemini_safety_settings=[\n        {\n            'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n            'threshold': 'BLOCK_ONLY_HIGH',\n        }\n    ]\n)\n\nmodel = GeminiModel('gemini-2.0-flash')\n\nagent = Agent(model, model_settings=model_settings)\n\n...",
  "```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-\nsettings) for more on safety settings.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/troubleshooting/#troubleshooting)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nTroubleshooting\n\nType to start searching\n\n[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai \"Go to\nrepository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai \"Go to\nrepository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * Troubleshooting  [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n    * [ Jupyter Notebook Errors  ](https://ai.pydantic.dev/troubleshooting/#jupyter-notebook-errors)\n      * [ RuntimeError: This event loop is already running  ](https://ai.pydantic.dev/troubleshooting/#runtimeerror-this-event-loop-is-already-running)\n    * [ API Key Configuration  ](https://ai.pydantic.dev/troubleshooting/#api-key-configuration)\n      * [ UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable  ](https://ai.pydantic.dev/troubleshooting/#usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable)\n    * [ Monitoring HTTPX Requests  ](https://ai.pydantic.dev/troubleshooting/#monitoring-httpx-requests)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_a",
  "i.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Jupyter Notebook Errors  ](https://ai.pydantic.dev/troubleshooting/#jupyter-notebook-errors)\n    * [ RuntimeError: This event loop is already running  ](https://ai.pydantic.dev/troubleshooting/#runtimeerror-this-event-loop-is-already-running)\n  * [ API Key Configuration  ](https://ai.pydantic.dev/troubleshooting/#api-key-configuration)\n    * [ UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable  ](https://ai.pydantic.dev/troubleshooting/#usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable)\n  * [ Monitoring HTTPX Requests  ](https://ai.pydantic.dev/troubleshooting/#monitoring-httpx-requests)\n\n# Troubleshooting\n\nBelow are suggestions on how to fix some common errors you might encounter while\nusing PydanticAI. If the issue you're experiencing is not listed below or\naddressed in the documentation, please feel free to ask in the [Pydantic\nSlack](https://ai.pydantic.dev/help/) or create an issue on\n[GitHub](https://github.com/pydantic/pydantic-ai/issues).\n\n## Jupyter Notebook Errors\n\n### `RuntimeError: This event loop is already running`\n\nThis error is caused by conflicts between the event loops in Jupyter notebook\nand PydanticAI's. One way to manage these conflicts is by using [`nest-\nasyncio`](https://pypi.org/project/nest-asyncio/). Namely, before you execute\nany agent runs, do the following:\n\n```\n\nimportnest_asyncio\n\nnest_asyncio.apply()",
  "```\n\nNote: This fix also applies to Google Colab and\n[Marimo](https://github.com/marimo-team/marimo).\n\n## API Key Configuration\n\n### `UserError: API key must be provided or set in the [MODEL]_API_KEY\nenvironment variable`\n\nIf you're running into issues with setting the API key for your model, visit the\n[Models](https://ai.pydantic.dev/models/) page to learn more about how to set an\nenvironment variable and/or pass in an `api_key` argument.\n\n## Monitoring HTTPX Requests\n\nYou can use custom `httpx` clients in your models in order to access specific\nrequests, responses, and headers at runtime.\n\nIt's particularly helpful to use `logfire`'s [HTTPX\nintegration](https://ai.pydantic.dev/logfire/#monitoring-http-requests) to\nmonitor the above.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/tools/#function-tools)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nFunction Tools\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * Function Tools  [ Function Tools  ](https://ai.pydantic.dev/tools/)\n      * [ Registering Function Tools via Decorator  ](https://ai.pydantic.dev/tools/#registering-function-tools-via-decorator)\n      * [ Registering Function Tools via Agent Argument  ](https://ai.pydantic.dev/tools/#registering-function-tools-via-agent-argument)\n      * [ Function Tool Output  ](https://ai.pydantic.dev/tools/#function-tool-output)\n      * [ Function Tools vs. Structured Outputs  ](https://ai.pydantic.dev/tools/#function-tools-vs-structured-outputs)\n      * [ Function tools and schema  ](https://ai.pydantic.dev/tools/#function-tools-and-schema)\n      * [ Dynamic Function tools  ](https://ai.pydantic.dev/tools/#tool-prepare)\n      * [ Agent-wide Dynamic Tool Preparation  ](https://ai.pydantic.dev/tools/#prepare-tools)\n      * [ Tool Execution and Retries  ](https://ai.pydantic.dev/tools/#tool-retries)\n      * [ Use LangChain Tools  ](https://ai.pydantic.dev/tools/#langchain-tools)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/ex",
  "amples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Registering Function Tools via Decorator  ](https://ai.pydantic.dev/tools/#registering-function-tools-via-decorator)\n  * [ Registering Function Tools via Agent Argument  ](https://ai.pydantic.dev/tools/#registering-function-tools-via-agent-argument)\n  * [ Function Tool Output  ](https://ai.pydantic.dev/tools/#function-tool-output)\n  * [ Function Tools vs. Structured Outputs  ](https://ai.pydantic.dev/tools/#function-tools-vs-structured-outputs)\n  * [ Function tools and schema  ](https://ai.pydantic.dev/tools/#function-tools-and-schema)\n  * [ Dynamic Function tools  ](https://ai.pydantic.dev/tools/#tool-prepare)\n  * [ Agent-wide Dynamic Tool Preparation  ](https://ai.pydantic.dev/tools/#prepare-tools)\n  * [ Tool Execution and Retries  ](https://ai.pydantic.dev/tools/#tool-retries)\n  * [ Use LangChain Tools  ](https://ai.pydantic.dev/tools/#langchain-tools)\n\n# Function Tools\n\nFunction tools provide a mechanism for models to retrieve extra information to\nhelp them generate a response.\n\nThey're useful when you want to enable the model to take some action and use the\nresult, when it is impractical or impossible to put all the context an agent\nmight need into the system prompt, or when you want to make agents' behavior\nmore deterministic or reliable by deferring some of the logic required to\ngenerate a response to another (not necessarily AI-powered) tool.\n\nIf you want a model to be able to call a function as its final action, without\nthe result being sent back to the model, you can use an [output\nfunction](https://ai.pydantic.dev/output/#output-functions) instead.\n\nFunction tools vs. RAG",
  "Function tools are basically the \"R\" of RAG (Retrieval-Augmented Generation) —\nthey augment what the model can do by letting it request extra information.\n\nThe main semantic difference between PydanticAI Tools and RAG is RAG is\nsynonymous with vector search, while PydanticAI tools are more general-purpose.\n(Note: we may add support for vector search functionality in the future,\nparticularly an API for generating embeddings. See\n[#58](https://github.com/pydantic/pydantic-ai/issues/58))\n\nThere are a number of ways to register tools with an agent:\n\n  * via the [`@agent.tool`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool) decorator — for tools that need access to the agent [context](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\n  * via the [`@agent.tool_plain`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool_plain) decorator — for tools that do not need access to the agent [context](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\n  * via the [`tools`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__) keyword argument to `Agent` which can take either plain functions, or instances of [`Tool`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.Tool)\n\n## Registering Function Tools via Decorator\n\n`@agent.tool` is considered the default decorator since in the majority of cases\ntools will need access to the agent context.\n\nHere's an example using both:\n\ndice_game.py```\n\nimportrandom\n\nfrompydantic_aiimport Agent, RunContext\n\nagent = Agent(\n\n    'google-gla:gemini-1.5-flash',  \nThis is a pretty simple task, so we can use the fast and cheap Gemini flash\nmodel.\n\n[](https://ai.pydantic.dev/tools/#__code_0_annotation_1)\n\n    deps_type=str,  \nWe pass the user's name as the dependency, to keep things simple we use just the\nname as a string as the dependency.\n\n[](https://ai.pydantic.dev/tools/#__code_0_annotation_2)\n\n    system_prompt=(\n        \"You're a dice game, you should roll the die and see if the number \"\n        \"you get back matches the user's guess. If so, tell them they're a winner. \"\n        \"Use the player's name in the response.\"\n    ),\n)\n\n@agent.tool_plain  \nThis tool doesn't need any context, it just returns a random number. You could\nprobably use a dynamic system prompt in this case.\n\n[](https://ai.pydantic.dev/tools/#__code_0_annotation_3)\n\ndefroll_dice() -> str:\n\n\"\"\"Roll a six-sided die and return the result.\"\"\"\n\n    return str(random.randint(1, 6))\n\n@agent.tool  \nThis tool needs the player's name, so it uses RunContext to access dependencies\nwhich are just the player's name in this case.\n\n[](https://ai.pydantic.dev/tools/#__code_0_annotation_4)\n\ndefget_player_name(ctx: RunContext[str]) -> str:\n\n\"\"\"Get the player's name.\"\"\"\n\n    return ctx.deps\n\ndice_result = agent.run_sync('My guess is 4', deps='Anne')  \nRun the agent, passing the player's name as the dependency.\n\n[](https://ai.pydantic.dev/tools/#__code_0_annotation_5)\n\nprint(dice_result.output)\n\n#> Congratulations Anne, you guessed correctly! You're a winner!\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nLet's print the messages from that game to see what happened:\n\ndice_game_messages.py",
  "```\n\nfromdice_gameimport dice_result\n\nprint(dice_result.all_messages())\n\n\"\"\"\n\n[\n\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content=\"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response.\",\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='My guess is 4',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='roll_dice', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=90, response_tokens=2, total_tokens=92),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='roll_dice',\n                content='4',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='get_player_name', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=91, response_tokens=4, total_tokens=95),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='get_player_name',\n                content='Anne',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content=\"Congratulations Anne, you guessed correctly! You're a winner!\"\n            )\n        ],\n        usage=Usage(\n            requests=1, request_tokens=92, response_tokens=12, total_tokens=104\n        ),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\n\"\"\"\n\n```\n\nWe can represent this with a diagram:\n\n```\n\nsequenceDiagram\n\n    participant Agent\n    participant LLM\n\n    Note over Agent: Send prompts\n    Agent ->> LLM: System: \"You're a dice game...\"<br>User: \"My guess is 4\"\n    activate LLM\n    Note over LLM: LLM decides to use<br>a tool\n\n    LLM ->> Agent: Call tool<br>roll_dice()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Rolls a six-sided die\n\n    Agent -->> LLM: ToolReturn<br>\"4\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM decides to use<br>another tool\n\n    LLM ->> Agent: Call tool<br>get_player_name()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Retrieves player name\n    Agent -->> LLM: ToolReturn<br>\"Anne\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM constructs final response\n\n    LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\n    deactivate LLM\n    Note over Agent: Game session complete\n```\n\n## Registering Function Tools via Agent Argument\n\nAs well as using the decorators, we can register tools via the `tools` argument\nto the [`Agent`\nconstructor](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__).\nThis is useful when you want to reuse tools, and can also give more fine-grained\ncontrol over the tools.\n\ndice_game_tool_kwarg.py",
  "```\n\nimportrandom\n\nfrompydantic_aiimport Agent, RunContext, Tool\n\nsystem_prompt = \"\"\"\\\n\nYou're a dice game, you should roll the die and see if the number\n\nyou get back matches the user's guess. If so, tell them they're a winner.\n\nUse the player's name in the response.\n\n\"\"\"\n\ndefroll_dice() -> str:\n\n\"\"\"Roll a six-sided die and return the result.\"\"\"\n\n    return str(random.randint(1, 6))\n\ndefget_player_name(ctx: RunContext[str]) -> str:\n\n\"\"\"Get the player's name.\"\"\"\n\n    return ctx.deps\n\nagent_a = Agent(\n\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[roll_dice, get_player_name],  \nThe simplest way to register tools via the Agent constructor is to pass a list\nof functions, the function signature is inspected to determine if the tool takes\nRunContext[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext).\n\n[](https://ai.pydantic.dev/tools/#__code_2_annotation_1)\n\n    system_prompt=system_prompt,\n)\n\nagent_b = Agent(\n\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[  \n\nagent_a and agent_b are identical — but we can use\nTool[](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.Tool) to reuse tool\ndefinitions and give more fine-grained control over how tools are defined, e.g.\nsetting their name or description, or using a custom\nprepare[](https://ai.pydantic.dev/tools/#tool-prepare) method.\n\n[](https://ai.pydantic.dev/tools/#__code_2_annotation_2)\n\n        Tool(roll_dice, takes_ctx=False),\n        Tool(get_player_name, takes_ctx=True),\n    ],\n    system_prompt=system_prompt,\n)\n\ndice_result = {}\n\ndice_result['a'] = agent_a.run_sync('My guess is 6', deps='Yashar')\n\ndice_result['b'] = agent_b.run_sync('My guess is 4', deps='Anne')\n\nprint(dice_result['a'].output)\n\n#> Tough luck, Yashar, you rolled a 4. Better luck next time.\n\nprint(dice_result['b'].output)\n\n#> Congratulations Anne, you guessed correctly! You're a winner!\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Function Tool Output\n\nTools can return anything that Pydantic can serialize to JSON, as well as audio,\nvideo, image or document content depending on the types of [multi-modal\ninput](https://ai.pydantic.dev/input/) the model supports:\n\nfunction_tool_output.py```\n\nfromdatetimeimport datetime\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, DocumentUrl, ImageUrl\n\nfrompydantic_ai.models.openaiimport OpenAIResponsesModel\n\nclassUser(BaseModel):\n\n    name: str\n    age: int\n\nagent = Agent(model=OpenAIResponsesModel('gpt-4o'))\n\n@agent.tool_plain\n\ndefget_current_time() -> datetime:\n\n    return datetime.now()\n\n@agent.tool_plain\n\ndefget_user() -> User:\n\n    return User(name='John', age=30)\n\n@agent.tool_plain\n\ndefget_company_logo() -> ImageUrl:\n\n    return ImageUrl(url='https://iili.io/3Hs4FMg.png')\n\n@agent.tool_plain\n\ndefget_document() -> DocumentUrl:\n\n    return DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n\nresult = agent.run_sync('What time is it?')\n\nprint(result.output)\n\n#> The current time is 10:45 PM on April 17, 2025.\n\nresult = agent.run_sync('What is the user name?')\n\nprint(result.output)\n\n#> The user's name is John.\n\nresult = agent.run_sync('What is the company name in the logo?')\n\nprint(result.output)\n\n#> The company name in the logo is \"Pydantic.\"\n\nresult = agent.run_sync('What is the main content of the document?')\n\nprint(result.output)\n\n#> The document contains just the text \"Dummy PDF file.\"",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\nSome models (e.g. Gemini) natively support semi-structured return values, while\nsome expect text (OpenAI) but seem to be just as good at extracting meaning from\nthe data. If a Python object is returned and the model expects a string, the\nvalue will be serialized to JSON.\n\n## Function Tools vs. Structured Outputs\n\nAs the name suggests, function tools use the model's \"tools\" or \"functions\" API\nto let the model know what is available to call. Tools or functions are also\nused to define the schema(s) for structured responses, thus a model might have\naccess to many tools, some of which call function tools while others end the run\nand produce a final output.\n\n## Function tools and schema\n\nFunction parameters are extracted from the function signature, and all\nparameters except `RunContext` are used to build the schema for that tool call.\n\nEven better, PydanticAI extracts the docstring from functions and (thanks to\n[griffe](https://mkdocstrings.github.io/griffe/)) extracts parameter\ndescriptions from the docstring and adds them to the schema.\n\n[Griffe\nsupports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings)\nextracting parameter descriptions from `google`, `numpy`, and `sphinx` style\ndocstrings. PydanticAI will infer the format to use based on the docstring, but\nyou can explicitly set it using\n[`docstring_format`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.DocstringFormat).\nYou can also enforce parameter requirements by setting\n`require_parameter_descriptions=True`. This will raise a\n[`UserError`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.UserError)\nif a parameter description is missing.\n\nTo demonstrate a tool's schema, here we use\n[`FunctionModel`](https://ai.pydantic.dev/api/models/function/#pydantic_ai.models.function.FunctionModel)\nto print the schema a model would receive:\n\ntool_schema.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.messagesimport ModelMessage, ModelResponse, TextPart\n\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\n\nagent = Agent()\n\n@agent.tool_plain(docstring_format='google',\nrequire_parameter_descriptions=True)\n\ndeffoobar(a: int, b: str, c: dict[str, list[float]]) -> str:\n\n\"\"\"Get me foobar.\n\n    Args:\n        a: apple pie\n        b: banana cake\n        c: carrot smoothie\n    \"\"\"\n    return f'{a}{b}{c}'\n\ndefprint_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n\n    tool = info.function_tools[0]\n    print(tool.description)\n    #> Get me foobar.\n    print(tool.parameters_json_schema)\n\"\"\"\n\n    {\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'apple pie', 'type': 'integer'},\n            'b': {'description': 'banana cake', 'type': 'string'},\n            'c': {\n                'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'},\n                'description': 'carrot smoothie',\n                'type': 'object',\n            },\n        },\n        'required': ['a', 'b', 'c'],\n        'type': 'object',\n    }\n    \"\"\"\n    return ModelResponse(parts=[TextPart('foobar')])\n\nagent.run_sync('hello', model=FunctionModel(print_schema))\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nIf a tool has a single parameter that can be represented as an object in JSON\nschema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is\nsimplified to be just that object.\n\nHere's an example where we use\n[`TestModel.last_model_request_parameters`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel.last_model_request_parameters)\nto inspect the tool schema that would be passed to the model.\n\nsingle_parameter_tool.py```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.testimport TestModel\n\nagent = Agent()\n\nclassFoobar(BaseModel):\n\n\"\"\"This is a Foobar\"\"\"\n\n    x: int\n    y: str\n    z: float = 3.14\n\n@agent.tool_plain\n\ndeffoobar(f: Foobar) -> str:\n\n    return str(f)\n\ntest_model = TestModel()\n\nresult = agent.run_sync('hello', model=test_model)\n\nprint(result.output)\n\n#> {\"foobar\":\"x=0 y='a' z=3.14\"}\n\nprint(test_model.last_model_request_parameters.function_tools)\n\n\"\"\"\n\n[\n\n    ToolDefinition(\n        name='foobar',\n        description='This is a Foobar',\n        parameters_json_schema={\n            'properties': {\n                'x': {'type': 'integer'},\n                'y': {'type': 'string'},\n                'z': {'default': 3.14, 'type': 'number'},\n            },\n            'required': ['x', 'y'],\n            'title': 'Foobar',\n            'type': 'object',\n        },\n    )\n]\n\n\"\"\"",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\nIf you have a function that lacks appropriate documentation (i.e. poorly named,\nno type information, poor docstring, use of _args or *_ kwargs and suchlike)\nthen you can still turn it into a tool that can be effectively used by the agent\nwith the `Tool.from_schema` function. With this you provide the name,\ndescription and JSON schema for the function directly:\n\n```\n\nfrompydantic_aiimport Agent, Tool\n\nfrompydantic_ai.models.testimport TestModel\n\ndeffoobar(**kwargs) -> str:\n\n    return kwargs['a'] + kwargs['b']\n\ntool = Tool.from_schema(\n\n    function=foobar,\n    name='sum',\n    description='Sum two numbers.',\n    json_schema={\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'the first number', 'type': 'integer'},\n            'b': {'description': 'the second number', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'type': 'object',\n    }\n)\n\ntest_model = TestModel()\n\nagent = Agent(test_model, tools=[tool])\n\nresult = agent.run_sync('testing...')\n\nprint(result.output)\n\n#> {\"sum\":0}\n\n```\n\nPlease note that validation of the tool arguments will not be performed, and\nthis will pass all arguments as keyword arguments.\n\n## Dynamic Function tools\n\nTools can optionally be defined with another function: `prepare`, which is\ncalled at each step of a run to customize the definition of the tool passed to\nthe model, or omit the tool completely from that step.\n\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool\nregistration mechanisms:\n\n  * [`@agent.tool`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool) decorator\n  * [`@agent.tool_plain`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.tool_plain) decorator\n  * [`Tool`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.Tool) dataclass\n\nThe `prepare` method, should be of type\n[`ToolPrepareFunc`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.ToolPrepareFunc),\na function which takes\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nand a pre-built\n[`ToolDefinition`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.ToolDefinition),\nand should either return that `ToolDefinition` with or without modifying it,\nreturn a new `ToolDefinition`, or return `None` to indicate this tools should\nnot be registered for that step.\n\nHere's a simple `prepare` method that only includes the tool if the value of the\ndependency is `42`.\n\nAs with the previous example, we use\n[`TestModel`](https://ai.pydantic.dev/api/models/test/#pydantic_ai.models.test.TestModel)\nto demonstrate the behavior without calling a real model.\n\ntool_only_if_42.py```\n\nfromtypingimport Union\n\nfrompydantic_aiimport Agent, RunContext\n\nfrompydantic_ai.toolsimport ToolDefinition\n\nagent = Agent('test')\n\nasync defonly_if_42(\n\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> Union[ToolDefinition, None]:\n\n    if ctx.deps == 42:\n        return tool_def\n\n@agent.tool(prepare=only_if_42)\n\ndefhitchhiker(ctx: RunContext[int], answer: str) -> str:\n\n    return f'{ctx.deps}{answer}'\n\nresult = agent.run_sync('testing...', deps=41)\n\nprint(result.output)\n\n#> success (no tool calls)\n\nresult = agent.run_sync('testing...', deps=42)\n\nprint(result.output)\n\n#> {\"hitchhiker\":\"42 a\"}\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's a more complex example where we change the description of the `name`\nparameter to based on the value of `deps`\n\nFor the sake of variation, we create this tool using the\n[`Tool`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.Tool) dataclass.\n\ncustomize_name.py```\n\nfrom__future__import annotations\n\nfromtypingimport Literal\n\nfrompydantic_aiimport Agent, RunContext\n\nfrompydantic_ai.models.testimport TestModel\n\nfrompydantic_ai.toolsimport Tool, ToolDefinition\n\ndefgreet(name: str) -> str:\n\n    return f'hello {name}'\n\nasync defprepare_greet(\n\n    ctx: RunContext[Literal['human', 'machine']], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    d = f'Name of the {ctx.deps} to greet.'\n    tool_def.parameters_json_schema['properties']['name']['description'] = d\n    return tool_def\n\ngreet_tool = Tool(greet, prepare=prepare_greet)\n\ntest_model = TestModel()\n\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal['human',\n'machine'])\n\nresult = agent.run_sync('testing...', deps='human')\n\nprint(result.output)\n\n#> {\"greet\":\"hello a\"}\n\nprint(test_model.last_model_request_parameters.function_tools)\n\n\"\"\"\n\n[\n\n    ToolDefinition(\n        name='greet',\n        description='',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {\n                'name': {'type': 'string', 'description': 'Name of the human to greet.'}\n            },\n            'required': ['name'],\n            'type': 'object',\n        },\n    )\n]\n\n\"\"\"",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Agent-wide Dynamic Tool Preparation\n\nIn addition to per-tool `prepare` methods, you can also define an agent-wide\n`prepare_tools` function. This function is called at each step of a run and\nallows you to filter or modify the list of all tool definitions available to the\nagent for that step. This is especially useful if you want to enable or disable\nmultiple tools at once, or apply global logic based on the current context.\n\nThe `prepare_tools` function should be of type\n[`ToolsPrepareFunc`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.ToolsPrepareFunc),\nwhich takes the\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nand a list of\n[`ToolDefinition`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.ToolDefinition),\nand returns a new list of tool definitions (or `None` to disable all tools for\nthat step).\n\nNote\n\nThe list of tool definitions passed to `prepare_tools` includes both regular\ntools and tools from any MCP servers attached to the agent.\n\nHere's an example that makes all tools strict if the model is an OpenAI model:\n\nagent_prepare_tools_customize.py```\n\nfromdataclassesimport replace\n\nfromtypingimport Union\n\nfrompydantic_aiimport Agent, RunContext\n\nfrompydantic_ai.toolsimport ToolDefinition\n\nfrompydantic_ai.models.testimport TestModel\n\nasync defturn_on_strict_if_openai(\n\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> Union[list[ToolDefinition], None]:\n\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\ntest_model = TestModel()\n\nagent = Agent(test_model, prepare_tools=turn_on_strict_if_openai)\n\n@agent.tool_plain\n\ndefecho(message: str) -> str:\n\n    return message\n\nagent.run_sync('testing...')\n\nassert test_model.last_model_request_parameters.function_tools[0].strict is None\n\n# Set the system attribute of the test_model to 'openai'\n\ntest_model._system = 'openai'\n\nagent.run_sync('testing with openai...')\n\nassert test_model.last_model_request_parameters.function_tools[0].strict\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's another example that conditionally filters out the tools by name if the\ndependency (`ctx.deps`) is `True`:\n\nagent_prepare_tools_filter_out.py```\n\nfromtypingimport Union\n\nfrompydantic_aiimport Agent, RunContext\n\nfrompydantic_ai.toolsimport Tool, ToolDefinition\n\ndeflaunch_potato(target: str) -> str:\n\n    return f'Potato launched at {target}!'\n\nasync deffilter_out_tools_by_name(\n\n    ctx: RunContext[bool], tool_defs: list[ToolDefinition]\n) -> Union[list[ToolDefinition], None]:\n\n    if ctx.deps:\n        return [tool_def for tool_def in tool_defs if tool_def.name != 'launch_potato']\n    return tool_defs\n\nagent = Agent(\n\n    'test',\n    tools=[Tool(launch_potato)],\n    prepare_tools=filter_out_tools_by_name,\n    deps_type=bool,\n)\n\nresult = agent.run_sync('testing...', deps=False)\n\nprint(result.output)\n\n#> {\"launch_potato\":\"Potato launched at a!\"}\n\nresult = agent.run_sync('testing...', deps=True)\n\nprint(result.output)\n\n#> success (no tool calls)\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nYou can use `prepare_tools` to:\n\n  * Dynamically enable or disable tools based on the current model, dependencies, or other context\n  * Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.)\n\nIf both per-tool `prepare` and agent-wide `prepare_tools` are used, the per-tool\n`prepare` is applied first to each tool, and then `prepare_tools` is called with\nthe resulting list of tool definitions.\n\n## Tool Execution and Retries\n\nWhen a tool is executed, its arguments (provided by the LLM) are first validated\nagainst the function's signature using Pydantic. If validation fails (e.g., due\nto incorrect types or missing required arguments), a `ValidationError` is\nraised, and the framework automatically generates a\n[`RetryPromptPart`](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.RetryPromptPart)\ncontaining the validation details. This prompt is sent back to the LLM,\ninforming it of the error and allowing it to correct the parameters and retry\nthe tool call.\n\nBeyond automatic validation errors, the tool's own internal logic can also\nexplicitly request a retry by raising the\n[`ModelRetry`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.ModelRetry)\nexception. This is useful for situations where the parameters were technically\nvalid, but an issue occurred during execution (like a transient network error,\nor the tool determining the initial attempt needs modification).\n\n```\n\nfrompydantic_aiimport ModelRetry\n\ndefmy_flaky_tool(query: str) -> str:\n\n    if query == 'bad':\n        # Tell the LLM the query was bad and it should try again\n        raise ModelRetry(\"The query 'bad' is not allowed. Please provide a different query.\")\n    # ... process query ...\n    return 'Success!'",
  "```\n\nRaising `ModelRetry` also generates a `RetryPromptPart` containing the exception\nmessage, which is sent back to the LLM to guide its next attempt. Both\n`ValidationError` and `ModelRetry` respect the `retries` setting configured on\nthe `Tool` or `Agent`.\n\n## Use LangChain Tools\n\nIf you'd like to use a tool from LangChain's [community tool\nlibrary](https://python.langchain.com/docs/integrations/tools/) with PydanticAI,\nyou can use the `pydancic_ai.ext.langchain.tool_from_langchain` convenience\nmethod. Note that PydanticAI will not validate the arguments in this case --\nit's up to the model to provide arguments matching the schema specified by the\nLangChain tool, and up to the LangChain tool to raise an error if the arguments\nare invalid.\n\nHere is how you can use it to augment model responses using a LangChain web\nsearch tool. This tool will need you to install the `langchain-community` and\n`duckduckgo-search` dependencies to work properly.\n\n```\n\nfromlangchain_community.toolsimport DuckDuckGoSearchRun\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.ext.langchainimport tool_from_langchain\n\nsearch = DuckDuckGoSearchRun()\n\nsearch_tool = tool_from_langchain(search)\n\nagent = Agent(\n\n    'google-gla:gemini-2.0-flash',  \nWhile this task is simple Gemini 1.5 didn't want to use the provided tool.\nGemini 2.0 is still fast and cheap.\n\n[](https://ai.pydantic.dev/tools/#__code_12_annotation_1)\n\n    tools=[search_tool],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')  \nThe release date of this game is the 30th of May 2025, which was confirmed after\nthe knowledge cutoff for Gemini 2.0 (August 2024).\n\n[](https://ai.pydantic.dev/tools/#__code_12_annotation_2)\n\nprint(result.output)\n\n#> Elden Ring Nightreign is planned to be released on May 30, 2025.",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/google/#google)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nGoogle\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * Google  [ Google  ](https://ai.pydantic.dev/models/google/)\n        * [ Install  ](https://ai.pydantic.dev/models/google/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/google/#configuration)\n          * [ API Key (Generative Language API)  ](https://ai.pydantic.dev/models/google/#api-key-generative-language-api)\n          * [ Vertex AI (Enterprise/Cloud)  ](https://ai.pydantic.dev/models/google/#vertex-ai-enterprisecloud)\n            * [ Application Default Credentials  ](https://ai.pydantic.dev/models/google/#application-default-credentials)\n            * [ Service Account  ](https://ai.pydantic.dev/models/google/#service-account)\n            * [ Customizing Location  ](https://ai.pydantic.dev/models/google/#customizing-location)\n        * [ Provider Argument  ](https://ai.pydantic.dev/models/google/#provider-argument)\n        * [ Model Settings  ](https://ai.pydantic.dev/models/google/#model-settings)\n        * [ Document, Image, Audio, and Video Input  ](https://ai.pydantic.dev/models/google/#document-image-audio-and-video-input)\n        * [ Model settings  ](https://ai.pydantic.dev/models/google/#model-settings_1)\n          * [ Disable thinking  ](https://ai.pydantic.dev/models/google/#disable-thinking)\n          * [ Safety settings  ](https://ai.pydantic.dev/models/google/#safety-settings)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api",
  "/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/google/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/google/#configuration)\n    * [ API Key (Generative Language API)  ](https://ai.pydantic.dev/models/google/#api-key-generative-language-api)\n    * [ Vertex AI (Enterprise/Cloud)  ](https://ai.pydantic.dev/models/google/#vertex-ai-enterprisecloud)\n      * [ Application Default Credentials  ](https://ai.pydantic.dev/models/google/#application-default-credentials)\n      * [ Service Account  ](https://ai.pydantic.dev/models/google/#service-account)\n      * [ Customizing Location  ](https://ai.pydantic.dev/models/google/#customizing-location)\n  * [ Provider Argument  ](https://ai.pydantic.dev/models/google/#provider-argument)\n  * [ Model Settings  ](https://ai.pydantic.dev/models/google/#model-settings)\n  * [ Document, Image, Audio, and Video Input  ](https://ai.pydantic.dev/models/google/#document-image-audio-and-video-input)\n  * [ Model settings  ](https://ai.pydantic.dev/models/google/#model-settings_1)\n    * [ Disable thinking  ](https://ai.pydantic.dev/models/google/#disable-thinking)\n    * [ Safety settings  ](https://ai.pydantic.dev/models/google/#safety-settings)\n\n# Google\n\nThe `GoogleModel` is a model that uses the [`google-\ngenai`](https://pypi.org/project/google-genai/) package under the hood to access\nGoogle's Gemini models via both the Generative Language API and Vertex AI.\n\n## Install\n\nTo use `GoogleModel`, you need to either install `pydantic-ai`, or install\n`pydantic-ai-slim` with the `google` optional group:\n\n[pip](https://ai.pydantic.dev/models/google/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/google/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[google]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[google]\"",
  "```\n\n* * *\nExplicit instantiation required\n\nYou **cannot** currently use `Agent('google-gla:gemini-1.5-flash')` or\n`Agent('google-vertex:gemini-1.5-flash')` directly with `GoogleModel`. The model\nname inference will select\n[`GeminiModel`](https://ai.pydantic.dev/models/gemini/) instead of\n`GoogleModel`.\n\nTo use `GoogleModel`, you **must** explicitly instantiate a\n[`GoogleProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.google.GoogleProvider)\nand pass it to\n[`GoogleModel`](https://ai.pydantic.dev/api/models/google/#pydantic_ai.models.google.GoogleModel),\nthen pass the model to\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent).\n\n* * *\n## Configuration\n\n`GoogleModel` lets you use Google's Gemini models through their [Generative\nLanguage API](https://ai.google.dev/api/all-methods)\n(`generativelanguage.googleapis.com`) or [Vertex AI\nAPI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models)\n(`*-aiplatform.googleapis.com`).\n\n### API Key (Generative Language API)\n\nTo use Gemini via the Generative Language API, go to\n[aistudio.google.com](https://aistudio.google.com/apikey) and create an API key.\n\nOnce you have the API key, set it as an environment variable:\n\n```\n\nexportGOOGLE_API_KEY=your-api-key\n\n```\n\nYou can then use `GoogleModel` by explicitly creating a provider:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel\n\nfrompydantic_ai.providers.googleimport GoogleProvider\n\nprovider = GoogleProvider(api_key='your-api-key')\n\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Vertex AI (Enterprise/Cloud)\n\nIf you are an enterprise user, you can use the `google-vertex` provider with\n`GoogleModel` to access Gemini via Vertex AI.\n\nTo use Vertex AI, you may need to set up [application default\ncredentials](https://cloud.google.com/docs/authentication/application-default-\ncredentials) or use a service account. You can also specify the region.\n\n#### Application Default Credentials\n\nIf you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) installed\nand configured, you can use:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel\n\nfrompydantic_ai.providers.googleimport GoogleProvider\n\nprovider = GoogleProvider(vertexai=True)\n\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\n\nagent = Agent(model)\n\n...\n\n```\n\n#### Service Account\n\nTo use a service account JSON file:\n\ngoogle_model_service_account.py```\n\nfromgoogle.oauth2import service_account\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel\n\nfrompydantic_ai.providers.googleimport GoogleProvider\n\ncredentials = service_account.Credentials.from_service_account_file(\n\n    'path/to/service-account.json',\n    scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\n\nprovider = GoogleProvider(credentials=credentials)\n\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\n\nagent = Agent(model)\n\n...\n\n```\n\n#### Customizing Location\n\nYou can specify the location when using Vertex AI:\n\ngoogle_model_location.py```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel\n\nfrompydantic_ai.providers.googleimport GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, location='asia-east1')\n\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\n\nagent = Agent(model)\n\n...\n\n```\n\n## Provider Argument\n\nYou can supply a custom `GoogleProvider` instance using the `provider` argument\nto configure advanced client options, such as setting a custom `base_url`.\n\nThis is useful if you're using a custom-compatible endpoint with the Google\nGenerative Language API.\n\n```\n\nfromgoogleimport genai\n\nfromgoogle.genai.typesimport HttpOptions\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel\n\nfrompydantic_ai.providers.googleimport GoogleProvider\n\nclient = genai.Client(\n\n    api_key='gemini-custom-api-key',\n    http_options=HttpOptions(base_url='gemini-custom-base-url'),\n)\n\nprovider = GoogleProvider(client=client)\n\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\n\nagent = Agent(model)\n\n...\n\n```\n\n## Model Settings\n\nYou can customize model behavior using\n[`GoogleModelSettings`](https://ai.pydantic.dev/api/models/google/#pydantic_ai.models.google.GoogleModelSettings):\n\n```\n\nfromgoogle.genai.typesimport HarmBlockThreshold, HarmCategory\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel, GoogleModelSettings\n\nsettings = GoogleModelSettings(\n\n    temperature=0.2,\n    max_tokens=1024,\n    google_thinking_config={'thinking_budget': 2048},\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\n\nmodel = GoogleModel('gemini-1.5-flash')\n\nagent = Agent(model, model_settings=settings)\n\n...",
  "```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings)\nfor more on safety settings, and [thinking config](https://ai.google.dev/gemini-\napi/docs/thinking).\n\n## Document, Image, Audio, and Video Input\n\n`GoogleModel` supports multi-modal input, including documents, images, audio,\nand video. See the [input documentation](https://ai.pydantic.dev/input/) for\ndetails and examples.\n\n## Model settings\n\nYou can use the\n[`GoogleModelSettings`](https://ai.pydantic.dev/api/models/google/#pydantic_ai.models.google.GoogleModelSettings)\nclass to customize the model request.\n\n### Disable thinking\n\nYou can disable thinking by setting the `thinking_budget` to `0` on the\n`google_thinking_config`:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(google_thinking_config={'thinking_budget':\n0})\n\nmodel = GoogleModel('gemini-2.0-flash')\n\nagent = Agent(model, model_settings=model_settings)\n\n...\n\n```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking)\nfor more on thinking.\n\n### Safety settings\n\nYou can customize the safety settings by setting the `google_safety_settings`\nfield.\n\n```\n\nfromgoogle.genai.typesimport HarmBlockThreshold, HarmCategory\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.googleimport GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(\n\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\n\nmodel = GoogleModel('gemini-2.0-flash')\n\nagent = Agent(model, model_settings=model_settings)\n\n...",
  "```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings)\nfor more on safety settings.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/output/#structured-output)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nOutput\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * Output  [ Output  ](https://ai.pydantic.dev/output/)\n      * [ Output data  ](https://ai.pydantic.dev/output/#structured-output)\n        * [ Output functions  ](https://ai.pydantic.dev/output/#output-functions)\n          * [ Text output  ](https://ai.pydantic.dev/output/#text-output)\n        * [ Output modes  ](https://ai.pydantic.dev/output/#output-modes)\n          * [ Tool Output  ](https://ai.pydantic.dev/output/#tool-output)\n          * [ Native Output  ](https://ai.pydantic.dev/output/#native-output)\n          * [ Prompted Output  ](https://ai.pydantic.dev/output/#prompted-output)\n        * [ Output validators  ](https://ai.pydantic.dev/output/#output-validator-functions)\n      * [ Streamed Results  ](https://ai.pydantic.dev/output/#streamed-results)\n        * [ Streaming Text  ](https://ai.pydantic.dev/output/#streaming-text)\n        * [ Streaming Structured Output  ](https://ai.pydantic.dev/output/#streaming-structured-output)\n      * [ Examples  ](https://ai.pydantic.dev/output/#examples)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev",
  "/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Output data  ](https://ai.pydantic.dev/output/#structured-output)\n    * [ Output functions  ](https://ai.pydantic.dev/output/#output-functions)\n      * [ Text output  ](https://ai.pydantic.dev/output/#text-output)\n    * [ Output modes  ](https://ai.pydantic.dev/output/#output-modes)\n      * [ Tool Output  ](https://ai.pydantic.dev/output/#tool-output)\n      * [ Native Output  ](https://ai.pydantic.dev/output/#native-output)\n      * [ Prompted Output  ](https://ai.pydantic.dev/output/#prompted-output)\n    * [ Output validators  ](https://ai.pydantic.dev/output/#output-validator-functions)\n  * [ Streamed Results  ](https://ai.pydantic.dev/output/#streamed-results)\n    * [ Streaming Text  ](https://ai.pydantic.dev/output/#streaming-text)\n    * [ Streaming Structured Output  ](https://ai.pydantic.dev/output/#streaming-structured-output)\n  * [ Examples  ](https://ai.pydantic.dev/output/#examples)\n\n# Output\n\n\"Output\" refers to the final value returned from [running an\nagent](https://ai.pydantic.dev/agents/#running-agents). This can be either plain\ntext, [structured data](https://ai.pydantic.dev/output/#structured-output), or\nthe result of a [function](https://ai.pydantic.dev/output/#output-functions)\ncalled with arguments provided by the model.\n\nThe output is wrapped in\n[`AgentRunResult`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.AgentRunResult)\nor\n[`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult)\nso that you can access other data, like\n[usage](https://ai.pydantic.dev/api/usage/#pydantic_ai.usage.Usage) of the run\nand [message history](https://ai.pydantic.dev/message-history/#accessing-\nmessages-from-results).\n\nBoth `AgentRunResult` and `StreamedRunResult` are generic in the data they wrap,\nso typing information about the data returned by the agent is preserved.\n\nA run ends when the model responds with one of the structured output types, or,\nif no output type is specified or `str` is one of the allowed options, when a\nplain text response is received. A run can also be cancelled if usage limits are\nexceeded, see [Usage Limits](https://ai.pydantic.dev/agents/#usage-limits).",
  "Here's an example using a Pydantic model as the `output_type`, forcing the model\nto respond with data matching our specification:\n\nolympics.py```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\nclassCityLocation(BaseModel):\n\n    city: str\n    country: str\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\n\nprint(result.output)\n\n#> city='London' country='United Kingdom'\n\nprint(result.usage())\n\n#> Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Output data\n\nThe [`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) class\nconstructor takes an `output_type` argument that takes one or more types or\n[output functions](https://ai.pydantic.dev/output/#output-functions). It\nsupports simple scalar types, list and dict types, dataclasses and Pydantic\nmodels, as well as type unions -- generally everything supported as type hints\nin a Pydantic model. You can also pass a list of multiple choices.\n\nBy default, Pydantic AI leverages the model's tool calling capability to make it\nreturn structured data. When multiple output types are specified (in a union or\nlist), each member is registered with the model as a separate output tool in\norder to reduce the complexity of the schema and maximise the chances a model\nwill respond correctly. This has been shown to work well across a wide range of\nmodels. If you'd like to change the names of the output tools, use a model's\nnative structured output feature, or pass the output schema to the model in its\n[instructions](https://ai.pydantic.dev/agents/#instructions), you can use an\n[output mode](https://ai.pydantic.dev/output/#output-modes) marker class.\n\nWhen no output type is specified, or when `str` is among the output types, any\nplain text response from the model will be used as the output data. If `str` is\nnot among the output types, the model is forced to return structured data or\ncall an output function.\n\nIf the output type schema is not of type `\"object\"` (e.g. it's `int` or\n`list[int]`), the output type is wrapped in a single element object, so the\nschema of all tools registered with the model are object schemas.\n\nStructured outputs (like tools) use Pydantic to build the JSON schema used for\nthe tool, and to validate the data returned by the model.\n\nType checking considerations\n\nThe Agent class is generic in its output type, and this type is carried through\nto `AgentRunResult.output` and `StreamedRunResult.output` so that your IDE or\nstatic type checker can warn you when your code doesn't properly take into\naccount all the possible values those outputs could have.\n\nStatic type checkers like pyright and mypy will do their best the infer the\nagent's output type from the `output_type` you've specified, but they're not\nalways able to do so correctly when you provide functions or multiple types in a\nunion or list, even though PydanticAI will behave correctly. When this happens,\nyour type checker will complain even when you're confident you've passed a valid\n`output_type`, and you'll need to help the type checker by explicitly specifying\nthe generic parameters on the `Agent` constructor. This is shown in the second\nexample below and the output functions example further down.\n\nSpecifically, there are three valid uses of `output_type` where you'll need to\ndo this:\n\n  1. When using a union of types, e.g. `output_type=Foo | Bar`, or in older Python, `output_type=Union[Foo, Bar]`. Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for `output_type`. In addition to the generic parameters on the `Agent` constructor, you'll need to add `# type: ignore` to the line that passes the union to `output_type`. Alternatively, you can use a list: `output_type=[Foo, Bar]`.\n  2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you're passing in [output functions](https://ai.pydantic.dev/output/#output-functions). Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19142) with mypy to try and get this fixed.\n  3. With mypy: when using an async output function. Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19143) with mypy to try and get this fixed.\n\nHere's an example of returning either text or structured data:\n\nbox_or_error.py",
  "```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\nclassBox(BaseModel):\n\n    width: int\n    height: int\n    depth: int\n    units: str\n\nagent = Agent(\n\n    'openai:gpt-4o-mini',\n    output_type=[Box, str], \nThis could also have been a union: output_type=Box | str (or in older Python, output_type=Union[Box, str]). However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly.\n\n[](https://ai.pydantic.dev/output/#__code_1_annotation_1)\n\n    system_prompt=(\n        \"Extract me the dimensions of a box, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\n\nprint(result.output)\n\n#> Please provide the units for the dimensions (e.g., cm, in, m).\n\nresult = agent.run_sync('The box is 10x20x30 cm')\n\nprint(result.output)\n\n#> width=10 height=20 depth=30 units='cm'\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's an example of using a union return type, which will register multiple\noutput tools and wrap non-object schemas in an object:\n\ncolors_or_sizes.py```\n\nfromtypingimport Union\n\nfrompydantic_aiimport Agent\n\nagent = Agent[None, Union[list[str], list[int]]](\n\n    'openai:gpt-4o-mini',\n    output_type=Union[list[str], list[int]],  \nAs explained in the \"Type checking considerations\" section above, using a union\nrather than a list requires explicitly specifying the generic parameters on the\nAgent constructor and adding # type: ignore to this line in order to be type\nchecked correctly.\n\n[](https://ai.pydantic.dev/output/#__code_2_annotation_1)\n\n    system_prompt='Extract either colors or sizes from the shapes provided.',\n)\n\nresult = agent.run_sync('red square, blue circle, green triangle')\n\nprint(result.output)\n\n#> ['red', 'blue', 'green']\n\nresult = agent.run_sync('square size 10, circle size 20, triangle size 30')\n\nprint(result.output)\n\n#> [10, 20, 30]\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n### Output functions\n\nInstead of plain text or structured data, you may want the output of your agent\nrun to be the result of a function called with arguments provided by the model,\nfor example to further process or validate the data provided through the\narguments (with the option to tell the model to try again), or to hand off to\nanother agent.\n\nOutput functions are similar to [function\ntools](https://ai.pydantic.dev/tools/), but the model is forced to call one of\nthem, the call ends the agent run, and the result is not passed back to the\nmodel.\n\nAs with tool functions, output function arguments provided by the model are\nvalidated using Pydantic, they can optionally take\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext)\nas the first argument, and they can raise\n[`ModelRetry`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.ModelRetry)\nto ask the model to try again with modified arguments (or with a different\noutput type).\n\nTo specify output functions, you set the agent's `output_type` to either a\nsingle function (or bound instance method), or a list of functions. The list can\nalso contain other output types like simple scalars or entire Pydantic models.\nYou typically do not want to also register your output function as a tool (using\nthe `@agent.tool` decorator or `tools` argument), as this could confuse the\nmodel about which it should be calling.\n\nHere's an example of all of these features in action:\n\noutput_functions.py",
  "```\n\nimportre\n\nfromtypingimport Union\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, ModelRetry, RunContext\n\nfrompydantic_ai.exceptionsimport UnexpectedModelBehavior\n\nclassRow(BaseModel):\n\n    name: str\n    country: str\n\ntables = {\n\n    'capital_cities': [\n        Row(name='Amsterdam', country='Netherlands'),\n        Row(name='Mexico City', country='Mexico'),\n    ]\n}\n\nclassSQLFailure(BaseModel):\n\n\"\"\"An unrecoverable failure. Only use this when you can't change the query to\nmake it work.\"\"\"\n\n    explanation: str\n\ndefrun_sql_query(query: str) -> list[Row]:\n\n\"\"\"Run a SQL query on the database.\"\"\"\n\n    select_table = re.match(r'SELECT (.+) FROM (\\w+)', query)\n    if select_table:\n        column_names = select_table.group(1)\n        if column_names != '*':\n            raise ModelRetry(\"Only 'SELECT *' is supported, you'll have to do column filtering manually.\")\n\n        table_name = select_table.group(2)\n        if table_name not in tables:\n            raise ModelRetry(\n                f\"Unknown table '{table_name}' in query '{query}'. Available tables: {', '.join(tables.keys())}.\"\n            )\n\n        return tables[table_name]\n\n    raise ModelRetry(f\"Unsupported query: '{query}'.\")\n\nsql_agent = Agent[None, Union[list[Row], SQLFailure]](\n\n    'openai:gpt-4o',\n    output_type=[run_sql_query, SQLFailure],\n    instructions='You are a SQL agent that can run SQL queries on a database.',\n)\n\nasync defhand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\n\n\"\"\"I take natural language queries, turn them into SQL, and run them on a\ndatabase.\"\"\"\n\n    # Drop the final message with the output tool call, as it shouldn't be passed on to the SQL agent\n    messages = ctx.messages[:-1]\n    try:\n        result = await sql_agent.run(query, message_history=messages)\n        output = result.output\n        if isinstance(output, SQLFailure):\n            raise ModelRetry(f'SQL agent failed: {output.explanation}')\n        return output\n    except UnexpectedModelBehavior as e:\n        # Bubble up potentially retryable errors to the router agent\n        if (cause := e.__cause__) and hasattr(cause, 'tool_retry'):\n            raise ModelRetry(f'SQL agent failed: {cause.tool_retry.content}') frome\n        else:\n            raise\n\nclassRouterFailure(BaseModel):\n\n\"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\n\n    explanation: str\n\nrouter_agent = Agent[None, Union[list[Row], RouterFailure]](\n\n    'openai:gpt-4o',\n    output_type=[hand_off_to_sql_agent, RouterFailure],\n    instructions='You are a router to other agents. Never try to solve a problem yourself, just pass it on.',\n)\n\nresult = router_agent.run_sync('Select the names and countries of all capitals')\n\nprint(result.output)\n\n\"\"\"\n\n[\n\n    Row(name='Amsterdam', country='Netherlands'),\n    Row(name='Mexico City', country='Mexico'),\n]\n\n\"\"\"\n\nresult = router_agent.run_sync('Select all pets')\n\nprint(repr(result.output))\n\n\"\"\"\n\nRouterFailure(explanation=\"The requested table 'pets' does not exist in the\ndatabase. The only available table is 'capital_cities', which does not contain\ndata about pets.\")\n\n\"\"\"\n\nresult = router_agent.run_sync('How do I fly from Amsterdam to Mexico City?')\n\nprint(repr(result.output))\n\n\"\"\"\n\nRouterFailure(explanation='I am not equipped to provide travel information, such\nas flights from Amsterdam to Mexico City.')\n\n\"\"\"\n\n```\n\n#### Text output\n\nIf you provide an output function that takes a string, Pydantic AI will by\ndefault create an output tool like for any other output function. If instead\nyou'd like the model to provide the string using plain text output, you can wrap\nthe function in the\n[`TextOutput`](https://ai.pydantic.dev/api/output/#pydantic_ai.output.TextOutput)\nmarker class. If desired, this marker class can be used alongside one or more\n[`ToolOutput`](https://ai.pydantic.dev/output/#tool-output) marker classes (or\nunmarked types or functions) in a list provided to `output_type`.\n\ntext_output_function.py```\n\nfrompydantic_aiimport Agent, TextOutput\n\ndefsplit_into_words(text: str) -> list[str]:\n\n    return text.split()\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=TextOutput(split_into_words),\n)\n\nresult = agent.run_sync('Who was Albert Einstein?')\n\nprint(result.output)\n\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical',\n'physicist.']",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\n### Output modes\n\nPydantic AI implements three different methods to get a model to output\nstructured data:\n\n  1. [Tool Output](https://ai.pydantic.dev/output/#tool-output), where tool calls are used to produce the output.\n  2. [Native Output](https://ai.pydantic.dev/output/#native-output), where the model is required to produce text content compliant with a provided JSON schema.\n  3. [Prompted Output](https://ai.pydantic.dev/output/#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate.\n\n#### Tool Output\n\nIn the default Tool Output mode, the output JSON schema of each output type (or\nfunction) is provided to the model as the parameters schema of a special output\ntool. This is the default as it's supported by virtually all models and has been\nshown to work very well.\n\nIf you'd like to change the name of the output tool, pass a custom description\nto aid the model, or turn on or off strict mode, you can wrap the type(s) in the\n[`ToolOutput`](https://ai.pydantic.dev/api/output/#pydantic_ai.output.ToolOutput)\nmarker class and provide the appropriate arguments. Note that by default, the\ndescription is taken from the docstring specified on a Pydantic model or output\nfunction, so specifying it using the marker class is typically not necessary.\n\ntool_output.py```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, ToolOutput\n\nclassFruit(BaseModel):\n\n    name: str\n    color: str\n\nclassVehicle(BaseModel):\n\n    name: str\n    wheels: int\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=[ \nIf we were passing just Fruit and Vehicle without custom tool names, we could have used a union: output_type=Fruit | Vehicle (or in older Python, output_type=Union[Fruit | Vehicle]). However, as ToolOutput is an object rather than a type, we have to use a list.\n\n[](https://ai.pydantic.dev/output/#__code_5_annotation_1)\n\n        ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\n\nresult = agent.run_sync('What is a banana?')\n\nprint(repr(result.output))\n\n#> Fruit(name='banana', color='yellow')\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n#### Native Output\n\nNative Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON\nSchema response format\"), where the model is forced to only output text matching\nthe provided JSON schema. Note that this is not supported by all models, and\nsometimes comes with restrictions. For example, Anthropic does not support this\nat all, and Gemini cannot use tools at the same time as structured output, and\nattempting to do so will result in an error.\n\nTo use this mode, you can wrap the output type(s) in the\n[`NativeOutput`](https://ai.pydantic.dev/api/output/#pydantic_ai.output.NativeOutput)\nmarker class that also lets you specify a `name` and `description` if the name\nand docstring of the type or function are not sufficient.\n\nnative_output.py```\n\nfromtool_outputimport Fruit, Vehicle\n\nfrompydantic_aiimport Agent, NativeOutput\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=NativeOutput(\n        [Fruit, Vehicle], \nThis could also have been a union: output_type=Fruit | Vehicle (or in older Python, output_type=Union[Fruit, Vehicle]). However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly.\n\n[](https://ai.pydantic.dev/output/#__code_6_annotation_1)\n\n        name='Fruit or vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\n\nresult = agent.run_sync('What is a Ford Explorer?')\n\nprint(repr(result.output))\n\n#> Vehicle(name='Ford Explorer', wheels=4)",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\n#### Prompted Output\n\nIn this mode, the model is prompted to output text matching the provided JSON\nschema through its [instructions](https://ai.pydantic.dev/agents/#instructions)\nand it's up to the model to interpret those instructions correctly. This is\nusable with all models, but is often the least reliable approach as the model is\nnot forced to match the schema.\n\nWhile we would generally suggest starting with tool or native output, in some\ncases this mode may result in higher quality outputs, and for models without\nnative tool calling or structured output support it is the only option for\nproducing structured outputs.\n\nIf the model API supports the \"JSON Mode\" feature (aka \"JSON Object response\nformat\") to force the model to output valid JSON, this is enabled, but it's\nstill up to the model to abide by the schema. Pydantic AI will validate the\nreturned structured data and tell the model to try again if validation fails,\nbut if the model is not intelligent enough this may not be sufficient.\n\nTo use this mode, you can wrap the output type(s) in the\n[`PromptedOutput`](https://ai.pydantic.dev/api/output/#pydantic_ai.output.PromptedOutput)\nmarker class that also lets you specify a `name` and `description` if the name\nand docstring of the type or function are not sufficient. Additionally, it\nsupports an `template` argument lets you specify a custom instructions template\nto be used instead of the\n[default](https://ai.pydantic.dev/api/profiles/#pydantic_ai.profiles.ModelProfile.prompted_output_template).\n\nprompted_output.py```\n\nfrompydanticimport BaseModel\n\nfromtool_outputimport Vehicle\n\nfrompydantic_aiimport Agent, PromptedOutput\n\nclassDevice(BaseModel):\n\n    name: str\n    kind: str\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device], \nThis could also have been a union: output_type=Vehicle | Device (or in older Python, output_type=Union[Vehicle, Device]). However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly.\n\n[](https://ai.pydantic.dev/output/#__code_7_annotation_1)\n\n        name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\n\nresult = agent.run_sync('What is a MacBook?')\n\nprint(repr(result.output))\n\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\n\nresult = agent.run_sync('What is a Ford Explorer?')\n\nprint(repr(result.output))\n\n#> Vehicle(name='Ford Explorer', wheels=4)\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n### Output validators\n\nSome validation is inconvenient or impossible to do in Pydantic validators, in\nparticular when the validation requires IO and is asynchronous. PydanticAI\nprovides a way to add validation functions via the\n[`agent.output_validator`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.output_validator)\ndecorator.\n\nIf you want to implement separate validation logic for different output types,\nit's recommended to use [output\nfunctions](https://ai.pydantic.dev/output/#output-functions) instead, to save\nyou from having to do `isinstance` checks inside the output validator. If you\nwant the model to output plain text, do your own processing or validation, and\nthen have the agent's final output be the result of your function, it's\nrecommended to use an [output function](https://ai.pydantic.dev/output/#output-\nfunctions) with the [`TextOutput` marker\nclass](https://ai.pydantic.dev/output/#text-output).\n\nHere's a simplified variant of the [SQL Generation\nexample](https://ai.pydantic.dev/examples/sql-gen/):\n\nsql_gen.py```\n\nfromtypingimport Union\n\nfromfake_databaseimport DatabaseConn, QueryError\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, RunContext, ModelRetry\n\nclassSuccess(BaseModel):\n\n    sql_query: str\n\nclassInvalidRequest(BaseModel):\n\n    error_message: str\n\nOutput = Union[Success, InvalidRequest]\n\nagent = Agent[DatabaseConn, Output](\n\n    'google-gla:gemini-1.5-flash',\n    output_type=Output,  # type: ignore\n    deps_type=DatabaseConn,\n    system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\n)\n\n@agent.output_validator\n\nasync defvalidate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n\n    if isinstance(output, InvalidRequest):\n        return output\n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') frome\n    else:\n        return output\n\nresult = agent.run_sync(\n\n    'get me users who were last active yesterday.', deps=DatabaseConn()\n)\n\nprint(result.output)\n\n#> sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1\nday'",
  "```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Streamed Results\n\nThere two main challenges with streamed results:\n\n  1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\n  2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. PydanticAI streams just enough of the response to sniff out if it's a tool call or an output, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult).\n\n### Streaming Text\n\nExample of streamed text output:\n\nstreamed_hello_world.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')  \nStreaming works with the standard\nAgent[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) class, and\ndoesn't require any special setup, just a model that supports streaming\n(currently all models support streaming).\n\n[](https://ai.pydantic.dev/output/#__code_9_annotation_1)\n\nasync defmain():\n\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:  \nThe\nAgent.run_stream()[](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.run_stream)\nmethod is used to start a streamed run, this method returns a context manager so\nthe connection can be closed when the stream completes.\n\n[](https://ai.pydantic.dev/output/#__code_9_annotation_2)\n\n        async for message in result.stream_text():  \nEach item yield by\nStreamedRunResult.stream_text()[](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_text)\nis the complete text response, extended as new data is received.\n\n[](https://ai.pydantic.dev/output/#__code_9_annotation_3)\n\n            print(message)\n            #> The first known\n            #> The first known use of \"hello,\n            #> The first known use of \"hello, world\" was in\n            #> The first known use of \"hello, world\" was in a 1974 textbook\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\nWe can also stream text as deltas rather than the entire text in each item:\n\nstreamed_delta_hello_world.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\nasync defmain():\n\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n        async for message in result.stream_text(delta=True):  \n\nstream_text[](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_text)\nwill error if the response is not text.\n\n[](https://ai.pydantic.dev/output/#__code_10_annotation_1)\n\n            print(message)\n            #> The first known\n            #> use of \"hello,\n            #> world\" was in\n            #> a 1974 textbook\n            #> about the C\n            #> programming language.\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\nOutput message not included in `messages`\n\nThe final output message will **NOT** be added to result messages if you use\n`.stream_text(delta=True)`, see [Messages and chat\nhistory](https://ai.pydantic.dev/message-history/) for more information.\n\n### Streaming Structured Output\n\nNot all types are supported with partial validation in Pydantic, see\n[pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748),\ngenerally for model-like structures it's currently best to use `TypeDict`.\n\nHere's an example of streaming a use profile as it's built:\n\nstreamed_user_profile.py",
  "```\n\nfromdatetimeimport date\n\nfromtyping_extensionsimport TypedDict\n\nfrompydantic_aiimport Agent\n\nclassUserProfile(TypedDict, total=False):\n\n    name: str\n    dob: date\n    bio: str\n\nagent = Agent(\n\n    'openai:gpt-4o',\n    output_type=UserProfile,\n    system_prompt='Extract a user profile from the input',\n)\n\nasync defmain():\n\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for profile in result.stream():\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\nIf you want fine-grained control of validation, particularly catching validation\nerrors, you can use the following pattern:\n\nstreamed_user_profile.py```\n\nfromdatetimeimport date\n\nfrompydanticimport ValidationError\n\nfromtyping_extensionsimport TypedDict\n\nfrompydantic_aiimport Agent\n\nclassUserProfile(TypedDict, total=False):\n\n    name: str\n    dob: date\n    bio: str\n\nagent = Agent('openai:gpt-4o', output_type=UserProfile)\n\nasync defmain():\n\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for message, last in result.stream_structured(debounce_by=0.01):  \n\nstream_structured[](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.stream_structured)\nstreams the data as\nModelResponse[](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.ModelResponse)\nobjects, thus iteration can't fail with a ValidationError.\n\n[](https://ai.pydantic.dev/output/#__code_12_annotation_1)\n\n            try:\n                profile = await result.validate_structured_output(  \n\nvalidate_structured_output[](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult.validate_structured_output)\nvalidates the data, allow_partial=True enables pydantic's\nexperimental_allow_partial flag on\nTypeAdapter[](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json).\n\n[](https://ai.pydantic.dev/output/#__code_12_annotation_2)\n\n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}",
  "```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n## Examples\n\nThe following examples demonstrate how to use streamed responses in PydanticAI:\n\n  * [Stream markdown](https://ai.pydantic.dev/examples/stream-markdown/)\n  * [Stream Whales](https://ai.pydantic.dev/examples/stream-whales/)\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/openai/#openai)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nOpenAI\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * OpenAI  [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n        * [ Install  ](https://ai.pydantic.dev/models/openai/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/openai/#configuration)\n        * [ Environment variable  ](https://ai.pydantic.dev/models/openai/#environment-variable)\n        * [ Configure the provider  ](https://ai.pydantic.dev/models/openai/#configure-the-provider)\n        * [ Custom OpenAI Client  ](https://ai.pydantic.dev/models/openai/#custom-openai-client)\n        * [ OpenAI Responses API  ](https://ai.pydantic.dev/models/openai/#openai-responses-api)\n        * [ OpenAI-compatible Models  ](https://ai.pydantic.dev/models/openai/#openai-compatible-models)\n          * [ Model Profile  ](https://ai.pydantic.dev/models/openai/#model-profile)\n          * [ DeepSeek  ](https://ai.pydantic.dev/models/openai/#deepseek)\n          * [ Ollama  ](https://ai.pydantic.dev/models/openai/#ollama)\n            * [ Example local usage  ](https://ai.pydantic.dev/models/openai/#example-local-usage)\n            * [ Example using a remote server  ](https://ai.pydantic.dev/models/openai/#example-using-a-remote-server)\n          * [ Azure AI Foundry  ](https://ai.pydantic.dev/models/openai/#azure-ai-foundry)\n          * [ OpenRouter  ](https://ai.pydantic.dev/models/openai/#openrouter)\n          * [ Grok (xAI)  ](https://ai.pydantic.dev/models/openai/#grok-xai)\n          * [ Perplexity  ](https://ai.pydantic.dev/models/openai/#perplexity)\n          * [ Fireworks AI  ](https://ai.pydantic.dev/models/openai/#fireworks-ai)\n          * [ Together AI  ](https://ai.pydantic.dev/models/openai/#together-ai)\n          * [ Heroku AI  ](https://ai.pydantic.dev/models/openai/#heroku-ai)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/exampl",
  "es/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)",
  "* [ Install  ](https://ai.pydantic.dev/models/openai/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/openai/#configuration)\n  * [ Environment variable  ](https://ai.pydantic.dev/models/openai/#environment-variable)\n  * [ Configure the provider  ](https://ai.pydantic.dev/models/openai/#configure-the-provider)\n  * [ Custom OpenAI Client  ](https://ai.pydantic.dev/models/openai/#custom-openai-client)\n  * [ OpenAI Responses API  ](https://ai.pydantic.dev/models/openai/#openai-responses-api)\n  * [ OpenAI-compatible Models  ](https://ai.pydantic.dev/models/openai/#openai-compatible-models)\n    * [ Model Profile  ](https://ai.pydantic.dev/models/openai/#model-profile)\n    * [ DeepSeek  ](https://ai.pydantic.dev/models/openai/#deepseek)\n    * [ Ollama  ](https://ai.pydantic.dev/models/openai/#ollama)\n      * [ Example local usage  ](https://ai.pydantic.dev/models/openai/#example-local-usage)\n      * [ Example using a remote server  ](https://ai.pydantic.dev/models/openai/#example-using-a-remote-server)\n    * [ Azure AI Foundry  ](https://ai.pydantic.dev/models/openai/#azure-ai-foundry)\n    * [ OpenRouter  ](https://ai.pydantic.dev/models/openai/#openrouter)\n    * [ Grok (xAI)  ](https://ai.pydantic.dev/models/openai/#grok-xai)\n    * [ Perplexity  ](https://ai.pydantic.dev/models/openai/#perplexity)\n    * [ Fireworks AI  ](https://ai.pydantic.dev/models/openai/#fireworks-ai)\n    * [ Together AI  ](https://ai.pydantic.dev/models/openai/#together-ai)\n    * [ Heroku AI  ](https://ai.pydantic.dev/models/openai/#heroku-ai)\n\n# OpenAI\n\n## Install\n\nTo use OpenAI models or OpenAI-compatible APIs, you need to either install\n`pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:\n\n[pip](https://ai.pydantic.dev/models/openai/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/openai/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[openai]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[openai]\"\n\n```\n\n## Configuration\n\nTo use `OpenAIModel` with the OpenAI API, go to\n[platform.openai.com](https://platform.openai.com/) and follow your nose until\nyou find the place to generate an API key.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportOPENAI_API_KEY='your-api-key'\n\n```\n\nYou can then use `OpenAIModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o')\n\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nmodel = OpenAIModel('gpt-4o')\n\nagent = Agent(model)\n\n...\n\n```\n\nBy default, the `OpenAIModel` uses the `OpenAIProvider` with the `base_url` set\nto `https://api.openai.com/v1`.\n\n## Configure the provider\n\nIf you want to pass parameters in code to the provider, you can programmatically\ninstantiate the\n[OpenAIProvider](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider)\nand pass it to the model:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(api_key='your-api-key'))\n\nagent = Agent(model)\n\n...\n\n```\n\n## Custom OpenAI Client\n\n`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the\n`openai_client` parameter, so you can customise the `organization`, `project`,\n`base_url` etc. as defined in the [OpenAI API\ndocs](https://platform.openai.com/docs/api-reference).\n\ncustom_openai_client.py```\n\nfromopenaiimport AsyncOpenAI\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\n\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(openai_client=client))\n\nagent = Agent(model)\n\n...\n\n```\n\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-\nus/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure\nOpenAI API. Note that the `AsyncAzureOpenAI` is a subclass of `AsyncOpenAI`.\n\n```\n\nfromopenaiimport AsyncAzureOpenAI\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIModel(\n\n    'gpt-4o',\n    provider=OpenAIProvider(openai_client=client),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n## OpenAI Responses API\n\nPydanticAI also supports OpenAI's [Responses\nAPI](https://platform.openai.com/docs/api-reference/responses) through the\n`OpenAIResponsesModel` class.\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-4o')\n\nagent = Agent(model)\n\n...",
  "```\n\nThe Responses API has built-in tools that you can use instead of building your\nown:\n\n  * [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response.\n  * [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response.\n  * [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf.\n\nYou can use the `OpenAIResponsesModelSettings` class to make use of those built-\nin tools:\n\n```\n\nfromopenai.types.responsesimport WebSearchToolParam  \nThe file search tool and computer use tool can also be imported from\nopenai.types.responses.\n\n[](https://ai.pydantic.dev/models/openai/#__code_9_annotation_1)\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIResponsesModel,\nOpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n\n    openai_builtin_tools=[WebSearchToolParam(type='web_search_preview')],\n)\n\nmodel = OpenAIResponsesModel('gpt-4o')\n\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('What is the weather in Tokyo?')\n\nprint(result.output)\n\n\"\"\"\n\nAs of 7:48 AM on Wednesday, April 2, 2025, in Tokyo, Japan, the weather is\ncloudy with a temperature of 53°F (12°C).\n\n\"\"\"\n\n```\n\nYou can learn more about the differences between the Responses API and Chat\nCompletions API in the [OpenAI API\ndocs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).\n\n## OpenAI-compatible Models\n\nMany providers and models are compatible with the OpenAI API, and can be used\nwith `OpenAIModel` in PydanticAI. Before getting started, check the\n[installation and configuration](https://ai.pydantic.dev/models/openai/#install)\ninstructions above.\n\nTo use another OpenAI-compatible API, you can make use of the `base_url` and\n`api_key` arguments from `OpenAIProvider`:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nmodel = OpenAIModel(\n\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nVarious providers also have their own provider classes so that you don't need to\nspecify the base URL yourself and you can use the standard `<PROVIDER>_API_KEY`\nenvironment variable to set the API key. When a provider has its own provider\nclass, you can use the `Agent(\"<provider>:<model>\")` shorthand, e.g.\n`Agent(\"deepseek:deepseek-chat\")` or `Agent(\"openrouter:google/gemini-2.5-pro-\npreview\")`, instead of building the `OpenAIModel` explicitly. Similarly, you can\npass the provider name as a string to the `provider` argument on `OpenAIModel`\ninstead of building instantiating the provider class explicitly.\n\n#### Model Profile\n\nSometimes, the provider or model you're using will have slightly different\nrequirements than OpenAI's API or models, like having different restrictions on\nJSON schemas for tool definitions, or not supporting tool definitions to be\nmarked as strict.\n\nWhen using an alternative provider class provided by PydanticAI, an appropriate\nmodel profile is typically selected automatically based on the model name. If\nthe model you're using is not working correctly out of the box, you can tweak\nvarious aspects of how model requests are constructed by providing your own\n[`ModelProfile`](https://ai.pydantic.dev/api/profiles/#pydantic_ai.profiles.ModelProfile)\n(for behaviors shared among all model classes) or\n[`OpenAIModelProfile`](https://ai.pydantic.dev/api/profiles/#pydantic_ai.profiles.openai.OpenAIModelProfile)\n(for behaviors specific to `OpenAIModel`):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.profiles._json_schemaimport InlineDefsJsonSchemaTransformer\n\nfrompydantic_ai.profiles.openaiimport OpenAIModelProfile\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nmodel = OpenAIModel(\n\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\n\nagent = Agent(model)\n\n```\n\n### DeepSeek\n\nTo use the [DeepSeek](https://deepseek.com) provider, first create an API key by\nfollowing the [Quick Start guide](https://api-docs.deepseek.com/). Once you have\nthe API key, you can use it with the\n[`DeepSeekProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.deepseek.DeepSeekProvider):",
  "```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.deepseekimport DeepSeekProvider\n\nmodel = OpenAIModel(\n\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize any provider with a custom `http_client`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.deepseekimport DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = OpenAIModel(\n\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Ollama\n\nTo use [Ollama](https://ollama.com/), you must first download the Ollama client,\nand then download a model using the [Ollama model\nlibrary](https://ollama.com/library).\n\nYou must also ensure the Ollama server is running when trying to make requests\nto it. For more information, please see the [Ollama\ndocumentation](https://github.com/ollama/ollama/tree/main/docs).\n\n#### Example local usage\n\nWith `ollama` installed, you can run the server with the model you want to use:\n\n```\n\nollama\n\n```\n\n(this will pull the `llama3.2` model if you don't already have it downloaded)\n\nThen run your code, here's a minimal example:\n\n```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nclassCityLocation(BaseModel):\n\n    city: str\n    country: str\n\nollama_model = OpenAIModel(\n\n    model_name='llama3.2', provider=OpenAIProvider(base_url='http://localhost:11434/v1')\n)\n\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\n\nprint(result.output)\n\n#> city='London' country='United Kingdom'\n\nprint(result.usage())\n\n#> Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n\n```\n\n#### Example using a remote server\n\n```\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nollama_model = OpenAIModel(\n\n    model_name='qwen2.5-coder:7b',  \nThe name of the model running on the remote server\n\n[](https://ai.pydantic.dev/models/openai/#__code_16_annotation_1)\n\n    provider=OpenAIProvider(base_url='http://192.168.1.74:11434/v1'),  \nThe url of the remote server\n\n[](https://ai.pydantic.dev/models/openai/#__code_16_annotation_2)\n\n)\n\nclassCityLocation(BaseModel):\n\n    city: str\n    country: str\n\nagent = Agent(model=ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\n\nprint(result.output)\n\n#> city='London' country='United Kingdom'\n\nprint(result.usage())\n\n#> Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n\n```\n\n### Azure AI Foundry\n\nIf you want to use [Azure AI Foundry](https://ai.azure.com/) as your provider,\nyou can do so by using the\n[`AzureProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.azure.AzureProvider)\nclass.\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.azureimport AzureProvider\n\nmodel = OpenAIModel(\n\n    'gpt-4o',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### OpenRouter\n\nTo use [OpenRouter](https://openrouter.ai), first create an API key at\n[openrouter.ai/keys](https://openrouter.ai/keys).\n\nOnce you have the API key, you can use it with the\n[`OpenRouterProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openrouter.OpenRouterProvider):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openrouterimport OpenRouterProvider\n\nmodel = OpenAIModel(\n\n    'anthropic/claude-3.5-sonnet',\n    provider=OpenRouterProvider(api_key='your-openrouter-api-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Grok (xAI)\n\nGo to [xAI API Console](https://console.x.ai/) and create an API key. Once you\nhave the API key, you can use it with the\n[`GrokProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.grok.GrokProvider):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.grokimport GrokProvider\n\nmodel = OpenAIModel(\n\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Perplexity\n\nFollow the Perplexity [getting\nstarted](https://docs.perplexity.ai/guides/getting-started) guide to create an\nAPI key. Then, you can query the Perplexity API with the following:",
  "```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.openaiimport OpenAIProvider\n\nmodel = OpenAIModel(\n\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Fireworks AI\n\nGo to [Fireworks.AI](https://fireworks.ai/) and create an API key in your\naccount settings. Once you have the API key, you can use it with the\n[`FireworksProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.fireworks.FireworksProvider):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.fireworksimport FireworksProvider\n\nmodel = OpenAIModel(\n\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Together AI\n\nGo to [Together.ai](https://www.together.ai/) and create an API key in your\naccount settings. Once you have the API key, you can use it with the\n[`TogetherProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.together.TogetherProvider):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.togetherimport TogetherProvider\n\nmodel = OpenAIModel(\n\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\n### Heroku AI\n\nTo use [Heroku AI](https://www.heroku.com/ai), you can use the\n[`HerokuProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.heroku.HerokuProvider):\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.openaiimport OpenAIModel\n\nfrompydantic_ai.providers.herokuimport HerokuProvider\n\nmodel = OpenAIModel(\n\n    'claude-3-7-sonnet',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can set the `HEROKU_INFERENCE_KEY` and `HEROKU_INFERENCE_URL` environment\nvariables to set the API key and base URL, respectively:\n\n```\n\nexportHEROKU_INFERENCE_KEY='your-heroku-inference-key'\n\nexportHEROKU_INFERENCE_URL='https://us.inference.heroku.com'",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/models/groq/#groq)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nGroq\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * Groq  [ Groq  ](https://ai.pydantic.dev/models/groq/)\n        * [ Install  ](https://ai.pydantic.dev/models/groq/#install)\n        * [ Configuration  ](https://ai.pydantic.dev/models/groq/#configuration)\n        * [ Environment variable  ](https://ai.pydantic.dev/models/groq/#environment-variable)\n        * [ provider argument  ](https://ai.pydantic.dev/models/groq/#provider-argument)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models",
  "/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Install  ](https://ai.pydantic.dev/models/groq/#install)\n  * [ Configuration  ](https://ai.pydantic.dev/models/groq/#configuration)\n  * [ Environment variable  ](https://ai.pydantic.dev/models/groq/#environment-variable)\n  * [ provider argument  ](https://ai.pydantic.dev/models/groq/#provider-argument)\n\n# Groq\n\n## Install\n\nTo use `GroqModel`, you need to either install `pydantic-ai`, or install\n`pydantic-ai-slim` with the `groq` optional group:\n\n[pip](https://ai.pydantic.dev/models/groq/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/groq/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai-slim[groq]\"\n\n```\n\n```\n\nuv\"pydantic-ai-slim[groq]\"\n\n```\n\n## Configuration\n\nTo use [Groq](https://groq.com/) through their API, go to\n[console.groq.com/keys](https://console.groq.com/keys) and follow your nose\nuntil you find the place to generate an API key.\n\n`GroqModelName` contains a list of available Groq models.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```\n\nexportGROQ_API_KEY='your-api-key'\n\n```\n\nYou can then use `GroqModel` by name:\n\n```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('groq:llama-3.3-70b-versatile')\n\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.groqimport GroqModel\n\nmodel = GroqModel('llama-3.3-70b-versatile')\n\nagent = Agent(model)\n\n...\n\n```\n\n##  `provider` argument\n\nYou can provide a custom `Provider` via the `provider` argument:\n\n```\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.groqimport GroqModel\n\nfrompydantic_ai.providers.groqimport GroqProvider\n\nmodel = GroqModel(\n\n    'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')\n)\n\nagent = Agent(model)\n\n...\n\n```\n\nYou can also customize the `GroqProvider` with a custom `httpx.AsyncHTTPClient`:\n\n```\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent\n\nfrompydantic_ai.models.groqimport GroqModel\n\nfrompydantic_ai.providers.groqimport GroqProvider\n\ncustom_http_client = AsyncClient(timeout=30)\n\nmodel = GroqModel(\n\n    'llama-3.3-70b-versatile',\n    provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),\n)\n\nagent = Agent(model)\n\n...",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/cli/#command-line-interface-cli)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nCommand Line Interface (CLI)\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * Command Line Interface (CLI)  [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n      * [ Usage  ](https://ai.pydantic.dev/cli/#usage)\n        * [ Help  ](https://ai.pydantic.dev/cli/#help)\n        * [ Choose a model  ](https://ai.pydantic.dev/cli/#choose-a-model)\n        * [ Custom Agents  ](https://ai.pydantic.dev/cli/#custom-agents)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)",
  "* [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Usage  ](https://ai.pydantic.dev/cli/#usage)\n    * [ Help  ](https://ai.pydantic.dev/cli/#help)\n    * [ Choose a model  ](https://ai.pydantic.dev/cli/#choose-a-model)\n    * [ Custom Agents  ](https://ai.pydantic.dev/cli/#custom-agents)\n\n# Command Line Interface (CLI)\n\n**PydanticAI** comes with a CLI, `clai` (pronounced \"clay\") which you can use to\ninteract with various LLMs from the command line. It provides a convenient way\nto chat with language models and quickly get answers right in the terminal.\n\nWe originally developed this CLI for our own use, but found ourselves using it\nso frequently that we decided to share it as part of the PydanticAI package.\n\nWe plan to continue adding new features, such as interaction with MCP servers,\naccess to tools, and more.\n\n## Usage\n\nYou'll need to set an environment variable depending on the provider you intend\nto use.\n\nE.g. if you're using OpenAI, set the `OPENAI_API_KEY` environment variable:\n\n```\n\nexportOPENAI_API_KEY='your-api-key-here'\n\n```\n\nThen with [`uvx`](https://docs.astral.sh/uv/guides/tools/), run:\n\n```\n\nuvx\n\n```\n\nOr to install `clai` globally [with\n`uv`](https://docs.astral.sh/uv/guides/tools/#installing-tools), run:\n\n```\n\nuv\n\n```\n\nOr with `pip`, run:\n\n```\n\npip\n\n```\n\nEither way, running `clai` will start an interactive session where you can chat\nwith the AI model. Special commands available in interactive mode:\n\n  * `/exit`: Exit the session\n  * `/markdown`: Show the last response in markdown format\n  * `/multiline`: Toggle multiline input mode (use Ctrl+D to submit)\n\n### Help\n\nTo get help on the CLI, use the `--help` flag:\n\n```\n\nuvx\n\n```\n\n### Choose a model\n\nYou can specify which model to use with the `--model` flag:\n\n```\n\nuvx\n\n```\n\n(a full list of models available can be printed with `uvx clai --list-models`)\n\n### Custom Agents\n\nYou can specify a custom agent using the `--agent` flag with a module path and\nvariable name:\n\ncustom_agent.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\n\n```\n\nThen run:\n\n```\n\nuvx\"What's the weather today?\"\n\n```\n\nThe format must be `module:variable` where:\n\n  * `module` is the importable Python module path\n  * `variable` is the name of the Agent instance in that module\n\nAdditionally, you can directly launch CLI mode from an `Agent` instance using\n`Agent.to_cli_sync()`:\n\nagent_to_cli_sync.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\n\nagent.to_cli_sync()\n\n```\n\nYou can also use the async interface with `Agent.to_cli()`:\n\nagent_to_cli.py```\n\nfrompydantic_aiimport Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\n\nasync defmain():\n\n    await agent.to_cli()",
  "```\n\n_(You'll need to add`asyncio.run(main())` to run `main`)_\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/direct/#direct-model-requests)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nDirect Model Requests\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * Direct Model Requests  [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n      * [ Basic Example  ](https://ai.pydantic.dev/direct/#basic-example)\n      * [ Advanced Example with Tool Calling  ](https://ai.pydantic.dev/direct/#advanced-example-with-tool-calling)\n      * [ When to Use the direct API vs Agent  ](https://ai.pydantic.dev/direct/#when-to-use-the-direct-api-vs-agent)\n      * [ OpenTelemetry or Logfire Instrumentation  ](https://ai.pydantic.dev/direct/#opentelemetry-or-logfire-instrumentation)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai",
  "](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Basic Example  ](https://ai.pydantic.dev/direct/#basic-example)\n  * [ Advanced Example with Tool Calling  ](https://ai.pydantic.dev/direct/#advanced-example-with-tool-calling)\n  * [ When to Use the direct API vs Agent  ](https://ai.pydantic.dev/direct/#when-to-use-the-direct-api-vs-agent)\n  * [ OpenTelemetry or Logfire Instrumentation  ](https://ai.pydantic.dev/direct/#opentelemetry-or-logfire-instrumentation)\n\n# Direct Model Requests\n\nThe `direct` module provides low-level methods for making imperative requests to\nLLMs where the only abstraction is input and output schema translation, enabling\nyou to use all models with the same API.\n\nThese methods are thin wrappers around the\n[`Model`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.Model)\nimplementations, offering a simpler interface when you don't need the full\nfunctionality of an\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent).\n\nThe following functions are available:\n\n  * [`model_request`](https://ai.pydantic.dev/api/direct/#pydantic_ai.direct.model_request): Make a non-streamed async request to a model\n  * [`model_request_sync`](https://ai.pydantic.dev/api/direct/#pydantic_ai.direct.model_request_sync): Make a non-streamed synchronous request to a model\n  * [`model_request_stream`](https://ai.pydantic.dev/api/direct/#pydantic_ai.direct.model_request_stream): Make a streamed async request to a model\n\n## Basic Example\n\nHere's a simple example demonstrating how to use the direct API to make a basic\nrequest:\n\ndirect_basic.py```\n\nfrompydantic_ai.directimport model_request_sync\n\nfrompydantic_ai.messagesimport ModelRequest\n\n# Make a synchronous request to the model\n\nmodel_response = model_request_sync(\n\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]\n)\n\nprint(model_response.parts[0].content)\n\n#> Paris\n\nprint(model_response.usage)\n\n#> Usage(requests=1, request_tokens=56, response_tokens=1, total_tokens=57)\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Advanced Example with Tool Calling\n\nYou can also use the direct API to work with function/tool calling.\n\nEven here we can use Pydantic to generate the JSON schema for the tool:",
  "```\n\nfrompydanticimport BaseModel\n\nfromtyping_extensionsimport Literal\n\nfrompydantic_ai.directimport model_request\n\nfrompydantic_ai.messagesimport ModelRequest\n\nfrompydantic_ai.modelsimport ModelRequestParameters\n\nfrompydantic_ai.toolsimport ToolDefinition\n\nclassDivide(BaseModel):\n\n\"\"\"Divide two numbers.\"\"\"\n\n    numerator: float\n    denominator: float\n    on_inf: Literal['error', 'infinity'] = 'infinity'\n\nasync defmain():\n\n    # Make a request to the model with tool access\n    model_response = await model_request(\n        'openai:gpt-4.1-nano',\n        [ModelRequest.user_text_prompt('What is 123 / 456?')],\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[\n                ToolDefinition(\n                    name=Divide.__name__.lower(),\n                    description=Divide.__doc__ or '',\n                    parameters_json_schema=Divide.model_json_schema(),\n                )\n            ],\n            allow_text_output=True,  # Allow model to either use tools or respond directly\n        ),\n    )\n    print(model_response)\n\"\"\"\n\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='divide',\n                args={'numerator': '123', 'denominator': '456'},\n                tool_call_id='pyd_ai_2e0e396768a14fe482df90a29a78dc7b',\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=55, response_tokens=7, total_tokens=62),\n        model_name='gpt-4.1-nano',\n        timestamp=datetime.datetime(...),\n    )\n    \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main())` to run `main`)_\n\n## When to Use the direct API vs Agent\n\nThe direct API is ideal when:\n\n  1. You need more direct control over model interactions\n  2. You want to implement custom behavior around model requests\n  3. You're building your own abstractions on top of model interactions\n\nFor most application use cases, the higher-level\n[`Agent`](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) API\nprovides a more convenient interface with additional features such as built-in\ntool execution, retrying, structured output parsing, and more.\n\n## OpenTelemetry or Logfire Instrumentation\n\nAs with [agents](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent),\nyou can enable OpenTelemetry/Logfire instrumentation with just a few extra lines\n\ndirect_instrumented.py```\n\nimportlogfire\n\nfrompydantic_ai.directimport model_request_sync\n\nfrompydantic_ai.messagesimport ModelRequest\n\nlogfire.configure()\n\nlogfire.instrument_pydantic_ai()\n\n# Make a synchronous request to the model\n\nmodel_response = model_request_sync(\n\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n)\n\nprint(model_response.parts[0].content)\n\n#> Paris\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nYou can also enable OpenTelemetry on a per call basis:\n\ndirect_instrumented.py```\n\nimportlogfire\n\nfrompydantic_ai.directimport model_request_sync\n\nfrompydantic_ai.messagesimport ModelRequest\n\nlogfire.configure()\n\n# Make a synchronous request to the model\n\nmodel_response = model_request_sync(\n\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n    instrument=True\n)\n\nprint(model_response.parts[0].content)\n\n#> Paris",
  "```\n\nSee [Debugging and Monitoring](https://ai.pydantic.dev/logfire/) for more\ndetails, including how to instrument with plain OpenTelemetry without Logfire.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/evals/#evals)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nEvals\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * Evals  [ Evals  ](https://ai.pydantic.dev/evals/)\n      * [ Installation  ](https://ai.pydantic.dev/evals/#installation)\n      * [ Datasets and Cases  ](https://ai.pydantic.dev/evals/#datasets-and-cases)\n      * [ Evaluators  ](https://ai.pydantic.dev/evals/#evaluators)\n      * [ Evaluation Process  ](https://ai.pydantic.dev/evals/#evaluation-process)\n      * [ Evaluation with LLMJudge  ](https://ai.pydantic.dev/evals/#evaluation-with-llmjudge)\n      * [ Saving and Loading Datasets  ](https://ai.pydantic.dev/evals/#saving-and-loading-datasets)\n      * [ Parallel Evaluation  ](https://ai.pydantic.dev/evals/#parallel-evaluation)\n      * [ OpenTelemetry Integration  ](https://ai.pydantic.dev/evals/#opentelemetry-integration)\n      * [ Generating Test Datasets  ](https://ai.pydantic.dev/evals/#generating-test-datasets)\n      * [ Integration with Logfire  ](https://ai.pydantic.dev/evals/#integration-with-logfire)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://a",
  "i.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Installation  ](https://ai.pydantic.dev/evals/#installation)\n  * [ Datasets and Cases  ](https://ai.pydantic.dev/evals/#datasets-and-cases)\n  * [ Evaluators  ](https://ai.pydantic.dev/evals/#evaluators)\n  * [ Evaluation Process  ](https://ai.pydantic.dev/evals/#evaluation-process)\n  * [ Evaluation with LLMJudge  ](https://ai.pydantic.dev/evals/#evaluation-with-llmjudge)\n  * [ Saving and Loading Datasets  ](https://ai.pydantic.dev/evals/#saving-and-loading-datasets)\n  * [ Parallel Evaluation  ](https://ai.pydantic.dev/evals/#parallel-evaluation)\n  * [ OpenTelemetry Integration  ](https://ai.pydantic.dev/evals/#opentelemetry-integration)\n  * [ Generating Test Datasets  ](https://ai.pydantic.dev/evals/#generating-test-datasets)\n  * [ Integration with Logfire  ](https://ai.pydantic.dev/evals/#integration-with-logfire)\n\n# Evals\n\n\"Evals\" refers to evaluating a model's performance for a specific application.\n\nWarning\n\nUnlike unit tests, evals are an emerging art/science; anyone who claims to know\nfor sure exactly how your evals should be defined can safely be ignored.\n\nPydantic Evals is a powerful evaluation framework designed to help you\nsystematically test and evaluate the performance and accuracy of the systems you\nbuild, especially when working with LLMs.\n\nWe've designed Pydantic Evals to be useful while not being too opinionated since\nwe (along with everyone else) are still figuring out best practices. We'd love\nyour [feedback](https://ai.pydantic.dev/help/) on the package and how we can\nimprove it.\n\nIn Beta\n\nPydantic Evals support was [introduced](https://github.com/pydantic/pydantic-\nai/pull/935) in v0.0.47 and is currently in beta. The API is subject to change\nand the documentation is incomplete.\n\n## Installation\n\nTo install the Pydantic Evals package, run:\n\n[pip](https://ai.pydantic.dev/evals/#__tabbed_1_1)[uv](https://ai.pydantic.dev/evals/#__tabbed_1_2)\n\n```\n\npip\n\n```\n\n```\n\nuv\n\n```\n\n`pydantic-evals` does not depend on `pydantic-ai`, but has an optional\ndependency on `logfire` if you'd like to use OpenTelemetry traces in your evals,\nor send evaluation results to [logfire](https://pydantic.dev/logfire).\n\n[pip](https://ai.pydantic.dev/evals/#__tabbed_2_1)[uv](https://ai.pydantic.dev/evals/#__tabbed_2_2)\n\n```\n\npip'pydantic-evals[logfire]'\n\n```",
  "```\n\nuv'pydantic-evals[logfire]'\n\n```\n\n## Datasets and Cases\n\nIn Pydantic Evals, everything begins with `Dataset`s and `Case`s:\n\n  * [`Case`](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Case): A single test scenario corresponding to \"task\" inputs. Can also optionally have a name, expected outputs, metadata, and evaluators.\n  * [`Dataset`](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset): A collection of test cases designed for the evaluation of a specific task or function.\n\nsimple_eval_dataset.py```\n\nfrompydantic_evalsimport Case, Dataset\n\ncase1 = Case(\n\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\ndataset = Dataset(cases=[case1])\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Evaluators\n\nEvaluators are the components that analyze and score the results of your task\nwhen tested against a case.\n\nPydantic Evals includes several built-in evaluators and allows you to create\ncustom evaluators:\n\nsimple_eval_evaluator.py```\n\nfromdataclassesimport dataclass\n\nfromsimple_eval_datasetimport dataset\n\nfrompydantic_evals.evaluatorsimport Evaluator, EvaluatorContext\n\nfrompydantic_evals.evaluators.commonimport IsInstance\n\ndataset.add_evaluator(IsInstance(type_name='str'))\n[](https://ai.pydantic.dev/evals/#__code_5_annotation_1)\n\n@dataclass\n\nclassMyEvaluator(Evaluator):\n\n    async defevaluate(self, ctx: EvaluatorContext[str, str]) -> float:  [](https://ai.pydantic.dev/evals/#__code_5_annotation_2)\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\ndataset.add_evaluator(MyEvaluator())\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Evaluation Process\n\nThe evaluation process involves running a task against all cases in a dataset:\n\nPutting the above two examples together and using the more declarative\n`evaluators` kwarg to\n[`Dataset`](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset):\n\nsimple_eval_complete.py```\n\nfrompydantic_evalsimport Case, Dataset\n\nfrompydantic_evals.evaluatorsimport Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(  [](https://ai.pydantic.dev/evals/#__code_6_annotation_1)\n\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\nclassMyEvaluator(Evaluator[str, str]):\n\n    defevaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\ndataset = Dataset(\n\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  [](https://ai.pydantic.dev/evals/#__code_6_annotation_3)\n)\n\nasync defguess_city(question: str) -> str:\n[](https://ai.pydantic.dev/evals/#__code_6_annotation_4)\n\n    return 'Paris'\n\nreport = dataset.evaluate_sync(guess_city)\n[](https://ai.pydantic.dev/evals/#__code_6_annotation_5)\n\nreport.print(include_input=True, include_output=True, include_durations=False)\n[](https://ai.pydantic.dev/evals/#__code_6_annotation_6)\n\n\"\"\"\n\n                              Evaluation Summary: guess_city\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n\n┃ Case ID     ┃ Inputs                         ┃ Outputs ┃ Scores            ┃\nAssertions ┃\n\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n\n│ simple_case │ What is the capital of France? │ Paris   │ MyEvaluator: 1.00 │ ✔\n│\n\n├─────────────┼────────────────────────────────┼─────────┼───────────────────┼────────────┤\n\n│ Averages    │                                │         │ MyEvaluator: 1.00 │\n100.0% ✔   │\n\n└─────────────┴────────────────────────────────┴─────────┴───────────────────┴────────────┘\n\n\"\"\"\n\n```\n\n  1. Also create a custom evaluator function as above\n\n_(This example is complete, it can be run \"as is\")_\n\n## Evaluation with `LLMJudge`\n\nIn this example we evaluate a method for generating recipes based on customer\norders.\n\njudge_recipes.py",
  "```\n\nfrom__future__import annotations\n\nfromtypingimport Any\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent, format_as_xml\n\nfrompydantic_evalsimport Case, Dataset\n\nfrompydantic_evals.evaluatorsimport IsInstance, LLMJudge\n\nclassCustomerOrder(BaseModel):\n[](https://ai.pydantic.dev/evals/#__code_7_annotation_1)\n\n    dish_name: str\n    dietary_restriction: str | None = None\n\nclassRecipe(BaseModel):\n\n    ingredients: list[str]\n    steps: list[str]\n\nrecipe_agent = Agent(\n\n    'groq:llama-3.3-70b-versatile',\n    output_type=Recipe,\n    system_prompt=(\n        'Generate a recipe to cook the dish that meets the dietary restrictions.'\n    ),\n)\n\nasync deftransform_recipe(customer_order: CustomerOrder) -> Recipe:\n[](https://ai.pydantic.dev/evals/#__code_7_annotation_2)\n\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.output\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](\n[](https://ai.pydantic.dev/evals/#__code_7_annotation_3)\n\n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(\n                dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'\n            ),\n            expected_output=None,  # [](https://ai.pydantic.dev/evals/#__code_7_annotation_4)\n            metadata={'focus': 'vegetarian'},\n            evaluators=(\n                LLMJudge(  [](https://ai.pydantic.dev/evals/#__code_7_annotation_5)\n                    rubric='Recipe should not contain meat or animal products',\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(\n                dish_name='Chocolate Cake', dietary_restriction='gluten-free'\n            ),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            # Case-specific evaluator with a focused rubric\n            evaluators=(\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                ),\n            ),\n        ),\n    ],\n    evaluators=[  [](https://ai.pydantic.dev/evals/#__code_7_annotation_6)\n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model='anthropic:claude-3-7-sonnet-latest',  [](https://ai.pydantic.dev/evals/#__code_7_annotation_7)\n        ),\n    ],\n)\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\n\nprint(report)\n\n\"\"\"\n\n     Evaluation Summary: transform_recipe\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n\n┃ Case ID            ┃ Assertions ┃ Duration ┃\n\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n\n│ vegetarian_recipe  │ ✔✔✔        │     10ms │\n\n├────────────────────┼────────────┼──────────┤\n\n│ gluten_free_recipe │ ✔✔✔        │     10ms │\n\n├────────────────────┼────────────┼──────────┤\n\n│ Averages           │ 100.0% ✔   │     10ms │\n\n└────────────────────┴────────────┴──────────┘\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Saving and Loading Datasets\n\nDatasets can be saved to and loaded from YAML or JSON files.\n\nsave_load_dataset_example.py```\n\nfrompathlibimport Path\n\nfromjudge_recipesimport CustomerOrder, Recipe, recipe_dataset\n\nfrompydantic_evalsimport Dataset\n\nrecipe_transforms_file = Path('recipe_transform_tests.yaml')\n\nrecipe_dataset.to_file(recipe_transforms_file)  # (1)!\n\nprint(recipe_transforms_file.read_text())\n\n\"\"\"\n\n# yaml-language-server: $schema=recipe_transform_tests_schema.json\n\ncases:\n\n- name: vegetarian_recipe\n  inputs:\n\n    dish_name: Spaghetti Bolognese\n    dietary_restriction: vegetarian\n  metadata:\n\n    focus: vegetarian\n  evaluators:\n\n  - LLMJudge: Recipe should not contain meat or animal products\n- name: gluten_free_recipe\n  inputs:\n\n    dish_name: Chocolate Cake\n    dietary_restriction: gluten-free\n  metadata:\n\n    focus: gluten-free\n  evaluators:\n\n  - LLMJudge: Recipe should not contain gluten or wheat products\nevaluators:\n\n- IsInstance: Recipe\n- LLMJudge:\n    rubric: Recipe should have clear steps and relevant ingredients\n    model: anthropic:claude-3-7-sonnet-latest\n    include_input: true\n\"\"\"\n\n# Load dataset from file\n\nloaded_dataset = Dataset[CustomerOrder, Recipe,\ndict].from_file(recipe_transforms_file)\n\nprint(f'Loaded dataset with {len(loaded_dataset.cases)} cases')\n\n#> Loaded dataset with 2 cases\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Parallel Evaluation\n\nYou can control concurrency during evaluation (this might be useful to prevent\nexceeding a rate limit):\n\nparallel_evaluation_example.py",
  "```\n\nimportasyncio\n\nimporttime\n\nfrompydantic_evalsimport Case, Dataset\n\n# Create a dataset with multiple test cases\n\ndataset = Dataset(\n\n    cases=[\n        Case(\n            name=f'case_{i}',\n            inputs=i,\n            expected_output=i * 2,\n        )\n        for i in range(5)\n    ]\n)\n\nasync defdouble_number(input_value: int) -> int:\n\n\"\"\"Function that simulates work by sleeping for a tenth of a second before\nreturning double the input.\"\"\"\n\n    await asyncio.sleep(0.1)  # Simulate work\n    return input_value * 2\n\n# Run evaluation with unlimited concurrency\n\nt0 = time.time()\n\nreport_default = dataset.evaluate_sync(double_number)\n\nprint(f'Evaluation took less than 0.5s: {time.time()-t0<0.5}')\n\n#> Evaluation took less than 0.5s: True\n\nreport_default.print(include_input=True, include_output=True,\ninclude_durations=False)\n[](https://ai.pydantic.dev/evals/#__code_9_annotation_1)\n\n\"\"\"\n\n      Evaluation Summary:\n         double_number\n┏━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n\n┃ Case ID  ┃ Inputs ┃ Outputs ┃\n\n┡━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n\n│ case_0   │ 0      │ 0       │\n\n├──────────┼────────┼─────────┤\n\n│ case_1   │ 1      │ 2       │\n\n├──────────┼────────┼─────────┤\n\n│ case_2   │ 2      │ 4       │\n\n├──────────┼────────┼─────────┤\n\n│ case_3   │ 3      │ 6       │\n\n├──────────┼────────┼─────────┤\n\n│ case_4   │ 4      │ 8       │\n\n├──────────┼────────┼─────────┤\n\n│ Averages │        │         │\n\n└──────────┴────────┴─────────┘\n\n\"\"\"\n\n# Run evaluation with limited concurrency\n\nt0 = time.time()\n\nreport_limited = dataset.evaluate_sync(double_number, max_concurrency=1)\n\nprint(f'Evaluation took more than 0.5s: {time.time()-t0>0.5}')\n\n#> Evaluation took more than 0.5s: True\n\nreport_limited.print(include_input=True, include_output=True,\ninclude_durations=False)\n[](https://ai.pydantic.dev/evals/#__code_9_annotation_2)\n\n\"\"\"\n\n      Evaluation Summary:\n         double_number\n┏━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n\n┃ Case ID  ┃ Inputs ┃ Outputs ┃\n\n┡━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n\n│ case_0   │ 0      │ 0       │\n\n├──────────┼────────┼─────────┤\n\n│ case_1   │ 1      │ 2       │\n\n├──────────┼────────┼─────────┤\n\n│ case_2   │ 2      │ 4       │\n\n├──────────┼────────┼─────────┤\n\n│ case_3   │ 3      │ 6       │\n\n├──────────┼────────┼─────────┤\n\n│ case_4   │ 4      │ 8       │\n\n├──────────┼────────┼─────────┤\n\n│ Averages │        │         │\n\n└──────────┴────────┴─────────┘\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## OpenTelemetry Integration\n\nPydantic Evals integrates with OpenTelemetry for tracing.\n\nThe\n[`EvaluatorContext`](https://ai.pydantic.dev/api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext)\nincludes a property called `span_tree` which returns a\n[`SpanTree`](https://ai.pydantic.dev/api/pydantic_evals/otel/#pydantic_evals.otel.SpanTree).\nThe `SpanTree` provides a way to query and analyze the spans generated during\nfunction execution. This provides a way to access the results of instrumentation\nduring evaluation.\n\nNote\n\nIf you just want to write unit tests that ensure that specific spans are\nproduced during calls to your evaluation task, it's usually better to just use\nthe `logfire.testing.capfire` fixture directly.\n\nThere are two main ways this is useful.\n\nopentelemetry_example.py",
  "```\n\nimportasyncio\n\nfromtypingimport Any\n\nimportlogfire\n\nfrompydantic_evalsimport Case, Dataset\n\nfrompydantic_evals.evaluatorsimport Evaluator\n\nfrompydantic_evals.evaluators.contextimport EvaluatorContext\n\nfrompydantic_evals.otel.span_treeimport SpanQuery\n\nlogfire.configure(  # ensure that an OpenTelemetry tracer is configured\n\n    send_to_logfire='if-token-present'\n)\n\nclassSpanTracingEvaluator(Evaluator[str, str]):\n\n\"\"\"Evaluator that analyzes the span tree generated during function execution.\"\"\"\n\n    defevaluate(self, ctx: EvaluatorContext[str, str]) -> dict[str, Any]:\n        # Get the span tree from the context\n        span_tree = ctx.span_tree\n        if span_tree is None:\n            return {'has_spans': False, 'performance_score': 0.0}\n\n        # Find all spans with \"processing\" in the name\n        processing_spans = span_tree.find(lambda node: 'processing' in node.name)\n\n        # Calculate total processing time\n        total_processing_time = sum(\n            (span.duration.total_seconds() for span in processing_spans), 0.0\n        )\n\n        # Check for error spans\n        error_query: SpanQuery = {'name_contains': 'error'}\n        has_errors = span_tree.any(error_query)\n\n        # Calculate a performance score (lower is better)\n        performance_score = 1.0 if total_processing_time < 1.0 else 0.5\n\n        return {\n            'has_spans': True,\n            'has_errors': has_errors,\n            'performance_score': 0 if has_errors else performance_score,\n        }\n\nasync defprocess_text(text: str) -> str:\n\n\"\"\"Function that processes text with OpenTelemetry instrumentation.\"\"\"\n\n    with logfire.span('process_text'):\n        # Simulate initial processing\n        with logfire.span('text_processing'):\n            await asyncio.sleep(0.1)\n            processed = text.strip().lower()\n\n        # Simulate additional processing\n        with logfire.span('additional_processing'):\n            if 'error' in processed:\n                with logfire.span('error_handling'):\n                    logfire.error(f'Error detected in text: {text}')\n                    return f'Error processing: {text}'\n            await asyncio.sleep(0.2)\n            processed = processed.replace(' ', '_')\n\n        return f'Processed: {processed}'\n\n# Create test cases\n\ndataset = Dataset(\n\n    cases=[\n        Case(\n            name='normal_text',\n            inputs='Hello World',\n            expected_output='Processed: hello_world',\n        ),\n        Case(\n            name='text_with_error',\n            inputs='Contains error marker',\n            expected_output='Error processing: Contains error marker',\n        ),\n    ],\n    evaluators=[SpanTracingEvaluator()],\n)\n\n# Run evaluation - spans are automatically captured since logfire is configured\n\nreport = dataset.evaluate_sync(process_text)\n\n# Print the report\n\nreport.print(include_input=True, include_output=True, include_durations=False)\n[](https://ai.pydantic.dev/evals/#__code_10_annotation_1)\n\n\"\"\"\n\n                                              Evaluation Summary: process_text\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n\n┃ Case ID         ┃ Inputs                ┃ Outputs\n┃ Scores                   ┃ Assertions ┃\n\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n\n│ normal_text     │ Hello World           │ Processed: hello_world\n│ performance_score: 1.00  │ ✔✗         │\n\n├─────────────────┼───────────────────────┼─────────────────────────────────────────┼──────────────────────────┼────────────┤\n\n│ text_with_error │ Contains error marker │ Error processing: Contains error\nmarker │ performance_score: 0     │ ✔✔         │\n\n├─────────────────┼───────────────────────┼─────────────────────────────────────────┼──────────────────────────┼────────────┤\n\n│ Averages        │                       │\n│ performance_score: 0.500 │ 75.0% ✔    │\n\n└─────────────────┴───────────────────────┴─────────────────────────────────────────┴──────────────────────────┴────────────┘\n\n\"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n## Generating Test Datasets\n\nPydantic Evals allows you to generate test datasets using LLMs with\n[`generate_dataset`](https://ai.pydantic.dev/api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset).\n\nDatasets can be generated in either JSON or YAML format, in both cases a JSON\nschema file is generated alongside the dataset and referenced in the dataset, so\nyou should get type checking and auto-completion in your editor.\n\ngenerate_dataset_example.py",
  "```\n\nfrom__future__import annotations\n\nfrompathlibimport Path\n\nfrompydanticimport BaseModel, Field\n\nfrompydantic_evalsimport Dataset\n\nfrompydantic_evals.generationimport generate_dataset\n\nclassQuestionInputs(BaseModel, use_attribute_docstrings=True):  \nDefine the schema for the inputs to the task.\n\n[](https://ai.pydantic.dev/evals/#__code_11_annotation_1)\n\n\"\"\"Model for question inputs.\"\"\"\n\n    question: str\n\"\"\"A question to answer\"\"\"\n\n    context: str | None = None\n\"\"\"Optional context for the question\"\"\"\n\nclassAnswerOutput(BaseModel, use_attribute_docstrings=True):  \nDefine the schema for the expected outputs of the task.\n\n[](https://ai.pydantic.dev/evals/#__code_11_annotation_2)\n\n\"\"\"Model for expected answer outputs.\"\"\"\n\n    answer: str\n\"\"\"The answer to the question\"\"\"\n\n    confidence: float = Field(ge=0, le=1)\n\"\"\"Confidence level (0-1)\"\"\"\n\nclassMetadataType(BaseModel, use_attribute_docstrings=True):  \nDefine the schema for the metadata of the test cases.\n\n[](https://ai.pydantic.dev/evals/#__code_11_annotation_3)\n\n\"\"\"Metadata model for test cases.\"\"\"\n\n    difficulty: str\n\"\"\"Difficulty level (easy, medium, hard)\"\"\"\n\n    category: str\n\"\"\"Question category\"\"\"\n\nasync defmain():\n\n    dataset = await generate_dataset(  \nCall\ngenerate_dataset[](https://ai.pydantic.dev/api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset)\nto create a\nDataset[](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset)\nwith 2 cases confirming to the schema.\n\n[](https://ai.pydantic.dev/evals/#__code_11_annotation_4)\n\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.yaml')\n    dataset.to_file(output_file)  \nSave the dataset to a YAML file, this will also write\nquestions_cases_schema.json with the schema JSON schema for questions_cases.yaml\nto make editing easier. The magic yaml-language-server comment is supported by\nat least vscode, jetbrains/pycharm (more details\nhere[](https://github.com/redhat-developer/yaml-language-server#using-inlined-\nschema)).\n\n[](https://ai.pydantic.dev/evals/#__code_11_annotation_5)\n\n    print(output_file.read_text())\n\"\"\"\n\n    # yaml-language-server: $schema=questions_cases_schema.json\n    cases:\n    - name: Easy Capital Question\n      inputs:\n        question: What is the capital of France?\n      metadata:\n        difficulty: easy\n        category: Geography\n      expected_output:\n        answer: Paris\n        confidence: 0.95\n      evaluators:\n      - EqualsExpected\n    - name: Challenging Landmark Question\n      inputs:\n        question: Which world-famous landmark is located on the banks of the Seine River?\n      metadata:\n        difficulty: hard\n        category: Landmarks\n      expected_output:\n        answer: Eiffel Tower\n        confidence: 0.9\n      evaluators:\n      - EqualsExpected\n    \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main(answer))` to run `main`)_\n\nYou can also write datasets as JSON files:\n\ngenerate_dataset_example_json.py",
  "```\n\nfrompathlibimport Path\n\nfromgenerate_dataset_exampleimport AnswerOutput, MetadataType, QuestionInputs\n\nfrompydantic_evalsimport Dataset\n\nfrompydantic_evals.generationimport generate_dataset\n\nasync defmain():\n\n    dataset = await generate_dataset(  \nGenerate the\nDataset[](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset)\nexactly as above.\n\n[](https://ai.pydantic.dev/evals/#__code_12_annotation_1)\n\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.json')\n    dataset.to_file(output_file)  \nSave the dataset to a JSON file, this will also write\nquestions_cases_schema.json with th JSON schema for questions_cases.json. This\ntime the $schema key is included in the JSON file to define the schema for IDEs\nto use while you edit the file, there's no formal spec for this, but it works in\nvscode and pycharm and is discussed at length in json-schema-org/json-schema-\nspec#828[](https://github.com/json-schema-org/json-schema-spec/issues/828).\n\n[](https://ai.pydantic.dev/evals/#__code_12_annotation_2)\n\n    print(output_file.read_text())\n\"\"\"\n\n    {\n      \"$schema\": \"questions_cases_schema.json\",\n      \"cases\": [\n        {\n          \"name\": \"Easy Capital Question\",\n          \"inputs\": {\n            \"question\": \"What is the capital of France?\"\n          },\n          \"metadata\": {\n            \"difficulty\": \"easy\",\n            \"category\": \"Geography\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Paris\",\n            \"confidence\": 0.95\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        },\n        {\n          \"name\": \"Challenging Landmark Question\",\n          \"inputs\": {\n            \"question\": \"Which world-famous landmark is located on the banks of the Seine River?\"\n          },\n          \"metadata\": {\n            \"difficulty\": \"hard\",\n            \"category\": \"Landmarks\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Eiffel Tower\",\n            \"confidence\": 0.9\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        }\n      ]\n    }\n    \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" — you'll need to\nadd`asyncio.run(main(answer))` to run `main`)_\n\n## Integration with Logfire\n\nPydantic Evals is implemented using OpenTelemetry to record traces of the\nevaluation process. These traces contain all the information included in the\nterminal output as attributes, but also include full tracing from the executions\nof the evaluation task function.\n\nYou can send these traces to any OpenTelemetry-compatible backend, including\n[Pydantic Logfire](https://logfire.pydantic.dev/docs).\n\nAll you need to do is configure Logfire via `logfire.configure`:\n\nlogfire_integration.py```\n\nimportlogfire\n\nfromjudge_recipesimport recipe_dataset, transform_recipe\n\nlogfire.configure(\n\n    send_to_logfire='if-token-present',  \nThe send_to_logfire argument controls when traces are sent to Logfire. You can\nset it to 'if-token-present' to send data to Logfire only if the LOGFIRE_TOKEN\nenvironment variable is set. See the Logfire configuration\ndocs[](https://logfire.pydantic.dev/docs/reference/configuration/) for more\ndetails.\n\n[](https://ai.pydantic.dev/evals/#__code_13_annotation_1)\n\n    environment='development',  \nThe environment argument sets the environment for the traces. It's a good idea\nto set this to 'development' when running tests or evaluations and sending data\nto a project with production data, to make it easier to filter these traces out\nwhile reviewing data from your production environment(s).\n\n[](https://ai.pydantic.dev/evals/#__code_13_annotation_2)\n\n    service_name='evals',  \nThe service_name argument sets the service name for the traces. This is\ndisplayed in the Logfire UI to help you identify the source of the associated\nspans.\n\n[](https://ai.pydantic.dev/evals/#__code_13_annotation_3)\n\n)\n\nrecipe_dataset.evaluate_sync(transform_recipe)",
  "```\n\nLogfire has some special integration with Pydantic Evals traces, including a\ntable view of the evaluation results on the evaluation root span (which is\ngenerated in each call to\n[`Dataset.evaluate`](https://ai.pydantic.dev/api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluate)):\n\n[![Logfire Evals Overview](https://ai.pydantic.dev/img/logfire-evals-\noverview.png)](https://ai.pydantic.dev/img/logfire-evals-overview.png)\n\nand a detailed view of the inputs and outputs for the execution of each case:\n\n[![Logfire Evals Case](https://ai.pydantic.dev/img/logfire-evals-\ncase.png)](https://ai.pydantic.dev/img/logfire-evals-case.png)\n\nIn addition, any OpenTelemetry spans generated during the evaluation process\nwill be sent to Logfire, allowing you to visualize the full execution of the\ncode called during the evaluation process:\n\n[![Logfire Evals Case Trace](https://ai.pydantic.dev/img/logfire-evals-case-\ntrace.png)](https://ai.pydantic.dev/img/logfire-evals-case-trace.png)\n\nThis can be especially helpful when attempting to write evaluators that make use\nof the `span_tree` property of the\n[`EvaluatorContext`](https://ai.pydantic.dev/api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext),\nas described in the [OpenTelemetry\nIntegration](https://ai.pydantic.dev/evals/#opentelemetry-integration) section\nabove.\n\nThis allows you to write evaluations that depend on information about which code\npaths were executed during the call to the task function without needing to\nmanually instrument the code being evaluated, as long as the code being\nevaluated is already adequately instrumented with OpenTelemetry. In the case of\nPydanticAI agents, for example, this can be used to ensure specific tools are\n(or are not) called during the execution of specific cases.\n\nUsing OpenTelemetry in this way also means that all data used to evaluate the\ntask executions will be accessible in the traces produced by production runs of\nthe code, making it straightforward to perform the same evaluations on\nproduction data.\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/examples/#examples)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nExamples\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")",
  "* [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)",
  "* [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Usage  ](https://ai.pydantic.dev/examples/#usage)\n    * [ Installing required dependencies  ](https://ai.pydantic.dev/examples/#installing-required-dependencies)\n    * [ Setting model environment variables  ](https://ai.pydantic.dev/examples/#setting-model-environment-variables)\n    * [ Running Examples  ](https://ai.pydantic.dev/examples/#running-examples)\n\n# Examples\n\nExamples of how to use PydanticAI and what it can do.\n\n## Usage\n\nThese examples are distributed with `pydantic-ai` so you can run them either by\ncloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by\nsimply installing `pydantic-ai` from PyPI with `pip` or `uv`.\n\n### Installing required dependencies\n\nEither way you'll need to install extra dependencies to run some examples, you\njust need to install the `examples` optional dependency group.\n\nIf you've installed `pydantic-ai` via pip/uv, you can install the extra\ndependencies with:\n\n[pip](https://ai.pydantic.dev/examples/#__tabbed_1_1)[uv](https://ai.pydantic.dev/examples/#__tabbed_1_2)\n\n```\n\npip\"pydantic-ai[examples]\"\n\n```\n\n```\n\nuv\"pydantic-ai[examples]\"\n\n```\n\nIf you clone the repo, you should instead use `uv sync --extra examples` to\ninstall extra dependencies.\n\n### Setting model environment variables\n\nThese examples will need you to set up authentication with one or more of the\nLLMs, see the [model configuration](https://ai.pydantic.dev/models/) docs for\ndetails on how to do this.\n\nTL;DR: in most cases you'll need to set one of the following environment\nvariables:\n\n[OpenAI](https://ai.pydantic.dev/examples/#__tabbed_2_1)[Google\nGemini](https://ai.pydantic.dev/examples/#__tabbed_2_2)\n\n```\n\nexportOPENAI_API_KEY=your-api-key\n\n```\n\n```\n\nexportGEMINI_API_KEY=your-api-key\n\n```\n\n### Running Examples\n\nTo run the examples (this will work whether you installed `pydantic_ai`, or\ncloned the repo), run:\n\n[pip](https://ai.pydantic.dev/examples/#__tabbed_3_1)[uv](https://ai.pydantic.dev/examples/#__tabbed_3_2)\n\n```\n\npython\n\n```\n\n```\n\nuv\n\n```\n\nFor examples, to run the very simple\n[`pydantic_model`](https://ai.pydantic.dev/examples/pydantic-model/) example:\n\n[pip](https://ai.pydantic.dev/examples/#__tabbed_4_1)[uv](https://ai.pydantic.dev/examples/#__tabbed_4_2)\n\n```\n\npython\n\n```\n\n```\n\nuv\n\n```\n\nIf you like one-liners and you're using uv, you can run a pydantic-ai example\nwith zero setup:\n\n```\n\nOPENAI_API_KEY='your-api-key'\\\n\n\"pydantic-ai[examples]\"\\\n\n```\n\n* * *\nYou'll probably want to edit examples in addition to just running them. You can\ncopy the examples to a new directory with:\n\n[pip](https://ai.pydantic.dev/examples/#__tabbed_5_1)[uv](https://ai.pydantic.dev/examples/#__tabbed_5_2)\n\n```\n\npython\n\n```\n\n```\n\nuv",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/examples/pydantic-model/#pydantic-\nmodel)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nPydantic Model\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * Pydantic Model  [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n      * [ Running the Example  ](https://ai.pydantic.dev/examples/pydantic-model/#running-the-example)\n      * [ Example Code  ](https://ai.pydantic.dev/examples/pydantic-model/#example-code)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [",
  "pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Running the Example  ](https://ai.pydantic.dev/examples/pydantic-model/#running-the-example)\n  * [ Example Code  ](https://ai.pydantic.dev/examples/pydantic-model/#example-code)\n\n# Pydantic Model\n\nSimple example of using PydanticAI to construct a Pydantic model from a text\ninput.\n\nDemonstrates:\n\n  * [structured `output_type`](https://ai.pydantic.dev/output/#structured-output)\n\n## Running the Example\n\nWith [dependencies installed and environment variables\nset](https://ai.pydantic.dev/examples/#usage), run:\n\n[pip](https://ai.pydantic.dev/examples/pydantic-\nmodel/#__tabbed_1_1)[uv](https://ai.pydantic.dev/examples/pydantic-\nmodel/#__tabbed_1_2)\n\n```\n\npython\n\n```\n\n```\n\nuv\n\n```\n\nThis examples uses `openai:gpt-4o` by default, but it works well with other\nmodels, e.g. you can run it with Gemini using:\n\n[pip](https://ai.pydantic.dev/examples/pydantic-\nmodel/#__tabbed_2_1)[uv](https://ai.pydantic.dev/examples/pydantic-\nmodel/#__tabbed_2_2)\n\n```\n\nPYDANTIC_AI_MODEL=gemini-1.5-pro\n\n```\n\n```\n\nPYDANTIC_AI_MODEL=gemini-1.5-pro\n\n```\n\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)\n\n## Example Code\n\npydantic_model.py```\n\nimportos\n\nimportlogfire\n\nfrompydanticimport BaseModel\n\nfrompydantic_aiimport Agent\n\n# 'if-token-present' means nothing will be sent (and the example will work) if\nyou don't have logfire configured\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nlogfire.instrument_pydantic_ai()\n\nclassMyModel(BaseModel):\n\n    city: str\n    country: str\n\nmodel = os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o')\n\nprint(f'Using model: {model}')\n\nagent = Agent(model, output_type=MyModel)\n\nif __name__ == '__main__':\n\n    result = agent.run_sync('The windy city in the US of A.')\n    print(result.output)\n    print(result.usage())",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/examples/weather-agent/#running-the-\nexample)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nWeather agent\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * [ Graphs  ](https://ai.pydantic.dev/graph/)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * Weather agent  [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n      * [ Running the Example  ](https://ai.pydantic.dev/examples/weather-agent/#running-the-example)\n      * [ Example Code  ](https://ai.pydantic.dev/examples/weather-agent/#example-code)\n      * [ Running the UI  ](https://ai.pydantic.dev/examples/weather-agent/#running-the-ui)\n      * [ UI Code  ](https://ai.pydantic.dev/examples/weather-agent/#ui-code)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)\n    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)",
  "* [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Running the Example  ](https://ai.pydantic.dev/examples/weather-agent/#running-the-example)\n  * [ Example Code  ](https://ai.pydantic.dev/examples/weather-agent/#example-code)\n  * [ Running the UI  ](https://ai.pydantic.dev/examples/weather-agent/#running-the-ui)\n  * [ UI Code  ](https://ai.pydantic.dev/examples/weather-agent/#ui-code)\n\n# Weather agent\n\nExample of PydanticAI with multiple tools which the LLM needs to call in turn to\nanswer a question.\n\nDemonstrates:\n\n  * [tools](https://ai.pydantic.dev/tools/)\n  * [agent dependencies](https://ai.pydantic.dev/dependencies/)\n  * [streaming text responses](https://ai.pydantic.dev/output/#streaming-text)\n  * Building a [Gradio](https://www.gradio.app/) UI for the agent\n\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in\nmultiple locations, the agent will use the `get_lat_lng` tool to get the\nlatitude and longitude of the locations, then use the `get_weather` tool to get\nthe weather for those locations.\n\n## Running the Example\n\nTo run this example properly, you might want to add two extra API keys **(Note\nif either key is missing, the code will fall back to dummy data, so they're not\nrequired)** :\n\n  * A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via `WEATHER_API_KEY`\n  * A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via `GEO_API_KEY`\n\nWith [dependencies installed and environment variables\nset](https://ai.pydantic.dev/examples/#usage), run:\n\n[pip](https://ai.pydantic.dev/examples/weather-\nagent/#__tabbed_1_1)[uv](https://ai.pydantic.dev/examples/weather-\nagent/#__tabbed_1_2)\n\n```\n\npython\n\n```\n\n```\n\nuv\n\n```\n\n## Example Code\n\npydantic_ai_examples/weather_agent.py",
  "```\n\nfrom__future__import annotations as _annotations\n\nimportasyncio\n\nimportos\n\nimporturllib.parse\n\nfromdataclassesimport dataclass\n\nfromtypingimport Any\n\nimportlogfire\n\nfromdevtoolsimport debug\n\nfromhttpximport AsyncClient\n\nfrompydantic_aiimport Agent, ModelRetry, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if\nyou don't have logfire configured\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nlogfire.instrument_pydantic_ai()\n\n@dataclass\n\nclassDeps:\n\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\nweather_agent = Agent(\n\n    'openai:gpt-4o',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    instructions=(\n        'Be concise, reply with one sentence.'\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    deps_type=Deps,\n    retries=2,\n)\n\n@weather_agent.tool\n\nasync defget_lat_lng(\n\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n\n\"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    if ctx.deps.geo_api_key is None:\n        # if no API key is provided, return a dummy response (London)\n        return {'lat': 51.1, 'lng': -0.1}\n\n    params = {'access_token': ctx.deps.geo_api_key}\n    loc = urllib.parse.quote(location_description)\n    r = await ctx.deps.client.get(\n        f'https://api.mapbox.com/geocoding/v5/mapbox.places/{loc}.json', params=params\n    )\n    r.raise_for_status()\n    data = r.json()\n\n    if features := data['features']:\n        lat, lng = features[0]['center']\n        return {'lat': lat, 'lng': lng}\n    else:\n        raise ModelRetry('Could not find the location')\n\n@weather_agent.tool\n\nasync defget_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str,\nAny]:\n\n\"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    if ctx.deps.weather_api_key is None:\n        # if no API key is provided, return a dummy response\n        return {'temperature': '21 °C', 'description': 'Sunny'}\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get(\n            'https://api.tomorrow.io/v4/weather/realtime', params=params\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        1000: 'Clear, Sunny',\n        1100: 'Mostly Clear',\n        1101: 'Partly Cloudy',\n        1102: 'Mostly Cloudy',\n        1001: 'Cloudy',\n        2000: 'Fog',\n        2100: 'Light Fog',\n        4000: 'Drizzle',\n        4001: 'Rain',\n        4200: 'Light Rain',\n        4201: 'Heavy Rain',\n        5000: 'Snow',\n        5001: 'Flurries',\n        5100: 'Light Snow',\n        5101: 'Heavy Snow',\n        6000: 'Freezing Drizzle',\n        6001: 'Freezing Rain',\n        6200: 'Light Freezing Rain',\n        6201: 'Heavy Freezing Rain',\n        7000: 'Ice Pellets',\n        7101: 'Heavy Ice Pellets',\n        7102: 'Light Ice Pellets',\n        8000: 'Thunderstorm',\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\nasync defmain():\n\n    async with AsyncClient() as client:\n        logfire.instrument_httpx(client, capture_all=True)\n        # create a free API key at https://www.tomorrow.io/weather-api/\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        # create a free API key at https://www.mapbox.com/\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        debug(result)\n        print('Response:', result.output)\n\nif __name__ == '__main__':\n\n    asyncio.run(main())\n\n```\n\n## Running the UI\n\nYou can build multi-turn chat applications for your agent with\n[Gradio](https://www.gradio.app/), a framework for building AI web applications\nentirely in python. Gradio comes with built-in chat components and agent support\nso the entire UI will be implemented in a single python file!\n\nHere's what the UI looks like for the weather agent:\n\nNote, to run the UI, you'll need Python 3.10+.\n\n```\n\npip=5.9.0\n\npython/uv-run",
  "```\n\n## UI Code\n\npydantic_ai_examples/weather_agent_gradio.py```\n\nfrom__future__import annotations as _annotations\n\nimportjson\n\nimportos\n\nfromhttpximport AsyncClient\n\nfrompydantic_ai.messagesimport ToolCallPart, ToolReturnPart\n\nfrompydantic_ai_examples.weather_agentimport Deps, weather_agent\n\ntry:\n\n    importgradioasgr\nexcept ImportError as e:\n\n    raise ImportError(\n        'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n    ) frome\n\nTOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather\nAPI'}\n\nclient = AsyncClient()\n\nweather_api_key = os.getenv('WEATHER_API_KEY')\n\n# create a free API key at https://geocode.maps.co/\n\ngeo_api_key = os.getenv('GEO_API_KEY')\n\ndeps = Deps(client=client, weather_api_key=weather_api_key,\ngeo_api_key=geo_api_key)\n\nasync defstream_from_agent(prompt: str, chatbot: list[dict], past_messages:\nlist):\n\n    chatbot.append({'role': 'user', 'content': prompt})\n    yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n    async with weather_agent.run_stream(\n        prompt, deps=deps, message_history=past_messages\n    ) as result:\n        for message in result.new_messages():\n            for call in message.parts:\n                if isinstance(call, ToolCallPart):\n                    call_args = call.args_as_json_str()\n                    metadata = {\n                        'title': f'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n                    }\n                    if call.tool_call_id is not None:\n                        metadata['id'] = call.tool_call_id\n\n                    gr_message = {\n                        'role': 'assistant',\n                        'content': 'Parameters: ' + call_args,\n                        'metadata': metadata,\n                    }\n                    chatbot.append(gr_message)\n                if isinstance(call, ToolReturnPart):\n                    for gr_message in chatbot:\n                        if (\n                            gr_message.get('metadata', {}).get('id', '')\n                            == call.tool_call_id\n                        ):\n                            gr_message['content'] += (\n                                f'\\nOutput: {json.dumps(call.content)}'\n                            )\n                yield gr.skip(), chatbot, gr.skip()\n        chatbot.append({'role': 'assistant', 'content': ''})\n        async for message in result.stream_text():\n            chatbot[-1]['content'] = message\n            yield gr.skip(), chatbot, gr.skip()\n        past_messages = result.all_messages()\n\n        yield gr.Textbox(interactive=True), gr.skip(), past_messages\n\nasync defhandle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n\n    new_history = chatbot[: retry_data.index]\n    previous_prompt = chatbot[retry_data.index]['content']\n    past_messages = past_messages[: retry_data.index]\n    async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n        yield update\n\ndefundo(chatbot, past_messages: list, undo_data: gr.UndoData):\n\n    new_history = chatbot[: undo_data.index]\n    past_messages = past_messages[: undo_data.index]\n    return chatbot[undo_data.index]['content'], new_history, past_messages\n\ndefselect_data(message: gr.SelectData) -> str:\n\n    return message.value['text']\n\nwith gr.Blocks() as demo:\n\n    gr.HTML(\n\"\"\"\n\n<div style=\"display: flex; justify-content: center; align-items: center; gap:\n2rem; padding: 1rem; width: 100%\">\n\n    <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n    <div>\n        <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n        <h3 style=\"margin: 0 0 0.5rem 0\">\n            This assistant answer your weather questions.\n        </h3>\n    </div>\n</div>\n\n\"\"\"\n\n    )\n    past_messages = gr.State([])\n    chatbot = gr.Chatbot(\n        label='Packing Assistant',\n        type='messages',\n        avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n        examples=[\n            {'text': 'What is the weather like in Miami?'},\n            {'text': 'What is the weather like in London?'},\n        ],\n    )\n    with gr.Row():\n        prompt = gr.Textbox(\n            lines=1,\n            show_label=False,\n            placeholder='What is the weather like in New York City?',\n        )\n    generation = prompt.submit(\n        stream_from_agent,\n        inputs=[prompt, chatbot, past_messages],\n        outputs=[prompt, chatbot, past_messages],\n    )\n    chatbot.example_select(select_data, None, [prompt])\n    chatbot.retry(\n        handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n    )\n    chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n\nif __name__ == '__main__':\n\n    demo.launch()",
  "```\n\n© Pydantic Services Inc. 2024 to present\n\n\n\n[ Skip to content ](https://ai.pydantic.dev/graph/#graphs)\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\")\n\nPydanticAI\n\nGraphs\n\nType to start searching\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/\n\"PydanticAI\") PydanticAI\n\n[ pydantic/pydantic-ai\n\n  * v0.3.4\n  * 10.5k\n  * 971\n\n](https://github.com/pydantic/pydantic-ai \"Go to repository\")\n\n  * [ Introduction  ](https://ai.pydantic.dev/)\n  * [ Installation  ](https://ai.pydantic.dev/install/)\n  * [ Getting Help  ](https://ai.pydantic.dev/help/)\n  * [ Contributing  ](https://ai.pydantic.dev/contributing/)\n  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)\n  * [ Upgrade Guide  ](https://ai.pydantic.dev/changelog/)\n  * Documentation \n    * [ Agents  ](https://ai.pydantic.dev/agents/)\n    * [ Models  ](https://ai.pydantic.dev/models/)\n      * [ OpenAI  ](https://ai.pydantic.dev/models/openai/)\n      * [ Anthropic  ](https://ai.pydantic.dev/models/anthropic/)\n      * [ Gemini  ](https://ai.pydantic.dev/models/gemini/)\n      * [ Google  ](https://ai.pydantic.dev/models/google/)\n      * [ Bedrock  ](https://ai.pydantic.dev/models/bedrock/)\n      * [ Cohere  ](https://ai.pydantic.dev/models/cohere/)\n      * [ Groq  ](https://ai.pydantic.dev/models/groq/)\n      * [ Mistral  ](https://ai.pydantic.dev/models/mistral/)\n    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)\n    * [ Function Tools  ](https://ai.pydantic.dev/tools/)\n    * [ Common Tools  ](https://ai.pydantic.dev/common-tools/)\n    * [ Output  ](https://ai.pydantic.dev/output/)\n    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)\n    * [ Unit testing  ](https://ai.pydantic.dev/testing/)\n    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)\n    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)\n    * Graphs  [ Graphs  ](https://ai.pydantic.dev/graph/)\n      * [ Installation  ](https://ai.pydantic.dev/graph/#installation)\n      * [ Graph Types  ](https://ai.pydantic.dev/graph/#graph-types)\n        * [ GraphRunContext  ](https://ai.pydantic.dev/graph/#graphruncontext)\n        * [ End  ](https://ai.pydantic.dev/graph/#end)\n        * [ Nodes  ](https://ai.pydantic.dev/graph/#nodes)\n        * [ Graph  ](https://ai.pydantic.dev/graph/#graph)\n      * [ Stateful Graphs  ](https://ai.pydantic.dev/graph/#stateful-graphs)\n      * [ GenAI Example  ](https://ai.pydantic.dev/graph/#genai-example)\n      * [ Iterating Over a Graph  ](https://ai.pydantic.dev/graph/#iterating-over-a-graph)\n        * [ Using Graph.iter for async for iteration  ](https://ai.pydantic.dev/graph/#using-graphiter-for-async-for-iteration)\n        * [ Using GraphRun.next(node) manually  ](https://ai.pydantic.dev/graph/#using-graphrunnextnode-manually)\n      * [ State Persistence  ](https://ai.pydantic.dev/graph/#state-persistence)\n        * [ Example: Human in the loop.  ](https://ai.pydantic.dev/graph/#example-human-in-the-loop)\n      * [ Dependency Injection  ](https://ai.pydantic.dev/graph/#dependency-injection)\n      * [ Mermaid Diagrams  ](https://ai.pydantic.dev/graph/#mermaid-diagrams)\n        * [ Setting Direction of the State Diagram  ](https://ai.pydantic.dev/graph/#setting-direction-of-the-state-diagram)\n    * [ Evals  ](https://ai.pydantic.dev/evals/)\n    * [ Image, Audio, Video & Document Input  ](https://ai.pydantic.dev/input/)\n    * [ Thinking  ](https://ai.pydantic.dev/thinking/)\n    * [ Direct Model Requests  ](https://ai.pydantic.dev/direct/)\n    * [ MCP  ](https://ai.pydantic.dev/mcp/)\n      * [ Client  ](https://ai.pydantic.dev/mcp/client/)\n      * [ Server  ](https://ai.pydantic.dev/mcp/server/)\n      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)\n    * [ A2A  ](https://ai.pydantic.dev/a2a/)\n    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)\n  * [ Examples  ](https://ai.pydantic.dev/examples/)\n    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)\n    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)\n    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)\n    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)\n    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)\n    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)\n    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)\n    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)\n    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)\n    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)\n  * API Reference \n    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)\n    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)",
  "* [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)\n    * [ pydantic_ai.output  ](https://ai.pydantic.dev/api/output/)\n    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)\n    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)\n    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)\n    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)\n    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)\n    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)\n    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)\n    * [ pydantic_ai.direct  ](https://ai.pydantic.dev/api/direct/)\n    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)\n    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)\n    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)\n    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)\n    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)\n    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)\n    * [ pydantic_ai.models.google  ](https://ai.pydantic.dev/api/models/google/)\n    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)\n    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)\n    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)\n    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)\n    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)\n    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)\n    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)\n    * [ pydantic_ai.models.mcp_sampling  ](https://ai.pydantic.dev/api/models/mcp-sampling/)\n    * [ pydantic_ai.profiles  ](https://ai.pydantic.dev/api/profiles/)\n    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)\n    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)\n    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)\n    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)\n    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)\n    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)\n    * [ pydantic_evals.dataset  ](https://ai.pydantic.dev/api/pydantic_evals/dataset/)\n    * [ pydantic_evals.evaluators  ](https://ai.pydantic.dev/api/pydantic_evals/evaluators/)\n    * [ pydantic_evals.reporting  ](https://ai.pydantic.dev/api/pydantic_evals/reporting/)\n    * [ pydantic_evals.otel  ](https://ai.pydantic.dev/api/pydantic_evals/otel/)\n    * [ pydantic_evals.generation  ](https://ai.pydantic.dev/api/pydantic_evals/generation/)\n    * [ fasta2a  ](https://ai.pydantic.dev/api/fasta2a/)\n\n  * [ Installation  ](https://ai.pydantic.dev/graph/#installation)\n  * [ Graph Types  ](https://ai.pydantic.dev/graph/#graph-types)\n    * [ GraphRunContext  ](https://ai.pydantic.dev/graph/#graphruncontext)\n    * [ End  ](https://ai.pydantic.dev/graph/#end)\n    * [ Nodes  ](https://ai.pydantic.dev/graph/#nodes)\n    * [ Graph  ](https://ai.pydantic.dev/graph/#graph)\n  * [ Stateful Graphs  ](https://ai.pydantic.dev/graph/#stateful-graphs)\n  * [ GenAI Example  ](https://ai.pydantic.dev/graph/#genai-example)\n  * [ Iterating Over a Graph  ](https://ai.pydantic.dev/graph/#iterating-over-a-graph)\n    * [ Using Graph.iter for async for iteration  ](https://ai.pydantic.dev/graph/#using-graphiter-for-async-for-iteration)\n    * [ Using GraphRun.next(node) manually  ](https://ai.pydantic.dev/graph/#using-graphrunnextnode-manually)\n  * [ State Persistence  ](https://ai.pydantic.dev/graph/#state-persistence)\n    * [ Example: Human in the loop.  ](https://ai.pydantic.dev/graph/#example-human-in-the-loop)\n  * [ Dependency Injection  ](https://ai.pydantic.dev/graph/#dependency-injection)\n  * [ Mermaid Diagrams  ](https://ai.pydantic.dev/graph/#mermaid-diagrams)\n    * [ Setting Direction of the State Diagram  ](https://ai.pydantic.dev/graph/#setting-direction-of-the-state-diagram)\n\n# Graphs\n\nDon't use a nail gun unless you need a nail gun\n\nIf PydanticAI [agents](https://ai.pydantic.dev/agents/) are a hammer, and\n[multi-agent workflows](https://ai.pydantic.dev/multi-agent-applications/) are a\nsledgehammer, then graphs are a nail gun:",
  "* sure, nail guns look cooler than hammers\n  * but nail guns take a lot more setup than hammers\n  * and nail guns don't make you a better builder, they make you a builder with a nail gun\n  * Lastly, (and at the risk of torturing this metaphor), if you're a fan of medieval tools like mallets and untyped Python, you probably won't like nail guns or our approach to graphs. (But then again, if you're not a fan of type hints in Python, you've probably already bounced off PydanticAI to use one of the toy agent frameworks — good luck, and feel free to borrow my sledgehammer when you realize you need it)\n\nIn short, graphs are a powerful tool, but they're not the right tool for every\njob. Please consider other [multi-agent\napproaches](https://ai.pydantic.dev/multi-agent-applications/) before\nproceeding.\n\nIf you're not confident a graph-based approach is a good idea, it might be\nunnecessary.\n\nGraphs and finite state machines (FSMs) are a powerful abstraction to model,\nexecute, control and visualize complex workflows.\n\nAlongside PydanticAI, we've developed `pydantic-graph` — an async graph and\nstate machine library for Python where nodes and edges are defined using type\nhints.\n\nWhile this library is developed as part of PydanticAI; it has no dependency on\n`pydantic-ai` and can be considered as a pure graph-based state machine library.\nYou may find it useful whether or not you're using PydanticAI or even building\nwith GenAI.\n\n`pydantic-graph` is designed for advanced users and makes heavy use of Python\ngenerics and type hints. It is not designed to be as beginner-friendly as\nPydanticAI.\n\n## Installation\n\n`pydantic-graph` is a required dependency of `pydantic-ai`, and an optional\ndependency of `pydantic-ai-slim`, see [installation\ninstructions](https://ai.pydantic.dev/install/#slim-install) for more\ninformation. You can also install it directly:\n\n[pip](https://ai.pydantic.dev/graph/#__tabbed_1_1)[uv](https://ai.pydantic.dev/graph/#__tabbed_1_2)\n\n```\n\npip\n\n```\n\n```\n\nuv",
  "```\n\n## Graph Types\n\n`pydantic-graph` is made up of a few key components:\n\n### GraphRunContext\n\n[`GraphRunContext`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext)\n— The context for the graph run, similar to PydanticAI's\n[`RunContext`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.RunContext).\nThis holds the state of the graph and dependencies and is passed to nodes when\nthey're run.\n\n`GraphRunContext` is generic in the state type of the graph it's used in,\n[`StateT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT).\n\n### End\n\n[`End`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End)\n— return value to indicate the graph run should end.\n\n`End` is generic in the graph return type of the graph it's used in,\n[`RunEndT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT).\n\n### Nodes\n\nSubclasses of\n[`BaseNode`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode)\ndefine nodes for execution in the graph.\n\nNodes, which are generally\n[`dataclass`es](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass),\ngenerally consist of:\n\n  * fields containing any parameters required/optional when calling the node\n  * the business logic to execute the node, in the [`run`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run) method\n  * return annotations of the [`run`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run) method, which are read by `pydantic-graph` to determine the outgoing edges of the node\n\nNodes are generic in:\n\n  * **state** , which must have the same type as the state of graphs they're included in, [`StateT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT) has a default of `None`, so if you're not using state you can omit this generic parameter, see [stateful graphs](https://ai.pydantic.dev/graph/#stateful-graphs) for more information\n  * **deps** , which must have the same type as the deps of the graph they're included in, [`DepsT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT) has a default of `None`, so if you're not using deps you can omit this generic parameter, see [dependency injection](https://ai.pydantic.dev/graph/#dependency-injection) for more information\n  * **graph return type** — this only applies if the node returns [`End`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End). [`RunEndT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT) has a default of [Never](https://docs.python.org/3/library/typing.html#typing.Never) so this generic parameter can be omitted if the node doesn't return `End`, but must be included if it does.\n\nHere's an example of a start or intermediate node in a graph — it can't end the\nrun as it doesn't return\n[`End`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End):\n\nintermediate_node.py```\n\nfromdataclassesimport dataclass\n\nfrompydantic_graphimport BaseNode, GraphRunContext\n\n@dataclass\n\nclassMyNode(BaseNode[MyState]):\n[](https://ai.pydantic.dev/graph/#__code_2_annotation_1)\n\n    foo: int  [](https://ai.pydantic.dev/graph/#__code_2_annotation_2)\n\n    async defrun(\n        self,\n        ctx: GraphRunContext[MyState],  [](https://ai.pydantic.dev/graph/#__code_2_annotation_3)\n    ) -> AnotherNode:  [](https://ai.pydantic.dev/graph/#__code_2_annotation_4)\n        ...\n        return AnotherNode()\n\n```\n\nWe could extend `MyNode` to optionally end the run if `foo` is divisible by 5:\n\nintermediate_or_end_node.py```\n\nfromdataclassesimport dataclass\n\nfrompydantic_graphimport BaseNode, End, GraphRunContext\n\n@dataclass\n\nclassMyNode(BaseNode[MyState, None, int]):\n[](https://ai.pydantic.dev/graph/#__code_3_annotation_1)\n\n    foo: int\n\n    async defrun(\n        self,\n        ctx: GraphRunContext[MyState],\n    ) -> AnotherNode | End[int]:  [](https://ai.pydantic.dev/graph/#__code_3_annotation_2)\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return AnotherNode()\n\n```\n\n### Graph\n\n[`Graph`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph)\n— this is the execution graph itself, made up of a set of [node\nclasses](https://ai.pydantic.dev/graph/#nodes) (i.e., `BaseNode` subclasses).\n\n`Graph` is generic in:\n\n  * **state** the state type of the graph, [`StateT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT)\n  * **deps** the deps type of the graph, [`DepsT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT)\n  * **graph return type** the return type of the graph run, [`RunEndT`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT)\n\nHere's an example of a simple graph:\n\ngraph_example.py",
  "```\n\nfrom__future__import annotations\n\nfromdataclassesimport dataclass\n\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\n\n@dataclass\n\nclassDivisibleBy5(BaseNode[None, None, int]):\n[](https://ai.pydantic.dev/graph/#__code_4_annotation_1)\n\n    foo: int\n\n    async defrun(\n        self,\n        ctx: GraphRunContext,\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n@dataclass\n\nclassIncrement(BaseNode):\n[](https://ai.pydantic.dev/graph/#__code_4_annotation_2)\n\n    foo: int\n\n    async defrun(self, ctx: GraphRunContext) -> DivisibleBy5:\n        return DivisibleBy5(self.foo + 1)\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\n[](https://ai.pydantic.dev/graph/#__code_4_annotation_3)\n\nresult = fives_graph.run_sync(DivisibleBy5(4))\n[](https://ai.pydantic.dev/graph/#__code_4_annotation_4)\n\nprint(result.output)\n\n#> 5\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+)_\n\nA [mermaid diagram](https://ai.pydantic.dev/graph/#mermaid-diagrams) for this\ngraph can be generated with the following code:\n\ngraph_example_diagram.py```\n\nfromgraph_exampleimport DivisibleBy5, fives_graph\n\nfives_graph.mermaid_code(start_node=DivisibleBy5)\n\n```\n\n```\n\n---\ntitle: fives_graph\n\n---\nstateDiagram-v2\n\n  [*] --> DivisibleBy5\n\n  DivisibleBy5 --> Increment\n\n  DivisibleBy5 --> [*]\n\n  Increment --> DivisibleBy5\n\n```\n\nIn order to visualize a graph within a `jupyter-notebook`, `IPython.display`\nneeds to be used:\n\njupyter_display_mermaid.py```\n\nfromgraph_exampleimport DivisibleBy5, fives_graph\n\nfromIPython.displayimport Image, display\n\ndisplay(Image(fives_graph.mermaid_image(start_node=DivisibleBy5)))\n\n```\n\n## Stateful Graphs\n\nThe \"state\" concept in `pydantic-graph` provides an optional way to access and\nmutate an object (often a `dataclass` or Pydantic model) as nodes run in a\ngraph. If you think of Graphs as a production line, then your state is the\nengine being passed along the line and built up by each node as the graph is\nrun.\n\n`pydantic-graph` provides state persistence, with the state recorded after each\nnode is run. (See [State Persistence](https://ai.pydantic.dev/graph/#state-\npersistence).)\n\nHere's an example of a graph which represents a vending machine where the user\nmay insert coins and select a product to purchase.\n\nvending_machine.py",
  "```\n\nfrom__future__import annotations\n\nfromdataclassesimport dataclass\n\nfromrich.promptimport Prompt\n\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\n\n@dataclass\n\nclassMachineState:  [](https://ai.pydantic.dev/graph/#__code_7_annotation_1)\n\n    user_balance: float = 0.0\n    product: str | None = None\n\n@dataclass\n\nclassInsertCoin(BaseNode[MachineState]):\n[](https://ai.pydantic.dev/graph/#__code_7_annotation_3)\n\n    async defrun(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:  [](https://ai.pydantic.dev/graph/#__code_7_annotation_16)\n        return CoinsInserted(float(Prompt.ask('Insert coins')))  [](https://ai.pydantic.dev/graph/#__code_7_annotation_4)\n\n@dataclass\n\nclassCoinsInserted(BaseNode[MachineState]):\n\n    amount: float  [](https://ai.pydantic.dev/graph/#__code_7_annotation_5)\n\n    async defrun(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> SelectProduct | Purchase:  [](https://ai.pydantic.dev/graph/#__code_7_annotation_17)\n        ctx.state.user_balance += self.amount  [](https://ai.pydantic.dev/graph/#__code_7_annotation_6)\n        if ctx.state.product is not None:  [](https://ai.pydantic.dev/graph/#__code_7_annotation_7)\n            return Purchase(ctx.state.product)\n        else:\n            return SelectProduct()\n\n@dataclass\n\nclassSelectProduct(BaseNode[MachineState]):\n\n    async defrun(self, ctx: GraphRunContext[MachineState]) -> Purchase:\n        return Purchase(Prompt.ask('Select product'))\n\nPRODUCT_PRICES = {  [](https://ai.pydantic.dev/graph/#__code_7_annotation_2)\n\n    'water': 1.25,\n    'soda': 1.50,\n    'crisps': 1.75,\n    'chocolate': 2.00,\n}\n\n@dataclass\n\nclassPurchase(BaseNode[MachineState, None, None]):\n[](https://ai.pydantic.dev/graph/#__code_7_annotation_18)\n\n    product: str\n\n    async defrun(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> End | InsertCoin | SelectProduct:\n        if price := PRODUCT_PRICES.get(self.product):  [](https://ai.pydantic.dev/graph/#__code_7_annotation_8)\n            ctx.state.product = self.product  [](https://ai.pydantic.dev/graph/#__code_7_annotation_9)\n            if ctx.state.user_balance >= price:  [](https://ai.pydantic.dev/graph/#__code_7_annotation_10)\n                ctx.state.user_balance -= price\n                return End(None)\n            else:\n                diff = price - ctx.state.user_balance\n                print(f'Not enough money for {self.product}, need {diff:0.2f} more')\n                #> Not enough money for crisps, need 0.75 more\n                return InsertCoin()  [](https://ai.pydantic.dev/graph/#__code_7_annotation_11)\n        else:\n            print(f'No such product: {self.product}, try again')\n            return SelectProduct()  [](https://ai.pydantic.dev/graph/#__code_7_annotation_12)\n\nvending_machine_graph = Graph(\n[](https://ai.pydantic.dev/graph/#__code_7_annotation_13)\n\n    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\n)\n\nasync defmain():\n\n    state = MachineState()  [](https://ai.pydantic.dev/graph/#__code_7_annotation_14)\n    await vending_machine_graph.run(InsertCoin(), state=state)  [](https://ai.pydantic.dev/graph/#__code_7_annotation_15)\n    print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\n    #> purchase successful item=crisps change=0.25\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you'll\nneed to add`asyncio.run(main())` to run `main`)_\n\nA [mermaid diagram](https://ai.pydantic.dev/graph/#mermaid-diagrams) for this\ngraph can be generated with the following code:\n\nvending_machine_diagram.py```\n\nfromvending_machineimport InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin)\n\n```\n\nThe diagram generated by the above code is:\n\n```\n\n---\ntitle: vending_machine_graph\n\n---\nstateDiagram-v2\n\n  [*] --> InsertCoin\n\n  InsertCoin --> CoinsInserted\n\n  CoinsInserted --> SelectProduct\n\n  CoinsInserted --> Purchase\n\n  SelectProduct --> Purchase\n\n  Purchase --> InsertCoin\n\n  Purchase --> SelectProduct\n\n  Purchase --> [*]\n\n```\n\nSee [below](https://ai.pydantic.dev/graph/#mermaid-diagrams) for more\ninformation on generating diagrams.\n\n## GenAI Example\n\nSo far we haven't shown an example of a Graph that actually uses PydanticAI or\nGenAI at all.\n\nIn this example, one agent generates a welcome email to a user and the other\nagent provides feedback on the email.\n\nThis graph has a very simple structure:\n\n```\n\n---\ntitle: feedback_graph\n\n---\nstateDiagram-v2\n\n  [*] --> WriteEmail\n\n  WriteEmail --> Feedback\n\n  Feedback --> WriteEmail\n\n  Feedback --> [*]\n\n```\n\ngenai_email_feedback.py",
  "```\n\nfrom__future__import annotations as _annotations\n\nfromdataclassesimport dataclass, field\n\nfrompydanticimport BaseModel, EmailStr\n\nfrompydantic_aiimport Agent, format_as_xml\n\nfrompydantic_ai.messagesimport ModelMessage\n\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\n\n@dataclass\n\nclassUser:\n\n    name: str\n    email: EmailStr\n    interests: list[str]\n\n@dataclass\n\nclassEmail:\n\n    subject: str\n    body: str\n\n@dataclass\n\nclassState:\n\n    user: User\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n\nemail_writer_agent = Agent(\n\n    'google-vertex:gemini-1.5-pro',\n    output_type=Email,\n    system_prompt='Write a welcome email to our tech blog.',\n)\n\n@dataclass\n\nclassWriteEmail(BaseNode[State]):\n\n    email_feedback: str | None = None\n\n    async defrun(self, ctx: GraphRunContext[State]) -> Feedback:\n        if self.email_feedback:\n            prompt = (\n                f'Rewrite the email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}\\n'\n                f'Feedback: {self.email_feedback}'\n            )\n        else:\n            prompt = (\n                f'Write a welcome email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}'\n            )\n\n        result = await email_writer_agent.run(\n            prompt,\n            message_history=ctx.state.write_agent_messages,\n        )\n        ctx.state.write_agent_messages += result.new_messages()\n        return Feedback(result.output)\n\nclassEmailRequiresWrite(BaseModel):\n\n    feedback: str\n\nclassEmailOk(BaseModel):\n\n    pass\n\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\n    'openai:gpt-4o',\n    output_type=EmailRequiresWrite | EmailOk,  # type: ignore\n    system_prompt=(\n        'Review the email and provide feedback, email must reference the users specific interests.'\n    ),\n)\n\n@dataclass\n\nclassFeedback(BaseNode[State, None, Email]):\n\n    email: Email\n\n    async defrun(\n        self,\n        ctx: GraphRunContext[State],\n    ) -> WriteEmail | End[Email]:\n        prompt = format_as_xml({'user': ctx.state.user, 'email': self.email})\n        result = await feedback_agent.run(prompt)\n        if isinstance(result.output, EmailRequiresWrite):\n            return WriteEmail(email_feedback=result.output.feedback)\n        else:\n            return End(self.email)\n\nasync defmain():\n\n    user = User(\n        name='John Doe',\n        email='john.joe@example.com',\n        interests=['Haskel', 'Lisp', 'Fortran'],\n    )\n    state = State(user)\n    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n    result = await feedback_graph.run(WriteEmail(), state=state)\n    print(result.output)\n\"\"\"\n\n    Email(\n        subject='Welcome to our tech blog!',\n        body='Hello John, Welcome to our tech blog! ...',\n    )\n    \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you'll\nneed to add`asyncio.run(main())` to run `main`)_\n\n## Iterating Over a Graph\n\n### Using `Graph.iter` for `async for` iteration\n\nSometimes you want direct control or insight into each node as the graph\nexecutes. The easiest way to do that is with the\n[`Graph.iter`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter)\nmethod, which returns a **context manager** that yields a\n[`GraphRun`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun)\nobject. The `GraphRun` is an async-iterable over the nodes of your graph,\nallowing you to record or modify them as they execute.\n\nHere's an example:\n\ncount_down.py```\n\nfrom__future__import annotations as _annotations\n\nfromdataclassesimport dataclass\n\nfrompydantic_graphimport Graph, BaseNode, End, GraphRunContext\n\n@dataclass\n\nclassCountDownState:\n\n    counter: int\n\n@dataclass\n\nclassCountDown(BaseNode[CountDownState, None, int]):\n\n    async defrun(self, ctx: GraphRunContext[CountDownState]) -> CountDown | End[int]:\n        if ctx.state.counter <= 0:\n            return End(ctx.state.counter)\n        ctx.state.counter -= 1\n        return CountDown()\n\ncount_down_graph = Graph(nodes=[CountDown])\n\nasync defmain():\n\n    state = CountDownState(counter=3)\n    async with count_down_graph.iter(CountDown(), state=state) as run:  [](https://ai.pydantic.dev/graph/#__code_10_annotation_1)\n        async for node in run:  [](https://ai.pydantic.dev/graph/#__code_10_annotation_2)\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: End(data=0)\n    print('Final output:', run.result.output)  [](https://ai.pydantic.dev/graph/#__code_10_annotation_3)\n    #> Final output: 0",
  "```\n\n### Using `GraphRun.next(node)` manually\n\nAlternatively, you can drive iteration manually with the\n[`GraphRun.next`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next)\nmethod, which allows you to pass in whichever node you want to run next. You can\nmodify or selectively skip nodes this way.\n\nBelow is a contrived example that stops whenever the counter is at 2, ignoring\nany node runs beyond that:\n\ncount_down_next.py```\n\nfrompydantic_graphimport End, FullStatePersistence\n\nfromcount_downimport CountDown, CountDownState, count_down_graph\n\nasync defmain():\n\n    state = CountDownState(counter=5)\n    persistence = FullStatePersistence()  \nUse\nFullStatePersistence[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence)\nso we can show the history of the run, see State\nPersistence[](https://ai.pydantic.dev/graph/#state-persistence) below for more\ninformation.\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_7)\n\n    async with count_down_graph.iter(\n        CountDown(), state=state, persistence=persistence\n    ) as run:\n        node = run.next_node  \nWe start by grabbing the first node that will be run in the agent's graph.\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_1)\n\n        while not isinstance(node, End):  \nThe agent run is finished once an End node has been produced; instances of End\ncannot be passed to next.\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_2)\n\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            if state.counter == 2:\n                break  \nIf the user decides to stop early, we break out of the loop. The graph run won't\nhave a real final result in that case (run.result remains None).\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_3)\n\n            node = await run.next(node)  \nAt each step, we call await run.next(node) to run it and get the next node (or\nan End).\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_4)\n\n        print(run.result)  \nBecause we did not continue the run until it finished, the result is not set.\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_5)\n\n        #> None\n\n        for step in persistence.history:  \nThe run's history is still populated with the steps we executed so far.\n\n[](https://ai.pydantic.dev/graph/#__code_11_annotation_6)\n\n            print('History Step:', step.state, step.state)\n            #> History Step: CountDownState(counter=5) CountDownState(counter=5)\n            #> History Step: CountDownState(counter=4) CountDownState(counter=4)\n            #> History Step: CountDownState(counter=3) CountDownState(counter=3)\n            #> History Step: CountDownState(counter=2) CountDownState(counter=2)",
  "```\n\n## State Persistence\n\nOne of the biggest benefits of finite state machine (FSM) graphs is how they\nsimplify the handling of interrupted execution. This might happen for a variety\nof reasons:\n\n  * the state machine logic might fundamentally need to be paused — e.g. the returns workflow for an e-commerce order needs to wait for the item to be posted to the returns center or because execution of the next node needs input from a user so needs to wait for a new http request,\n  * the execution takes so long that the entire graph can't reliably be executed in a single continuous run — e.g. a deep research agent that might take hours to run,\n  * you want to run multiple graph nodes in parallel in different processes / hardware instances (note: parallel node execution is not yet supported in `pydantic-graph`, see [#704](https://github.com/pydantic/pydantic-ai/issues/704)).\n\nTrying to make a conventional control flow (i.e., boolean logic and nested\nfunction calls) implementation compatible with these usage scenarios generally\nresults in brittle and over-complicated spaghetti code, with the logic required\nto interrupt and resume execution dominating the implementation.\n\nTo allow graph runs to be interrupted and resumed, `pydantic-graph` provides\nstate persistence — a system for snapshotting the state of a graph run before\nand after each node is run, allowing a graph run to be resumed from any point in\nthe graph.\n\n`pydantic-graph` includes three state persistence implementations:\n\n  * [`SimpleStatePersistence`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.SimpleStatePersistence) — Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default.\n  * [`FullStatePersistence`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence) — In memory state persistence that hold a list of snapshots.\n  * [`FileStatePersistence`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence) — File-based state persistence that saves snapshots to a JSON file.\n\nIn production applications, developers should implement their own state\npersistence by subclassing\n[`BaseStatePersistence`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence)\nabstract base class, which might persist runs in a relational database like\nPostgresQL.\n\nAt a high level the role of `StatePersistence` implementations is to store and\nretrieve\n[`NodeSnapshot`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.NodeSnapshot)\nand\n[`EndSnapshot`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.EndSnapshot)\nobjects.\n\n[`graph.iter_from_persistence()`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence)\nmay be used to run the graph based on the state stored in persistence.\n\nWe can run the `count_down_graph` from\n[above](https://ai.pydantic.dev/graph/#iterating-over-a-graph), using\n[`graph.iter_from_persistence()`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence)\nand\n[`FileStatePersistence`](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence).\n\nAs you can see in this code, `run_node` requires no external application state\n(apart from state persistence) to be run, meaning graphs can easily be executed\nby distributed execution and queueing systems.\n\ncount_down_from_persistence.py",
  "```\n\nfrompathlibimport Path\n\nfrompydantic_graphimport End\n\nfrompydantic_graph.persistence.fileimport FileStatePersistence\n\nfromcount_downimport CountDown, CountDownState, count_down_graph\n\nasync defmain():\n\n    run_id = 'run_abc123'\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))  \nCreate a\nFileStatePersistence[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence)\nto use to start the graph.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_1)\n\n    state = CountDownState(counter=5)\n    await count_down_graph.initialize(  \nCall\ngraph.initialize()[](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.initialize)\nto set the initial graph state in the persistence object.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_2)\n\n        CountDown(), state=state, persistence=persistence\n    )\n\n    done = False\n    while not done:\n        done = await run_node(run_id)\n\nasync defrun_node(run_id: str) -> bool:  \n\nrun_node is a pure function that doesn't need access to any other process state\nto run the next node of the graph, except the ID of the run.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_3)\n\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))\n    async with count_down_graph.iter_from_persistence(persistence) as run:  \nCall\ngraph.iter_from_persistence()[](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence)\ncreate a\nGraphRun[](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun)\nobject that will run the next node of the graph from the state stored in\npersistence. This will return either a node or an End object.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_4)\n\n        node_or_end = await run.next()  \n\ngraph.run()[](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run)\nwill return either a\nnode[](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode)\nor an\nEnd[](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End)\nobject.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_5)\n\n    print('Node:', node_or_end)\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: End(data=0)\n    return isinstance(node_or_end, End)  \nCheck if the node is an\nEnd[](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.End)\nobject, if it is, the graph run is complete.\n\n[](https://ai.pydantic.dev/graph/#__code_12_annotation_6)\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you'll\nneed to add`asyncio.run(main())` to run `main`)_\n\n### Example: Human in the loop.\n\nAs noted above, state persistence allows graphs to be interrupted and resumed.\nOne use case of this is to allow user input to continue.\n\nIn this example, an AI asks the user a question, the user provides an answer,\nthe AI evaluates the answer and ends if the user got it right or asks another\nquestion if they got it wrong.\n\nInstead of running the entire graph in a single process invocation, we run the\ngraph by running the process repeatedly, optionally providing an answer to the\nquestion as a command line argument.\n\n`ai_q_and_a_graph.py` — `question_graph` definition\n\nai_q_and_a_graph.py",
  "```\n\nfrom__future__import annotations as _annotations\n\nfromdataclassesimport dataclass, field\n\nfrompydanticimport BaseModel\n\nfrompydantic_graphimport (\n\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\n\nfrompydantic_aiimport Agent, format_as_xml\n\nfrompydantic_ai.messagesimport ModelMessage\n\nask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n\n@dataclass\n\nclassQuestionState:\n\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n@dataclass\n\nclassAsk(BaseNode[QuestionState]):\n\n    async defrun(self, ctx: GraphRunContext[QuestionState]) -> Answer:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n@dataclass\n\nclassAnswer(BaseNode[QuestionState]):\n\n    question: str\n\n    async defrun(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\nclassEvaluationResult(BaseModel, use_attribute_docstrings=True):\n\n    correct: bool\n\"\"\"Whether the answer is correct.\"\"\"\n\n    comment: str\n\"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\nevaluate_agent = Agent(\n\n    'openai:gpt-4o',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n@dataclass\n\nclassEvaluate(BaseNode[QuestionState, None, str]):\n\n    answer: str\n\n    async defrun(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> End[str] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n@dataclass\n\nclassReprimand(BaseNode[QuestionState]):\n\n    comment: str\n\n    async defrun(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\nquestion_graph = Graph(\n\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+)_\n\nai_q_and_a_run.py",
  "```\n\nimportsys\n\nfrompathlibimport Path\n\nfrompydantic_graphimport End\n\nfrompydantic_graph.persistence.fileimport FileStatePersistence\n\nfrompydantic_ai.messagesimport ModelMessage  # noqa: F401\n\nfromai_q_and_a_graphimport Ask, question_graph, Evaluate, QuestionState, Answer\n\nasync defmain():\n\n    answer: str | None = sys.argv[1] if len(sys.argv) > 1 else None  \nGet the user's answer from the command line, if provided. See question graph\nexample[](https://ai.pydantic.dev/examples/question-graph/) for a complete\nexample.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_1)\n\n    persistence = FileStatePersistence(Path('question_graph.json'))  \nCreate a state persistence instance the 'question_graph.json' file may or may\nnot already exist.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_2)\n\n    persistence.set_graph_types(question_graph)  \nSince we're using the persistence\ninterface[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence)\noutside a graph, we need to call\nset_graph_types[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.set_graph_types)\nto set the graph generic types StateT and RunEndT for the persistence instance.\nThis is necessary to allow the persistence instance to know how to serialize and\ndeserialize graph nodes.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_3)\n\n    if snapshot := await persistence.load_next():  \nIf we're run the graph before,\nload_next[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_next)\nwill return a snapshot of the next node to run, here we use state from that\nsnapshot, and create a new Evaluate node with the answer provided on the command\nline.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_4)\n\n        state = snapshot.state\n        assert answer is not None\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()  \nIf the graph hasn't been run before, we create a new QuestionState and start\nwith the Ask node.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_5)\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()  \nCall\nGraphRun.next()[](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next)\nto run the node. This will return either a node or an End object.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_6)\n\n            if isinstance(node, End):  \nIf the node is an End object, the graph run is complete. The data field of the\nEnd object contains the comment returned by the evaluate_agent about the correct\nanswer.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_7)\n\n                print('END:', node.data)\n                history = await persistence.load_all()  \nTo demonstrate the state persistence, we call\nload_all[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_all)\nto get all the snapshots from the persistence instance. This will return a list\nof\nSnapshot[](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.Snapshot)\nobjects.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_8)\n\n                print([e.node for e in history])\n                break\n            elif isinstance(node, Answer):  \nIf the node is an Answer object, we print the question and break out of the loop\nto end the process and wait for user input.\n\n[](https://ai.pydantic.dev/graph/#__code_14_annotation_9)\n\n                print(node.question)\n                #> What is the capital of France?\n                break\n            # otherwise just continue\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you'll\nneed to add`asyncio.run(main())` to run `main`)_\n\nFor a complete example of this graph, see the [question graph\nexample](https://ai.pydantic.dev/examples/question-graph/).\n\n## Dependency Injection\n\nAs with PydanticAI, `pydantic-graph` supports dependency injection via a generic\nparameter on\n[`Graph`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph)\nand\n[`BaseNode`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode),\nand the\n[`GraphRunContext.deps`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext.deps)\nfield.\n\nAs an example of dependency injection, let's modify the `DivisibleBy5` example\n[above](https://ai.pydantic.dev/graph/#graph) to use a\n[`ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor)\nto run the compute load in a separate process (this is a contrived example,\n`ProcessPoolExecutor` wouldn't actually improve performance in this example):\n\ndeps_example.py",
  "```\n\nfrom__future__import annotations\n\nimportasyncio\n\nfromconcurrent.futuresimport ProcessPoolExecutor\n\nfromdataclassesimport dataclass\n\nfrompydantic_graphimport BaseNode, End, FullStatePersistence, Graph,\nGraphRunContext\n\n@dataclass\n\nclassGraphDeps:\n\n    executor: ProcessPoolExecutor\n\n@dataclass\n\nclassDivisibleBy5(BaseNode[None, GraphDeps, int]):\n\n    foo: int\n\n    async defrun(\n        self,\n        ctx: GraphRunContext[None, GraphDeps],\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n@dataclass\n\nclassIncrement(BaseNode[None, GraphDeps]):\n\n    foo: int\n\n    async defrun(self, ctx: GraphRunContext[None, GraphDeps]) -> DivisibleBy5:\n        loop = asyncio.get_running_loop()\n        compute_result = await loop.run_in_executor(\n            ctx.deps.executor,\n            self.compute,\n        )\n        return DivisibleBy5(compute_result)\n\n    defcompute(self) -> int:\n        return self.foo + 1\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\n\nasync defmain():\n\n    with ProcessPoolExecutor() as executor:\n        deps = GraphDeps(executor)\n        result = await fives_graph.run(DivisibleBy5(3), deps=deps, persistence=FullStatePersistence())\n    print(result.output)\n    #> 5\n    # the full history is quite verbose (see below), so we'll just print the summary\n    print([item.node for item in result.persistence.history])\n\"\"\"\n\n    [\n        DivisibleBy5(foo=3),\n        Increment(foo=3),\n        DivisibleBy5(foo=4),\n        Increment(foo=4),\n        DivisibleBy5(foo=5),\n        End(data=5),\n    ]\n    \"\"\"\n\n```\n\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you'll\nneed to add`asyncio.run(main())` to run `main`)_\n\n## Mermaid Diagrams\n\nPydantic Graph can generate [mermaid](https://mermaid.js.org/)\n[`stateDiagram-v2`](https://mermaid.js.org/syntax/stateDiagram.html) diagrams\nfor graphs, as shown above.\n\nThese diagrams can be generated with:\n\n  * [`Graph.mermaid_code`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.mermaid_code) to generate the mermaid code for a graph\n  * [`Graph.mermaid_image`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.mermaid_image) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/)\n  * [`Graph.mermaid_save`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.mermaid_save) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) and save it to a file\n\nBeyond the diagrams shown above, you can also customize mermaid diagrams with\nthe following options:\n\n  * [`Edge`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.Edge) allows you to apply a label to an edge\n  * [`BaseNode.docstring_notes`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.docstring_notes) and [`BaseNode.get_note`](https://ai.pydantic.dev/api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.get_note) allows you to add notes to nodes\n  * The [`highlighted_nodes`](https://ai.pydantic.dev/api/pydantic_graph/graph/#pydantic_graph.graph.Graph.mermaid_code) parameter allows you to highlight specific node(s) in the diagram\n\nPutting that together, we can edit the last\n[`ai_q_and_a_graph.py`](https://ai.pydantic.dev/graph/#example-human-in-the-\nloop) example to:\n\n  * add labels to some edges\n  * add a note to the `Ask` node\n  * highlight the `Answer` node\n  * save the diagram as a `PNG` image to file\n\nai_q_and_a_graph_extra.py```\n\n...\n\nfromtypingimport Annotated\n\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext, Edge\n\n...\n\n@dataclass\n\nclassAsk(BaseNode[QuestionState]):\n\n\"\"\"Generate question using GPT-4o.\"\"\"\n\n    docstring_notes = True\n    async defrun(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        ...\n\n...\n\n@dataclass\n\nclassEvaluate(BaseNode[QuestionState]):\n\n    answer: str\n\n    async defrun(\n            self,\n            ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        ...\n\n...\n\nquestion_graph.mermaid_save('image.png', highlighted_nodes=[Answer])\n\n```\n\n_(This example is not complete and cannot be run directly)_\n\nThis would generate an image that looks like this:\n\n```\n\n---\ntitle: question_graph\n\n---\nstateDiagram-v2\n\n  Ask --> Answer: Ask the question\n\n  note right of Ask\n\n    Judge the answer.\n    Decide on next step.\n  end note\n\n  Answer --> Evaluate\n\n  Evaluate --> Reprimand\n\n  Evaluate --> [*]: success\n\n  Reprimand --> Ask\n\nclassDef highlighted fill:#fdff32\n\nclass Answer highlighted",
  "```\n\n### Setting Direction of the State Diagram\n\nYou can specify the direction of the state diagram using one of the following\nvalues:\n\n  * `'TB'`: Top to bottom, the diagram flows vertically from top to bottom.\n  * `'LR'`: Left to right, the diagram flows horizontally from left to right.\n  * `'RL'`: Right to left, the diagram flows horizontally from right to left.\n  * `'BT'`: Bottom to top, the diagram flows vertically from bottom to top.\n\nHere is an example of how to do this using 'Left to Right' (LR) instead of the\ndefault 'Top to Bottom' (TB):\n\nvending_machine_diagram.py```\n\nfromvending_machineimport InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\n\n```\n\n```\n\n---\ntitle: vending_machine_graph\n\n---\nstateDiagram-v2\n\n  direction LR\n\n  [*] --> InsertCoin\n\n  InsertCoin --> CoinsInserted\n\n  CoinsInserted --> SelectProduct\n\n  CoinsInserted --> Purchase\n\n  SelectProduct --> Purchase\n\n  Purchase --> InsertCoin\n\n  Purchase --> SelectProduct\n\n  Purchase --> [*]\n\n```\n\n© Pydantic Services Inc. 2024 to present"
]